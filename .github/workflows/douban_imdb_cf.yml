# .github/workflows/douban_imdb_cf.yml

# å·¥ä½œæµçš„åç§°
name: Import Douban Data to Cloudflare D1

# è§¦å‘æ¡ä»¶ï¼šå½“æ‰‹åŠ¨è§¦å‘æ—¶è¿è¡Œ (workflow_dispatch)
on:
  workflow_dispatch:
    inputs:
      database_name:
        description: 'Your D1 Database Name'
        required: true
        default: 'pt-nexus' # æ‚¨çš„D1æ•°æ®åº“å
      json_url:
        description: 'URL of the JSON file to import'
        required: true
        default: 'https://raw.githubusercontent.com/ourbits/PtGen/refs/heads/main/internal_map/douban_imdb_map.json'

# å®šä¹‰ä¸€ä¸ªä»»åŠ¡ (job)
jobs:
  import-data:
    # ä»»åŠ¡è¿è¡Œåœ¨ GitHub æä¾›çš„æœ€æ–°ç‰ˆ Ubuntu è™šæ‹ŸæœåŠ¡å™¨ä¸Š
    runs-on: ubuntu-latest

    # ä»»åŠ¡çš„æ­¥éª¤
    steps:
      # æ­¥éª¤ 1: æ£€å‡ºä»£ç  (è™½ç„¶æˆ‘ä»¬æ²¡æœ‰ä»£ç ï¼Œä½†è¿™æ˜¯æ ‡å‡†æ­¥éª¤)
      - name: Checkout repository
        uses: actions/checkout@v4

      # æ­¥éª¤ 2: å®‰è£… Node.js
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20' # ä½¿ç”¨è¾ƒæ–°çš„ Node.js ç‰ˆæœ¬

      # æ­¥éª¤ 3: å®‰è£… Cloudflare Wrangler CLI
      - name: Install Wrangler
        run: npm install -g wrangler

      # æ­¥éª¤ 4: ä¸‹è½½ JSON æ•°æ®æ–‡ä»¶
      - name: Download JSON data
        run: |
          echo "Downloading data from ${{ github.event.inputs.json_url }}..."
          curl -o data.json "${{ github.event.inputs.json_url }}"
          echo "Download complete."

      # æ­¥éª¤ 5: åˆ›å»ºå¹¶æ‰§è¡Œå¯¼å…¥è„šæœ¬
      - name: Create and run import script
        # å°† Cloudflare çš„å‡­è¯è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼Œè¿™æ · Wrangler æ‰èƒ½ä½¿ç”¨
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
        run: |
          # ä½¿ç”¨ Node.js åŠ¨æ€ç”Ÿæˆå¹¶æ‰§è¡Œå¯¼å…¥è„šæœ¬
          node -e "
            const fs = require('fs');
            const { execSync } = require('child_process');

            const DATABASE_NAME = '${{ github.event.inputs.database_name }}';
            const INPUT_FILE = './data.json';
            const BATCH_SIZE = 200;

            async function main() {
              console.log('[æ­¥éª¤ 1/3] è¯»å–æœ¬åœ°å·²ä¸‹è½½çš„æ•°æ®...');
              const content = fs.readFileSync(INPUT_FILE, 'utf-8');
              const allData = JSON.parse(content);
              console.log(`è¯»å–æˆåŠŸ! å…± ${allData.length} æ¡è®°å½•ã€‚`);

              console.log('[æ­¥éª¤ 2/3] (å¯é€‰) æ¸…ç©ºäº‘ç«¯çš„æ•°æ®è¡¨...');
              try {
                execSync(`wrangler d1 execute ${DATABASE_NAME} --command 'DELETE FROM \"douban-imdb\";'`, { stdio: 'inherit' });
                console.log('äº‘ç«¯æ•°æ®è¡¨å·²æ¸…ç©ºã€‚');
              } catch (e) {
                console.error('æ¸…ç©ºæ•°æ®è¡¨å¤±è´¥:', e.message);
              }

              console.log('[æ­¥éª¤ 3/3] å¼€å§‹åˆ†æ‰¹å¯¼å…¥æ•°æ®åˆ°äº‘ç«¯...');
              for (let i = 0; i < allData.length; i += BATCH_SIZE) {
                const batch = allData.slice(i, i + BATCH_SIZE);
                const currentBatchNum = i / BATCH_SIZE + 1;
                
                const sqlValues = batch.map(() => '(?, ?, ?, ?)').join(',');
                const fullSql = \`INSERT OR IGNORE INTO \\"douban-imdb\\" (doubanid, imdbid, name, year) VALUES \${sqlValues}\`;
                const bindings = batch.flatMap(item => [item.dbid, item.imdbid, item.name, item.year]);
                
                const query = { sql: fullSql, params: bindings };
                fs.writeFileSync('./query.json', JSON.stringify(query));

                try {
                  console.log(\`æ­£åœ¨æ¨é€ç¬¬ \${currentBatchNum} æ‰¹æ•°æ®...\`);
                  execSync(\`wrangler d1 execute ${DATABASE_NAME} --json-file=./query.json\`, { stdio: 'inherit' });
                } catch (e) {
                  console.error(\`ç¬¬ \${currentBatchNum} æ‰¹æ•°æ®æ¨é€å¤±è´¥:\`, e.message);
                  process.exit(1);
                }
              }

              console.log('ğŸ‰ å…¨éƒ¨æ•°æ®å¯¼å…¥æˆåŠŸ!');
            }

            main();
          "
