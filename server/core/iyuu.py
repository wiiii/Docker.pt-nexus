# core/iyuu.py
import logging
import time
import os
import requests
import json
import hashlib
from threading import Thread
from collections import defaultdict
from datetime import datetime


class IYUUThread(Thread):
    """IYUUåå°çº¿ç¨‹ï¼Œå®šæœŸèšåˆç§å­ä¿¡æ¯å¹¶è¿›è¡Œç›¸å…³å¤„ç†ã€‚"""

    def __init__(self, db_manager, config_manager):
        super().__init__(daemon=True, name="IYUUThread")
        self.db_manager = db_manager
        self.config_manager = config_manager
        self._is_running = True
        # è®¾ç½®ä¸º6å°æ—¶è¿è¡Œä¸€æ¬¡
        self.interval = 21600  # 6å°æ—¶

    def run(self):
        print("IYUUThread çº¿ç¨‹å·²å¯åŠ¨ï¼Œæ¯6å°æ—¶æ‰§è¡Œä¸€æ¬¡æŸ¥è¯¢ä»»åŠ¡ã€‚")
        # ç­‰å¾…5ç§’å†å¼€å§‹æ‰§è¡Œï¼Œé¿å…ä¸ä¸»ç¨‹åºå¯åŠ¨å†²çª
        time.sleep(5)

        while self._is_running:
            start_time = time.monotonic()
            try:
                self._process_torrents()
            except Exception as e:
                logging.error(f"IYUUThread æ‰§è¡Œå‡ºé”™: {e}", exc_info=True)

            # ç­‰å¾…ä¸‹æ¬¡æ‰§è¡Œ
            elapsed = time.monotonic() - start_time
            time.sleep(max(0, self.interval - elapsed))

    def _get_configured_sites(self):
        """è·å–torrentsè¡¨ä¸­å·²å­˜åœ¨çš„ç«™ç‚¹åˆ—è¡¨"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)

            # æŸ¥è¯¢torrentsè¡¨ä¸­æ‰€æœ‰ä¸åŒçš„ç«™ç‚¹
            cursor.execute(
                "SELECT DISTINCT sites FROM torrents WHERE sites IS NOT NULL AND sites != ''"
            )
            sites_result = cursor.fetchall()

            # æå–ç«™ç‚¹åç§°å¹¶å»é‡
            sites = set()
            for row in sites_result:
                site = row['sites']
                if site:
                    # å¦‚æœç«™ç‚¹å­—æ®µåŒ…å«å¤šä¸ªç«™ç‚¹ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼Œåˆ™åˆ†å‰²å®ƒä»¬
                    if ',' in site:
                        site_list = site.split(',')
                        sites.update(s.strip() for s in site_list if s.strip())
                    else:
                        sites.add(site.strip())

            cursor.close()
            conn.close()

            sites_list = list(sites)
            log_iyuu_message(f"è·å–åˆ° {len(sites_list)} ä¸ªå·²å­˜åœ¨çš„ç«™ç‚¹", "INFO")
            log_iyuu_message(f"ç«™ç‚¹åˆ—è¡¨: {', '.join(sites_list)}", "INFO")

            return sites_list
        except Exception as e:
            logging.error(f"è·å–torrentsè¡¨ä¸­çš„ç«™ç‚¹ä¿¡æ¯æ—¶å‡ºé”™: {e}", exc_info=True)
            return []

    def _process_torrents(self, is_manual_trigger=False):
        """å¤„ç†ç§å­æ•°æ®ï¼ŒæŒ‰nameåˆ—è¿›è¡Œèšåˆ"""
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨æŸ¥è¯¢ï¼ˆä»…åœ¨è‡ªåŠ¨è§¦å‘æ—¶æ£€æŸ¥ï¼‰
        if not is_manual_trigger:
            config = self.config_manager.get()
            iyuu_settings = config.get("iyuu_settings", {})
            auto_query_enabled = iyuu_settings.get("auto_query_enabled", True)

            # å¦‚æœæœªå¯ç”¨è‡ªåŠ¨æŸ¥è¯¢ï¼Œåˆ™è·³è¿‡
            if not auto_query_enabled:
                log_iyuu_message("IYUUè‡ªåŠ¨æŸ¥è¯¢å·²ç¦ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡æŸ¥è¯¢ä»»åŠ¡", "INFO")
                return

        log_iyuu_message(f"[{current_time}] å¼€å§‹æ‰§è¡ŒIYUUç§å­èšåˆä»»åŠ¡", "INFO")
        conn = None
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)

            # æŸ¥è¯¢æ‰€æœ‰ç§å­æ•°æ®ï¼Œåªç­›é€‰ä½“ç§¯å¤§äº1GBçš„ç§å­ï¼ˆ1GB = 1073741824å­—èŠ‚ï¼‰
            cursor.execute(
                "SELECT hash, name, sites, size FROM torrents WHERE name IS NOT NULL AND name != '' AND size > 207374182"
            )
            torrents_raw = [dict(row) for row in cursor.fetchall()]

            # è·å–é…ç½®çš„ç«™ç‚¹åˆ—è¡¨ï¼ˆç”¨äºè¿‡æ»¤æ”¯æŒçš„ç«™ç‚¹ï¼‰
            configured_sites = self._get_configured_sites()

            # æŒ‰ç§å­åç§°è¿›è¡Œèšåˆï¼Œè®°å½•æ‰€æœ‰åŒåç§å­ï¼ˆåŒ…æ‹¬ä¸æ”¯æŒIYUUçš„ç«™ç‚¹ï¼‰
            all_torrents = defaultdict(list)
            for t in torrents_raw:
                torrent_name = t['name']
                site = t.get('sites', None)
                all_torrents[torrent_name].append({
                    'hash': t['hash'],
                    'sites': site,
                    'size': t.get('size', 0)
                })

            # ä¸ºèšåˆåˆ›å»ºä¸€ä¸ªåªåŒ…å«æ”¯æŒç«™ç‚¹çš„ç‰ˆæœ¬ï¼ˆç”¨äºé€‰æ‹©hashè¿›è¡ŒæŸ¥è¯¢ï¼‰
            agg_torrents = defaultdict(list)
            for t in torrents_raw:
                torrent_name = t['name']
                site = t.get('sites', None)
                # åªæœ‰å½“ç«™ç‚¹æ˜¯IYUUæ”¯æŒçš„ç«™ç‚¹æ—¶æ‰æ·»åŠ åˆ°èšåˆåˆ—è¡¨ä¸­ï¼ˆç”¨äºé€‰æ‹©hashï¼‰
                # è¿‡æ»¤æ‰é’è›™å’ŒæŸ æª¬ä¸¤ä¸ªç«™ç‚¹
                if site and site in configured_sites and site not in [
                        'é’è›™', 'æŸ æª¬ä¸ç”œ'
                ]:
                    agg_torrents[torrent_name].append({
                        'hash': t['hash'],
                        'sites': site,
                        'size': t.get('size', 0)
                    })

            # å‡†å¤‡å†™å…¥æ–‡ä»¶çš„å†…å®¹
            output_lines = []
            output_lines.append("=== IYUUç§å­èšåˆç»“æœ ===\n")
            output_lines.append(
                f"èšåˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            output_lines.append(f"å…±èšåˆäº† {len(agg_torrents)} ä¸ªå”¯ä¸€ç§å­ç»„\n")
            output_lines.append("=" * 50 + "\n")

            # ç”Ÿæˆèšåˆç»“æœ
            for name, torrents in agg_torrents.items():
                # é€‰æ‹©ä¸€ä¸ªhashç”¨äºåç»­çš„IYUUæœç´¢
                selected_hash = torrents[0]['hash'] if torrents else None
                sites_list = [t['sites'] for t in torrents if t['sites']]

                # æ·»åŠ èšåˆä¿¡æ¯åˆ°è¾“å‡ºå†…å®¹
                output_lines.append(f"[IYUU] ç§å­ç»„: {name}\n")
                output_lines.append(f"  - åŒ…å« {len(torrents)} ä¸ªç§å­\n")
                output_lines.append(f"  - é€‰æ‹©çš„hash: {selected_hash}\n")
                output_lines.append(
                    f"  - å­˜åœ¨äºç«™ç‚¹: {', '.join(sites_list) if sites_list else 'æ— '}\n"
                )
                output_lines.append("---\n")

            output_lines.append("=" * 50 + "\n")
            output_lines.append("=== IYUUç§å­èšåˆä»»åŠ¡æ‰§è¡Œå®Œæˆ ===\n")

            # ä»é…ç½®è·å–tmpç›®å½•ï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸´æ—¶ç›®å½•
            config = self.config_manager.get()
            iyuu_settings = config.get("iyuu_settings", {})
            tmp_dir = iyuu_settings.get("tmp_dir", "")

            # å¦‚æœé…ç½®ä¸­æœªæŒ‡å®štmp_dirï¼Œåˆ™ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸´æ—¶ç›®å½•
            if not tmp_dir:
                from config import TEMP_DIR
                tmp_dir = TEMP_DIR

            if not os.path.exists(tmp_dir):
                os.makedirs(tmp_dir)

            # å†™å…¥æ–‡ä»¶
            timestamp = int(time.time())
            output_file = os.path.join(tmp_dir,
                                       f"iyuu_aggregation_{timestamp}.txt")
            with open(output_file, 'w', encoding='utf-8') as f:
                f.writelines(output_lines)

            log_iyuu_message(f"IYUUç§å­èšåˆç»“æœå·²ä¿å­˜åˆ°: {output_file}", "INFO")

            # è·å–å·²é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            configured_sites = self._get_configured_sites()
            log_iyuu_message(f"æ•°æ®åº“ä¸­å­˜åœ¨ {len(configured_sites)} ä¸ªé…ç½®ç«™ç‚¹", "INFO")

            # æ‰§è¡ŒIYUUæœç´¢é€»è¾‘ï¼Œä¼ é€’å·²é…ç½®çš„ç«™ç‚¹åˆ—è¡¨å’Œæ‰€æœ‰ç§å­ä¿¡æ¯
            self._perform_iyuu_search(agg_torrents, configured_sites,
                                      all_torrents)

            log_iyuu_message("=== IYUUç§å­èšåˆä»»åŠ¡æ‰§è¡Œå®Œæˆ ===", "INFO")

        except Exception as e:
            logging.error(f"å¤„ç†ç§å­æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
        finally:
            if conn:
                if 'cursor' in locals() and cursor:
                    cursor.close()
                conn.close()

    def _get_existing_sites(self):
        """è·å–æ•°æ®åº“ä¸­é…ç½®çš„ç«™ç‚¹ä¿¡æ¯"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)

            # æŸ¥è¯¢æ‰€æœ‰å·²é…ç½®çš„ç«™ç‚¹
            cursor.execute("SELECT nickname, base_url, site FROM sites")
            sites = {}
            for row in cursor.fetchall():
                # ä»¥æ˜µç§°ä¸ºé”®å­˜å‚¨ç«™ç‚¹ä¿¡æ¯
                site_data = dict(row)
                sites[site_data['nickname']] = site_data

            cursor.close()
            conn.close()

            return sites
        except Exception as e:
            logging.error(f"è·å–æ•°æ®åº“ç«™ç‚¹ä¿¡æ¯æ—¶å‡ºé”™: {e}", exc_info=True)
            return {}

    def _perform_iyuu_search(self, agg_torrents, configured_sites,
                             all_torrents, force_query=False):
        """æ‰§è¡ŒIYUUæœç´¢é€»è¾‘
        
        Args:
            agg_torrents: èšåˆçš„ç§å­æ•°æ®
            configured_sites: é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            all_torrents: æ‰€æœ‰ç§å­æ•°æ®
            force_query: æ˜¯å¦å¼ºåˆ¶æŸ¥è¯¢ï¼Œå¿½ç•¥æ—¶é—´é—´éš”é™åˆ¶ï¼ˆé»˜è®¤Falseï¼‰
        """
        try:
            # è·å–IYUU token
            config = self.config_manager.get()
            iyuu_token = config.get("iyuu_token", "")

            if not iyuu_token:
                logging.warning("IYUU Tokenæœªé…ç½®ï¼Œè·³è¿‡IYUUæœç´¢ã€‚")
                return

            print(f"å¼€å§‹æ‰§è¡ŒIYUUæœç´¢ï¼Œå…± {len(agg_torrents)} ä¸ªç§å­ç»„")

            # è·å–è¿‡æ»¤åçš„sid_sha1å’Œç«™ç‚¹åˆ—è¡¨ï¼ŒåªåŒ…å«åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹
            sid_sha1, all_sites = get_filtered_sid_sha1_and_sites(
                iyuu_token, self.db_manager)

            # åˆ›å»ºç«™ç‚¹æ˜ å°„
            sites_map = {site['id']: site for site in all_sites}

            # åˆ›å»ºIYUUç«™ç‚¹åç§°åˆ°æ•°æ®åº“ç«™ç‚¹æ˜µç§°çš„æ˜ å°„è¡¨
            # åªåŒ…å«éœ€è¦æ˜ å°„çš„ç«™ç‚¹ï¼ˆIYUUåç§°ä¸æ•°æ®åº“æ˜µç§°ä¸åŒçš„æƒ…å†µï¼‰
            site_name_mapping = {
                # IYUUåç§° -> æ•°æ®åº“æ˜µç§°
                "ä¼˜å ¡": "æˆ‘å ¡",
                "è§‚ä¼—": "äººäºº",
                "æŸ æª¬": "æŸ æª¬ä¸é…¸",
                "hdclone": "HDClone",
                "æˆ‘çš„PT(CC)": "æˆ‘çš„PT",
                "LongPT": "é¾™PT",
                "March": "ä¸‰æœˆä¼ åª’",
                "hdbao": "çº¢è±†åŒ…",
                "LuckPT": "å¹¸è¿",
                "13city": "13City",
                "PTSKit": "PTSkit",
                "æ—¶å…‰": "æ—¶å…‰HDT",
                "æ˜¥å¤©": "ä¸å¯è¯´",
            }

            # è·å–æ•°æ®åº“ä¸­ç°æœ‰çš„ç«™ç‚¹ä¿¡æ¯
            existing_sites = self._get_existing_sites()
            print(f"æ•°æ®åº“ä¸­å­˜åœ¨ {len(existing_sites)} ä¸ªé…ç½®ç«™ç‚¹")

            # åªå¤„ç†å‰3ä¸ªç§å­ç»„ç”¨äºæµ‹è¯•
            test_torrents = list(agg_torrents.items())

            # è·å–æ€»ç§å­ç»„æ•°
            total_torrents = len(test_torrents)

            for i, (name, torrents) in enumerate(test_torrents):
                if not self._is_running:  # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦åº”è¯¥åœæ­¢
                    break

                # å¦‚æœä¸æ˜¯å¼ºåˆ¶æŸ¥è¯¢ï¼Œåˆ™æ£€æŸ¥æ—¶é—´é—´éš”
                if not force_query:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡ŒIYUUæŸ¥è¯¢ï¼ˆè·ç¦»ä¸Šæ¬¡æŸ¥è¯¢è¶…è¿‡è®¾ç½®çš„æ—¶é—´é—´éš”æˆ–ä»æœªæŸ¥è¯¢è¿‡ï¼‰
                    # è·å–è®¾ç½®çš„æŸ¥è¯¢é—´éš”æ—¶é—´ï¼ˆé»˜è®¤ä¸º72å°æ—¶ï¼‰
                    config = self.config_manager.get()
                    iyuu_settings = config.get("iyuu_settings", {})
                    query_interval_hours = iyuu_settings.get("query_interval_hours", 72)

                    if not self._should_query_iyuu(name, query_interval_hours):
                        skip_message = f"[{i+1}/{total_torrents}] ğŸ”„ ç§å­ç»„ '{name}' è·ç¦»ä¸Šæ¬¡æŸ¥è¯¢ä¸è¶³{query_interval_hours}å°æ—¶ï¼Œè·³è¿‡æŸ¥è¯¢"
                        log_iyuu_message(skip_message, "INFO")
                        continue

                print(f"[{i+1}/{total_torrents}] ğŸ” æ­£åœ¨å¤„ç†ç§å­ç»„: {name}")

                # å°è¯•æœ€å¤š3ä¸ªä¸åŒçš„hashè¿›è¡ŒæŸ¥è¯¢
                max_attempts = 3
                results = None
                selected_hash = None

                # è·å–å½“å‰ç§å­ç»„çš„æ‰€æœ‰torrentsï¼ŒæŒ‰ç«™ç‚¹è¿‡æ»¤
                filtered_torrents = [
                    t for t in torrents
                    if t.get('sites') and t['sites'] in configured_sites
                    and t['sites'] not in ['é’è›™', 'æŸ æª¬ä¸ç”œ']
                ]

                # å¦‚æœæ²¡æœ‰æ”¯æŒçš„ç«™ç‚¹ï¼Œåˆ™è·³è¿‡
                if not filtered_torrents:
                    log_iyuu_message(
                        f"[{i+1}/{total_torrents}] âš ï¸ ç§å­ç»„ '{name}' æ²¡æœ‰æ”¯æŒçš„ç«™ç‚¹ï¼Œè·³è¿‡æŸ¥è¯¢", "INFO"
                    )
                    # æ›´æ–°æ‰€æœ‰åŒåç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´ï¼ˆåŒ…æ‹¬ä¸æ”¯æŒIYUUçš„ç«™ç‚¹ï¼‰
                    self._update_iyuu_last_check(name, [],
                                                 all_torrents.get(name, []))
                    continue

                for attempt in range(min(max_attempts,
                                         len(filtered_torrents))):
                    if attempt >= len(filtered_torrents):
                        break

                    selected_hash = filtered_torrents[attempt]['hash']
                    site_name = filtered_torrents[attempt]['sites']
                    log_iyuu_message(
                        f"ä½¿ç”¨çš„hash [{attempt+1}/{min(max_attempts, len(filtered_torrents))}]: {selected_hash} (ç«™ç‚¹: {site_name})", "INFO"
                    )

                    try:
                        # æ‰§è¡Œæœç´¢
                        results = query_cross_seed(iyuu_token, selected_hash,
                                                   sid_sha1)
                        # å¦‚æœæˆåŠŸæŸ¥è¯¢åˆ°ç»“æœï¼Œåˆ™è·³å‡ºå¾ªç¯
                        log_iyuu_message(
                            f"[{i+1}/{total_torrents}] âœ… Hash {selected_hash[:8]}... æŸ¥è¯¢æˆåŠŸï¼Œåœæ­¢å°è¯•å…¶ä»–hash", "INFO"
                        )
                        break
                    except Exception as e:
                        error_msg = str(e)
                        # å¦‚æœæ˜¯"æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®"é”™è¯¯ï¼Œåˆ™å°è¯•ä¸‹ä¸€ä¸ªhash
                        if "æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®" in error_msg or "400" in error_msg:
                            log_iyuu_message(
                                f"[{i+1}/{total_torrents}] âš ï¸  Hash {selected_hash[:8]}... æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªhash...", "INFO"
                            )
                            continue
                        else:
                            # å…¶ä»–é”™è¯¯åˆ™é‡æ–°æŠ›å‡º
                            raise e

                # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
                if results is None:
                    log_iyuu_message(
                        f"[{i+1}/{total_torrents}] âŒ ç§å­ç»„ '{name}' æ‰€æœ‰hashéƒ½æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®", "INFO"
                    )
                    # æ›´æ–°æ‰€æœ‰åŒåç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´ï¼ˆåŒ…æ‹¬ä¸æ”¯æŒIYUUçš„ç«™ç‚¹ï¼‰
                    self._update_iyuu_last_check(name, [],
                                                 all_torrents.get(name, []))
                    continue

                # å¦‚æœæˆåŠŸæŸ¥è¯¢åˆ°ç»“æœï¼Œç»§ç»­å¤„ç†
                # æ‰“å°æœç´¢ç»“æœå¹¶ç­›é€‰ç°æœ‰ç«™ç‚¹
                if not results:
                    print(
                        f"[{i+1}/{total_torrents}] ç§å­ {selected_hash[:8]}... æœªåœ¨å…¶ä»–ç«™ç‚¹å‘ç°ã€‚"
                    )
                else:
                    # ç­›é€‰å‡ºç°åœ¨æ•°æ®åº“ä¸­çš„ç«™ç‚¹
                    matched_sites = []
                    for item in results:
                        sid = item.get("sid")
                        site_info = sites_map.get(sid)

                        if not site_info:
                            continue

                        scheme = "https" if site_info.get(
                            "is_https") != 0 else "http"
                        details_page = site_info.get(
                            "details_page", "details.php?id={}").replace(
                                "{}", str(item.get("torrent_id")))
                        full_url = f"{scheme}://{site_info.get('base_url', '')}/{details_page}"

                        # è·å–IYUUç«™ç‚¹åç§°
                        iyuu_site_name = site_info.get(
                            "nickname") or site_info.get(
                                "site") or f"SID {sid}"

                        # å°è¯•æ˜ å°„åˆ°æ•°æ®åº“ä¸­çš„ç«™ç‚¹åç§°
                        db_site_name = site_name_mapping.get(
                            iyuu_site_name, iyuu_site_name)

                        # æ£€æŸ¥ç«™ç‚¹æ˜¯å¦åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹åˆ—è¡¨ä¸­
                        if db_site_name in configured_sites:
                            # å¦‚æœç«™ç‚¹åœ¨æ•°æ®åº“ä¸­ä¹Ÿæœ‰é…ç½®ä¿¡æ¯ï¼Œåˆ™ä½¿ç”¨å®ƒ
                            site_info_dict = existing_sites.get(
                                db_site_name, {})
                            matched_sites.append({
                                'iyuu_name': iyuu_site_name,
                                'db_name': db_site_name,
                                'url': full_url,
                                'site_info': site_info_dict
                            })

                    # åªæ˜¾ç¤ºåŒ¹é…åˆ°çš„å·²é…ç½®ç«™ç‚¹
                    if matched_sites:
                        log_iyuu_message(
                            f"[{i+1}/{total_torrents}] ç§å­ {selected_hash[:8]}... åœ¨ {len(matched_sites)} ä¸ªå·²å­˜åœ¨çš„ç«™ç‚¹å‘ç°ï¼", "INFO"
                        )
                        for site in matched_sites:
                            iyuu_site_name = site['iyuu_name']
                            db_site_name = site['db_name']
                            full_url = site['url']

                            if iyuu_site_name != db_site_name:
                                log_iyuu_message(
                                    f"âœ… åŒ¹é…ç«™ç‚¹: {iyuu_site_name} -> {db_site_name}", "INFO"
                                )
                            else:
                                log_iyuu_message(f"âœ… åŒ¹é…ç«™ç‚¹: {iyuu_site_name}", "INFO")
                            log_iyuu_message(f"   é“¾æ¥: {full_url}", "INFO")
                    else:
                        log_iyuu_message(
                            f"[{i+1}/{total_torrents}] ç§å­ {selected_hash[:8]}... æœªåœ¨ä»»ä½•å·²å­˜åœ¨çš„ç«™ç‚¹å‘ç°ã€‚", "INFO"
                        )

                    log_iyuu_message(f"åœ¨torrentsè¡¨ä¸­æ‰¾åˆ° {len(matched_sites)} ä¸ªå·²å­˜åœ¨çš„ç«™ç‚¹", "INFO")

                    # ä¸ºç¼ºå¤±ç«™ç‚¹æ·»åŠ ç§å­è®°å½•
                    if matched_sites:
                        torrent_data = {
                            'hash': selected_hash,
                            'name': name,
                            'save_path': filtered_torrents[0].get('save_path', ''),
                            'size': filtered_torrents[0].get('size', 0),
                        }
                        self._add_missing_site_torrents(name, torrent_data, matched_sites)

                    # æ›´æ–°æ‰€æœ‰åŒåç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´ï¼ˆåŒ…æ‹¬ä¸æ”¯æŒIYUUçš„ç«™ç‚¹ï¼‰
                    self._update_iyuu_last_check(name, matched_sites,
                                                 all_torrents.get(name, []))

                # æ¯æ¬¡æŸ¥è¯¢ä¹‹é—´é—´éš”5ç§’ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
                if i < len(test_torrents) - 1:
                    log_iyuu_message(f"[{i+1}/{total_torrents}] ç­‰å¾…5ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡æŸ¥è¯¢...", "INFO")
                    for _ in range(5):  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦åœæ­¢
                        if not self._is_running:
                            return
                        time.sleep(1)

        except Exception as e:
            logging.error(f"IYUUæœç´¢æ‰§è¡Œå‡ºé”™: {e}", exc_info=True)

    def _should_query_iyuu(self, torrent_name, query_interval_hours=72):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡ŒIYUUæŸ¥è¯¢ï¼ˆæ ¹æ®è®¾ç½®çš„æ—¶é—´é—´éš”æˆ–ä»æœªæŸ¥è¯¢è¿‡ï¼‰"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)
            ph = self.db_manager.get_placeholder()

            # æŸ¥è¯¢è¯¥ç§å­æœ€è¿‘ä¸€æ¬¡çš„iyuu_last_checkæ—¶é—´
            if self.db_manager.db_type == "postgresql":
                cursor.execute(
                    f"SELECT MAX(iyuu_last_check) as last_check FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            else:
                cursor.execute(
                    f"SELECT MAX(iyuu_last_check) as last_check FROM torrents WHERE name = {ph}",
                    (torrent_name, ))

            result = cursor.fetchone()
            last_check_str = result['last_check'] if isinstance(
                result, dict) else (result[0] if result else None)

            # å¦‚æœä»æœªæŸ¥è¯¢è¿‡ï¼Œåˆ™åº”è¯¥æŸ¥è¯¢
            if not last_check_str:
                return True

            # è§£æä¸Šæ¬¡æŸ¥è¯¢æ—¶é—´
            from datetime import datetime, timedelta
            # å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
            try:
                if isinstance(last_check_str, str):
                    # å°è¯•è§£æå¸¸è§çš„æ—¥æœŸæ—¶é—´æ ¼å¼
                    last_check = datetime.strptime(last_check_str,
                                                   "%Y-%m-%d %H:%M:%S")
                else:
                    last_check = last_check_str
            except ValueError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå‡è®¾éœ€è¦é‡æ–°æŸ¥è¯¢
                return True

            # è®¡ç®—è·ç¦»ç°åœ¨çš„æ—¶é—´å·®
            now = datetime.now()
            time_diff = now - last_check

            # å¦‚æœè¶…è¿‡è®¾ç½®çš„æ—¶é—´é—´éš”ï¼Œåˆ™åº”è¯¥æŸ¥è¯¢
            return time_diff > timedelta(hours=query_interval_hours)

        except Exception as e:
            logging.error(f"æ£€æŸ¥IYUUæŸ¥è¯¢æ¡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)
            # å‡ºé”™æ—¶é»˜è®¤è¿›è¡ŒæŸ¥è¯¢
            return True
        finally:
            if 'cursor' in locals() and cursor:
                cursor.close()
            if 'conn' in locals() and conn:
                conn.close()

    def _update_iyuu_last_check(self, torrent_name, matched_sites,
                                all_torrents_for_name):
        """æ›´æ–°æ‰€æœ‰åŒåç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´ï¼Œå¹¶ä¸ºæ²¡æœ‰detailså†…å®¹çš„è®°å½•å¡«å…¥è¯¦æƒ…é“¾æ¥"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)
            ph = self.db_manager.get_placeholder()

            # è·å–å½“å‰æ—¶é—´
            from datetime import datetime
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # è·å–æ•°æ®åº“ä¸­è¯¥ç§å­çš„æ‰€æœ‰ç°æœ‰è®°å½•
            if self.db_manager.db_type == "postgresql":
                cursor.execute(
                    f"SELECT hash, sites, details FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            else:
                cursor.execute(
                    f"SELECT hash, sites, details FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            existing_records = [dict(row) for row in cursor.fetchall()]

            updated_count = 0
            filled_details_count = 0

            # ä¸ºæ¯æ¡è®°å½•æ›´æ–°iyuu_last_checkæ—¶é—´ï¼Œå¹¶ä¸ºæ²¡æœ‰detailsçš„è®°å½•å¡«å…¥è¯¦æƒ…é“¾æ¥
            for record in existing_records:
                site_name = record['sites']
                current_details = record['details']
                hash_value = record['hash']

                # æ„å»ºæ›´æ–°å‚æ•°
                update_params = [current_time]  # iyuu_last_checkæ—¶é—´
                update_fields = [f"iyuu_last_check = {ph}"]

                # æŸ¥æ‰¾è¯¥ç«™ç‚¹åœ¨matched_sitesä¸­çš„è¯¦æƒ…é“¾æ¥
                matched_site = next(
                    (s for s in matched_sites if s['db_name'] == site_name),
                    None)

                # å¦‚æœå½“å‰è®°å½•æ²¡æœ‰detailsä¸”IYUUè¿”å›äº†è¯¦æƒ…é“¾æ¥ï¼Œåˆ™å¡«å…¥
                if (not current_details
                        or current_details.strip() == '') and matched_site:
                    update_params.append(matched_site['url'])
                    update_fields.append(f"details = {ph}")
                    filled_details_count += 1

                # æ·»åŠ WHEREæ¡ä»¶å‚æ•°
                update_params.extend([hash_value, torrent_name])

                # æ‰§è¡Œæ›´æ–°
                if self.db_manager.db_type == "postgresql":
                    cursor.execute(
                        f"UPDATE torrents SET {', '.join(update_fields)} WHERE hash = {ph} AND name = {ph}",
                        update_params)
                else:
                    cursor.execute(
                        f"UPDATE torrents SET {', '.join(update_fields)} WHERE hash = {ph} AND name = {ph}",
                        update_params)

                updated_count += cursor.rowcount

            conn.commit()
            print(f"ğŸ”„ å·²æ›´æ–° {updated_count} æ¡ç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´")
            if filled_details_count > 0:
                print(f"âœ… å·²ä¸º {filled_details_count} æ¡ç§å­è®°å½•å¡«å…¥è¯¦æƒ…é“¾æ¥")

        except Exception as e:
            logging.error(f"æ›´æ–°ç§å­è®°å½•iyuu_last_checkæ—¶é—´å’Œè¯¦æƒ…é“¾æ¥æ—¶å‡ºé”™: {e}",
                          exc_info=True)
        finally:
            if 'cursor' in locals() and cursor:
                cursor.close()
            if 'conn' in locals() and conn:
                conn.close()

    def _add_missing_site_torrents(self, torrent_name, torrent_data,
                                   matched_sites):
        """ä¸ºç¼ºå¤±ç«™ç‚¹æ·»åŠ ç§å­è®°å½•"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)
            ph = self.db_manager.get_placeholder()

            # è·å–æ•°æ®åº“ä¸­è¯¥ç§å­å·²å­˜åœ¨çš„æ‰€æœ‰ç«™ç‚¹è®°å½•
            if self.db_manager.db_type == "postgresql":
                cursor.execute(
                    f"SELECT hash, sites, save_path, size, \"group\", details, downloader_id, progress, state FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            else:
                cursor.execute(
                    f"SELECT hash, sites, save_path, size, `group`, details, downloader_id, progress, state FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            existing_torrents = [dict(row) for row in cursor.fetchall()]

            # æå–å·²å­˜åœ¨çš„ç«™ç‚¹åˆ—è¡¨
            existing_sites = set()
            for t in existing_torrents:
                site = t['sites']
                if site:
                    if ',' in site:
                        site_list = site.split(',')
                        existing_sites.update(s.strip() for s in site_list
                                              if s.strip())
                    else:
                        existing_sites.add(site.strip())

            # è·å–IYUUè¿”å›çš„ç«™ç‚¹åˆ—è¡¨
            iyuu_sites = {site['db_name'] for site in matched_sites}

            # æ‰¾å‡ºç¼ºå¤±çš„ç«™ç‚¹
            missing_sites = iyuu_sites - existing_sites

            print(
                f"å‘ç° {len(missing_sites)} ä¸ªç¼ºå¤±çš„ç«™ç‚¹: {', '.join(missing_sites)}")

            # ä¸ºæ¯ä¸ªç¼ºå¤±çš„ç«™ç‚¹æ·»åŠ è®°å½•
            for site_name in missing_sites:
                # æ‰¾åˆ°è¯¥ç«™ç‚¹çš„åŒ¹é…ä¿¡æ¯
                matched_site = next(
                    (s for s in matched_sites if s['db_name'] == site_name),
                    None)
                if not matched_site:
                    continue

                # ä½¿ç”¨ç°æœ‰ç§å­ä¿¡æ¯åˆ›å»ºæ–°è®°å½•
                existing_torrent = existing_torrents[
                    0] if existing_torrents else torrent_data

                # è·å–å½“å‰æ—¶é—´
                from datetime import datetime
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                # ä¸ºç¼ºå¤±ç«™ç‚¹çš„ç§å­è®°å½•ç”Ÿæˆå”¯ä¸€hash
                # ä½¿ç”¨åŸå§‹hash+ç«™ç‚¹åç§°+æ—¶é—´æˆ³çš„ç»„åˆæ¥ç”Ÿæˆæ–°çš„å”¯ä¸€hash
                import hashlib
                unique_string = f"{torrent_data['hash']}_{site_name}_{current_time}"
                new_hash = hashlib.sha1(
                    unique_string.encode('utf-8')).hexdigest()

                if self.db_manager.db_type == "postgresql":
                    cursor.execute(
                        f"INSERT INTO torrents (hash, name, save_path, size, progress, state, sites, \"group\", details, downloader_id, last_seen, iyuu_last_check) VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})",
                        (
                            new_hash,  # ä½¿ç”¨æ–°ç”Ÿæˆçš„å”¯ä¸€hash
                            torrent_name,
                            existing_torrent.get('save_path', ''),
                            existing_torrent.get('size', 0),
                            0.0,  # è¿›åº¦è®¾ä¸º0ï¼Œè¡¨ç¤ºæœªä¸‹è½½
                            'æœªåšç§',  # çŠ¶æ€è®¾ä¸ºæœªåšç§ï¼Œè¡¨ç¤ºæœªåœ¨å®¢æˆ·ç«¯ä¸­
                            site_name,
                            existing_torrent.get('group', ''),
                            matched_site['url'],  # ä½¿ç”¨IYUUæä¾›çš„è¯¦æƒ…é“¾æ¥
                            existing_torrent.get('downloader_id', None),
                            current_time,  # last_seenè®¾ä¸ºå½“å‰æ—¶é—´
                            current_time  # iyuu_last_checkè®¾ä¸ºå½“å‰æ—¶é—´
                        ))
                else:
                    cursor.execute(
                        f"INSERT INTO torrents (hash, name, save_path, size, progress, state, sites, `group`, details, downloader_id, last_seen, iyuu_last_check) VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})",
                        (
                            new_hash,  # ä½¿ç”¨æ–°ç”Ÿæˆçš„å”¯ä¸€hash
                            torrent_name,
                            existing_torrent.get('save_path', ''),
                            existing_torrent.get('size', 0),
                            0.0,  # è¿›åº¦è®¾ä¸º0ï¼Œè¡¨ç¤ºæœªä¸‹è½½
                            'æœªåšç§',  # çŠ¶æ€è®¾ä¸ºæœªåšç§ï¼Œè¡¨ç¤ºæœªåœ¨å®¢æˆ·ç«¯ä¸­
                            site_name,
                            existing_torrent.get('group', ''),
                            matched_site['url'],  # ä½¿ç”¨IYUUæä¾›çš„è¯¦æƒ…é“¾æ¥
                            existing_torrent.get('downloader_id', None),
                            current_time,  # last_seenè®¾ä¸ºå½“å‰æ—¶é—´
                            current_time  # iyuu_last_checkè®¾ä¸ºå½“å‰æ—¶é—´
                        ))
                print(f"âœ… å·²ä¸ºç«™ç‚¹ '{site_name}' æ·»åŠ ç§å­è®°å½•")

            conn.commit()
            print(f"æˆåŠŸå¤„ç† {len(missing_sites)} ä¸ªç¼ºå¤±ç«™ç‚¹çš„ç§å­è®°å½•")

        except Exception as e:
            logging.error(f"å¤„ç†ç¼ºå¤±ç«™ç‚¹ç§å­è®°å½•æ—¶å‡ºé”™: {e}", exc_info=True)
        finally:
            if 'cursor' in locals() and cursor:
                cursor.close()
            if 'conn' in locals() and conn:
                conn.close()

    def _process_single_torrent(self, torrent_name, torrent_size, force_query=True):
        """å¤„ç†å•ä¸ªç§å­çš„IYUUæŸ¥è¯¢
        
        Args:
            torrent_name: ç§å­åç§°
            torrent_size: ç§å­å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            force_query: æ˜¯å¦å¼ºåˆ¶æŸ¥è¯¢ï¼Œå¿½ç•¥æ—¶é—´é—´éš”é™åˆ¶ï¼ˆé»˜è®¤Trueï¼‰
        """
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        log_iyuu_message(f"[{current_time}] å¼€å§‹æ‰§è¡Œå•ä¸ªç§å­çš„IYUUæŸ¥è¯¢: {torrent_name} (å¤§å°: {torrent_size} å­—èŠ‚)", "INFO")
        if force_query:
            log_iyuu_message("å¼ºåˆ¶æŸ¥è¯¢æ¨¡å¼ï¼šå¿½ç•¥æ—¶é—´é—´éš”é™åˆ¶", "INFO")
        
        conn = None
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)
            ph = self.db_manager.get_placeholder()
            
            # æŸ¥è¯¢æŒ‡å®šåç§°å’Œå¤§å°çš„ç§å­æ•°æ®ï¼Œåªç­›é€‰ä½“ç§¯å¤§äº200MBçš„ç§å­
            cursor.execute(
                f"SELECT hash, name, sites, size, save_path FROM torrents WHERE name = {ph} AND size = {ph} AND size > 207374182",
                (torrent_name, torrent_size)
            )
            torrents_raw = [dict(row) for row in cursor.fetchall()]
            
            if not torrents_raw:
                log_iyuu_message(f"æœªæ‰¾åˆ°ç§å­: {torrent_name} (å¤§å°: {torrent_size})", "WARNING")
                return
            
            # è·å–é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            configured_sites = self._get_configured_sites()
            
            # æŒ‰ç§å­åç§°è¿›è¡Œèšåˆ
            all_torrents = defaultdict(list)
            agg_torrents = defaultdict(list)
            
            for t in torrents_raw:
                site = t.get('sites', None)
                torrent_info = {
                    'hash': t['hash'],
                    'sites': site,
                    'size': t.get('size', 0),
                    'save_path': t.get('save_path', '')
                }
                all_torrents[torrent_name].append(torrent_info)
                
                # åªæœ‰å½“ç«™ç‚¹æ˜¯IYUUæ”¯æŒçš„ç«™ç‚¹æ—¶æ‰æ·»åŠ åˆ°èšåˆåˆ—è¡¨ä¸­
                if site and site in configured_sites and site not in ['é’è›™', 'æŸ æª¬ä¸ç”œ']:
                    agg_torrents[torrent_name].append(torrent_info)
            
            if not agg_torrents:
                log_iyuu_message(f"ç§å­ '{torrent_name}' æ²¡æœ‰æ”¯æŒçš„ç«™ç‚¹å¯ç”¨äºIYUUæŸ¥è¯¢", "WARNING")
                return
            
            log_iyuu_message(f"æ‰¾åˆ°ç§å­ '{torrent_name}'ï¼ŒåŒ…å« {len(agg_torrents[torrent_name])} ä¸ªæ”¯æŒçš„ç«™ç‚¹", "INFO")
            
            # è·å–å·²é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            log_iyuu_message(f"æ•°æ®åº“ä¸­å­˜åœ¨ {len(configured_sites)} ä¸ªé…ç½®ç«™ç‚¹", "INFO")
            
            # æ‰§è¡ŒIYUUæœç´¢é€»è¾‘ï¼Œä¼ å…¥force_queryå‚æ•°
            self._perform_iyuu_search(agg_torrents, configured_sites, all_torrents, force_query=force_query)
            
            log_iyuu_message(f"=== ç§å­ '{torrent_name}' çš„IYUUæŸ¥è¯¢ä»»åŠ¡æ‰§è¡Œå®Œæˆ ===", "INFO")
            
        except Exception as e:
            logging.error(f"å¤„ç†å•ä¸ªç§å­æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            log_iyuu_message(f"å¤„ç†ç§å­æ—¶å‡ºé”™: {str(e)}", "ERROR")
        finally:
            if conn:
                if 'cursor' in locals() and cursor:
                    cursor.close()
                conn.close()

    def stop(self):
        """åœæ­¢çº¿ç¨‹"""
        print("æ­£åœ¨åœæ­¢ IYUUThread çº¿ç¨‹...")
        self._is_running = False


# --- IYUU API é…ç½® ---
API_BASE = "https://2025.iyuu.cn"
CLIENT_VERSION = "8.2.0"

# --- è¯·æ±‚é¢‘ç‡æ§åˆ¶ ---
_last_request_time = 0
_rate_limit_delay = 5.0  # è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰

# --- IYUU API è¾…åŠ©å‡½æ•° ---


def get_sha1_hex(text: str) -> str:
    """è®¡ç®—å­—ç¬¦ä¸²çš„ SHA-1 å“ˆå¸Œå€¼"""
    return hashlib.sha1(text.encode('utf-8')).hexdigest()


def make_api_request(method: str, url: str, token: str, **kwargs) -> dict:
    """
    å°è£… API è¯·æ±‚ï¼Œç»Ÿä¸€å¤„ç† headers å’Œé”™è¯¯ã€‚
    ã€å·²ä¿®æ­£ã€‘æ­¤å‡½æ•°ç°åœ¨èƒ½æ­£ç¡®åˆå¹¶ headersã€‚
    """
    global _last_request_time, _rate_limit_delay

    # è¯·æ±‚é¢‘ç‡æ§åˆ¶ - ç¡®ä¿è¯·æ±‚ä¹‹é—´æœ‰é€‚å½“çš„å»¶è¿Ÿ
    current_time = time.time()
    time_since_last_request = current_time - _last_request_time
    if time_since_last_request < _rate_limit_delay:
        sleep_time = _rate_limit_delay - time_since_last_request
        print(f"è¯·æ±‚é¢‘ç‡æ§åˆ¶: ç­‰å¾… {sleep_time:.2f} ç§’")
        time.sleep(sleep_time)

    # æ›´æ–°æœ€åè¯·æ±‚æ—¶é—´
    _last_request_time = time.time()

    # åŸºç¡€ headersï¼ŒåŒ…å« Token
    final_headers = {'Token': token}

    # å¦‚æœè°ƒç”¨æ—¶ä¼ å…¥äº†é¢å¤–çš„ headers (å¦‚ Content-Type)ï¼Œåˆ™è¿›è¡Œåˆå¹¶
    if 'headers' in kwargs:
        # ä½¿ç”¨ update æ–¹æ³•å°†ä¼ å…¥çš„ headers åˆå¹¶è¿›æ¥
        final_headers.update(kwargs.pop('headers'))

    try:
        if method.upper() == 'GET':
            response = requests.get(url,
                                    headers=final_headers,
                                    timeout=20,
                                    **kwargs)
        elif method.upper() == 'POST':
            response = requests.post(url,
                                     headers=final_headers,
                                     timeout=20,
                                     **kwargs)
        else:
            raise ValueError("Unsupported HTTP method")

        response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

        data = response.json()
        if data.get("code") != 0:
            error_msg = data.get("msg", "æœªçŸ¥ API é”™è¯¯")
            raise Exception(f"API é”™è¯¯: {error_msg} (ä»£ç : {data.get('code')})")

        return data

    except requests.exceptions.RequestException as e:
        raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except json.JSONDecodeError:
        raise Exception("æ— æ³•è§£ææœåŠ¡å™¨è¿”å›çš„ JSON æ•°æ®")


def get_supported_sites(token: str) -> list:
    """è·å– IYUU æ”¯æŒçš„æ‰€æœ‰å¯è¾…ç§ç«™ç‚¹åˆ—è¡¨"""
    print("æ­£åœ¨è·å– IYUU æ”¯æŒçš„å¯è¾…ç§ç«™ç‚¹åˆ—è¡¨...")
    url = f"{API_BASE}/reseed/sites/index"
    response_data = make_api_request("GET", url, token)
    sites = response_data.get("data", {}).get("sites", [])
    print(f"ä»APIè·å–åˆ°çš„å¯è¾…ç§ç«™ç‚¹æ•°é‡: {len(sites) if response_data.get('data') else 0}")
    if not sites:
        raise Exception("æœªèƒ½è·å–åˆ°å¯è¾…ç§ç«™ç‚¹åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ Token æˆ– IYUU æœåŠ¡çŠ¶æ€ã€‚")
    print(f"æˆåŠŸè·å–åˆ° {len(sites)} ä¸ªå¯è¾…ç§ç«™ç‚¹ä¿¡æ¯ã€‚")
    return sites


def get_sid_sha1(token: str, all_sites: list) -> str:
    """æ ¹æ®ç«™ç‚¹åˆ—è¡¨ä¸ŠæŠ¥å¹¶è·å– sid_sha1"""
    print("æ­£åœ¨ç”Ÿæˆç«™ç‚¹æ ¡éªŒå“ˆå¸Œ (sid_sha1)...")
    print(f"æ¥æ”¶åˆ°çš„ç«™ç‚¹æ•°é‡: {len(all_sites)}")

    # æ‰“å°å‰å‡ ä¸ªç«™ç‚¹çš„ä¿¡æ¯ç”¨äºè°ƒè¯•
    for i, site in enumerate(all_sites[:5]):
        print(f"ç«™ç‚¹ {i+1}: ID={site.get('id')}, åç§°={site.get('nickname')}")

    site_ids = [site['id'] for site in all_sites]
    print(f"æå–çš„ç«™ç‚¹IDæ•°é‡: {len(site_ids)}")

    if not site_ids:
        raise Exception("ç«™ç‚¹IDåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆsid_sha1")

    payload = {"sid_list": site_ids}
    print(f"å‘é€çš„payload: {payload}")

    url = f"{API_BASE}/reseed/sites/reportExisting"

    # è¿™é‡Œçš„ headers ä¼šè¢«æ­£ç¡®åˆå¹¶
    headers = {'Content-Type': 'application/json'}
    response_data = make_api_request("POST",
                                     url,
                                     token,
                                     json=payload,
                                     headers=headers)

    sid_sha1 = response_data.get("data", {}).get("sid_sha1")
    if not sid_sha1:
        raise Exception("æœªèƒ½ä» API è·å– sid_sha1ã€‚")
    print("æˆåŠŸç”Ÿæˆ sid_sha1ã€‚")
    return sid_sha1


def get_filtered_sid_sha1_and_sites(token: str, db_manager) -> tuple:
    """è·å–è¿‡æ»¤åçš„sid_sha1å’Œç«™ç‚¹åˆ—è¡¨ï¼ŒåªåŒ…å«åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹"""
    print("=== å¼€å§‹è·å–è¿‡æ»¤åçš„sid_sha1å’Œç«™ç‚¹åˆ—è¡¨ ===")

    # 1. è·å–IYUUæ”¯æŒçš„æ‰€æœ‰å¯è¾…ç§ç«™ç‚¹
    try:
        supported_sites = get_supported_sites(token)
        print(f"è·å–åˆ° {len(supported_sites)} ä¸ªIYUUæ”¯æŒçš„å¯è¾…ç§ç«™ç‚¹")
    except Exception as e:
        logging.error(f"è·å–IYUUæ”¯æŒç«™ç‚¹åˆ—è¡¨å¤±è´¥: {e}")
        raise

    # åœ¨è·å–ç«™ç‚¹åˆ—è¡¨å’Œåç»­è¯·æ±‚ä¹‹é—´æ·»åŠ é¢å¤–å»¶è¿Ÿ
    print("ç­‰å¾…é¢å¤–å»¶è¿Ÿä»¥é¿å…è¯·æ±‚é¢‘ç‡è¿‡å¿«...")
    time.sleep(2)

    # 2. è·å–torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹åˆ—è¡¨
    try:
        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)

        # æŸ¥è¯¢torrentsè¡¨ä¸­æ‰€æœ‰ä¸åŒçš„ç«™ç‚¹
        cursor.execute(
            "SELECT DISTINCT sites FROM torrents WHERE sites IS NOT NULL AND sites != ''"
        )
        sites_result = cursor.fetchall()

        # æå–ç«™ç‚¹åç§°å¹¶å»é‡
        torrent_sites = set()
        for row in sites_result:
            site = row['sites'] if isinstance(row, dict) else row[0]
            if site:
                # å¦‚æœç«™ç‚¹å­—æ®µåŒ…å«å¤šä¸ªç«™ç‚¹ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼Œåˆ™åˆ†å‰²å®ƒä»¬
                if ',' in site:
                    site_list = site.split(',')
                    torrent_sites.update(s.strip() for s in site_list
                                         if s.strip())
                else:
                    torrent_sites.add(site.strip())

        cursor.close()
        conn.close()

        torrent_sites_list = list(torrent_sites)
        print(f"torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹æ•°é‡: {len(torrent_sites_list)}")
        print(f"ç«™ç‚¹åˆ—è¡¨: {', '.join(torrent_sites_list)}")

    except Exception as e:
        logging.error(f"è·å–torrentsè¡¨ä¸­çš„ç«™ç‚¹ä¿¡æ¯æ—¶å‡ºé”™: {e}", exc_info=True)
        raise

    # 3. ä½¿ç”¨site_name_mappingæ˜ å°„æ›¿æ¢nickname
    site_name_mapping = {
        # IYUUåç§° -> æ•°æ®åº“æ˜µç§°
        "ä¼˜å ¡": "æˆ‘å ¡",
        "è§‚ä¼—": "äººäºº",
        "æŸ æª¬": "æŸ æª¬ä¸é…¸",
        "hdclone": "HDClone",
        "æˆ‘çš„PT(CC)": "æˆ‘çš„PT",
        "LongPT": "é¾™PT",
        "March": "ä¸‰æœˆä¼ åª’",
        "hdbao": "çº¢è±†åŒ…",
        "LuckPT": "å¹¸è¿",
        "13city": "13City",
        "PTSKit": "PTSkit",
        "æ—¶å…‰": "æ—¶å…‰HDT",
        "æ˜¥å¤©": "ä¸å¯è¯´",
    }

    # åˆ›å»ºåå‘æ˜ å°„ (æ•°æ®åº“æ˜µç§° -> IYUUåç§°)
    reverse_mapping = {v: k for k, v in site_name_mapping.items()}

    # 4. è¿‡æ»¤å‡ºIYUUæ”¯æŒä¸”åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹
    filtered_site_ids = []
    site_id_mapping = {}  # ç”¨äºå­˜å‚¨ç«™ç‚¹IDæ˜ å°„
    filtered_sites = []  # ç”¨äºå­˜å‚¨è¿‡æ»¤åçš„ç«™ç‚¹ä¿¡æ¯

    for site in supported_sites:
        iyuu_nickname = site.get('nickname')
        iyuu_id = site.get('id')

        if not iyuu_nickname or not iyuu_id:
            continue

        # æ£€æŸ¥ç«™ç‚¹æ˜¯å¦åœ¨torrentsè¡¨ä¸­å­˜åœ¨
        # å…ˆæ£€æŸ¥åŸå§‹åç§°
        if iyuu_nickname in torrent_sites_list:
            filtered_site_ids.append(iyuu_id)
            site_id_mapping[iyuu_nickname] = iyuu_id
            filtered_sites.append(site)
            continue

        # å†æ£€æŸ¥æ˜ å°„åçš„åç§°
        if iyuu_nickname in site_name_mapping:
            db_nickname = site_name_mapping[iyuu_nickname]
            if db_nickname in torrent_sites_list:
                filtered_site_ids.append(iyuu_id)
                site_id_mapping[db_nickname] = iyuu_id
                filtered_sites.append(site)
                continue

        # æ£€æŸ¥åå‘æ˜ å°„ (æ•°æ®åº“ä¸­çš„åç§°æ˜¯å¦éœ€è¦æ˜ å°„åˆ°IYUU)
        for db_site in torrent_sites_list:
            if db_site in reverse_mapping and reverse_mapping[
                    db_site] == iyuu_nickname:
                filtered_site_ids.append(iyuu_id)
                site_id_mapping[db_site] = iyuu_id
                filtered_sites.append(site)
                break

    print(f"è¿‡æ»¤åå¾—åˆ° {len(filtered_site_ids)} ä¸ªæ”¯æŒçš„ç«™ç‚¹ID")
    print(f"ç«™ç‚¹IDåˆ—è¡¨: {filtered_site_ids}")

    if not filtered_site_ids:
        raise Exception("æ²¡æœ‰æ‰¾åˆ°åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„IYUUæ”¯æŒç«™ç‚¹")

    # åœ¨å‘é€reportExistingè¯·æ±‚å‰æ·»åŠ é¢å¤–å»¶è¿Ÿ
    print("å‡†å¤‡å‘é€reportExistingè¯·æ±‚ï¼Œç­‰å¾…é¢å¤–å»¶è¿Ÿ...")
    time.sleep(2)

    # 5. æ„å»ºsid_sha1
    try:
        payload = {"sid_list": filtered_site_ids}
        url = f"{API_BASE}/reseed/sites/reportExisting"

        headers = {'Content-Type': 'application/json'}
        response_data = make_api_request("POST",
                                         url,
                                         token,
                                         json=payload,
                                         headers=headers)

        sid_sha1 = response_data.get("data", {}).get("sid_sha1")
        if not sid_sha1:
            raise Exception("æœªèƒ½ä» API è·å– sid_sha1ã€‚")
        print(f"æˆåŠŸç”Ÿæˆè¿‡æ»¤åçš„ sid_sha1: {sid_sha1}")
        return sid_sha1, filtered_sites
    except Exception as e:
        logging.error(f"ç”Ÿæˆsid_sha1æ—¶å‡ºé”™: {e}")
        raise


def query_cross_seed(token: str, infohash: str, sid_sha1: str) -> list:
    """æŸ¥è¯¢æŒ‡å®š infohash çš„è¾…ç§ä¿¡æ¯"""
    print(f"æ­£åœ¨ä¸ºç§å­ {infohash[:8]}... æŸ¥è¯¢è¾…ç§ä¿¡æ¯...")
    url = f"{API_BASE}/reseed/index/index"

    hashes_json_str = json.dumps([infohash.lower()])
    form_data = {
        "hash": hashes_json_str,
        "sha1": get_sha1_hex(hashes_json_str),
        "sid_sha1": sid_sha1,
        "timestamp": str(int(time.time())),
        "version": CLIENT_VERSION
    }

    # è¿™é‡Œçš„ headers ä¹Ÿä¼šè¢«æ­£ç¡®åˆå¹¶
    headers = {'Content-Type': 'application/x-www-form-urlencoded'}
    response_data = make_api_request("POST",
                                     url,
                                     token,
                                     data=form_data,
                                     headers=headers)

    data = response_data.get("data", {})
    if not data or infohash.lower() not in data:
        return []

    results = data[infohash.lower()].get("torrent", [])
    return results


# å…¨å±€å˜é‡
iyuu_thread = None

# IYUUæ—¥å¿—å­˜å‚¨
iyuu_logs = []


def log_iyuu_message(message, level="INFO"):
    """è®°å½•IYUUæ—¥å¿—æ¶ˆæ¯"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = {
        "timestamp": timestamp,
        "level": level,
        "message": message
    }
    iyuu_logs.append(log_entry)

    # é™åˆ¶æ—¥å¿—æ•°é‡ï¼Œåªä¿ç•™æœ€è¿‘100æ¡
    if len(iyuu_logs) > 100:
        iyuu_logs.pop(0)

    # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    print(f"[IYUU-{level}] {timestamp} {message}")


def start_iyuu_thread(db_manager, config_manager):
    """åˆå§‹åŒ–å¹¶å¯åŠ¨å…¨å±€ IYUUThread çº¿ç¨‹å®ä¾‹ã€‚"""
    global iyuu_thread
    # æ£€æŸ¥æ˜¯å¦åœ¨è°ƒè¯•æ¨¡å¼ä¸‹è¿è¡Œï¼Œé¿å…é‡å¤å¯åŠ¨
    import os
    if os.environ.get('WERKZEUG_RUN_MAIN') != 'true':
        # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼Œè¿™æ˜¯ç›‘æ§è¿›ç¨‹ï¼Œä¸éœ€è¦å¯åŠ¨çº¿ç¨‹
        print("æ£€æµ‹åˆ°è°ƒè¯•ç›‘æ§è¿›ç¨‹ï¼Œè·³è¿‡IYUUçº¿ç¨‹å¯åŠ¨ã€‚")
        return iyuu_thread

    if iyuu_thread is None or not iyuu_thread.is_alive():
        iyuu_thread = IYUUThread(db_manager, config_manager)
        iyuu_thread.start()
        print("å·²åˆ›å»ºå¹¶å¯åŠ¨æ–°çš„ IYUUThread å®ä¾‹ã€‚")
    return iyuu_thread


def stop_iyuu_thread():
    """åœæ­¢å¹¶æ¸…ç†å½“å‰çš„ IYUUThread çº¿ç¨‹å®ä¾‹ã€‚"""
    global iyuu_thread
    if iyuu_thread and iyuu_thread.is_alive():
        iyuu_thread.stop()
        iyuu_thread.join(timeout=10)
        print("IYUUThread çº¿ç¨‹å·²åœæ­¢ã€‚")
    iyuu_thread = None
