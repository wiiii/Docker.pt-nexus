# core/iyuu.py
import logging
import time
import os
import requests
import json
import hashlib
from threading import Thread
from collections import defaultdict
from datetime import datetime, timedelta


class IYUUThread(Thread):
    """IYUUåå°çº¿ç¨‹ï¼Œå®šæœŸèšåˆç§å­ä¿¡æ¯å¹¶è¿›è¡Œç›¸å…³å¤„ç†ã€‚"""

    def __init__(self, db_manager, config_manager):
        super().__init__(daemon=True, name="IYUUThread")
        self.db_manager = db_manager
        self.config_manager = config_manager
        self._is_running = True
        # è®¾ç½®ä¸º6å°æ—¶è¿è¡Œä¸€æ¬¡
        self.interval = 21600  # 6å°æ—¶

    def run(self):
        print("IYUUThread çº¿ç¨‹å·²å¯åŠ¨ï¼Œæ¯6å°æ—¶æ‰§è¡Œä¸€æ¬¡æŸ¥è¯¢ä»»åŠ¡ã€‚")
        # ç­‰å¾…5ç§’å†å¼€å§‹æ‰§è¡Œï¼Œé¿å…ä¸ä¸»ç¨‹åºå¯åŠ¨å†²çª
        time.sleep(5)

        while self._is_running:
            start_time = time.monotonic()
            try:
                self._process_torrents()
            except Exception as e:
                logging.error(f"IYUUThread æ‰§è¡Œå‡ºé”™: {e}", exc_info=True)

            # ç­‰å¾…ä¸‹æ¬¡æ‰§è¡Œ
            elapsed = time.monotonic() - start_time
            time.sleep(max(0, self.interval - elapsed))

    def _get_configured_sites(self):
        """è·å–torrentsè¡¨ä¸­å·²å­˜åœ¨çš„ç«™ç‚¹åˆ—è¡¨"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)

            # æŸ¥è¯¢torrentsè¡¨ä¸­æ‰€æœ‰ä¸åŒçš„ç«™ç‚¹
            cursor.execute(
                "SELECT DISTINCT sites FROM torrents WHERE sites IS NOT NULL AND sites != ''"
            )
            sites_result = cursor.fetchall()

            # æå–ç«™ç‚¹åç§°å¹¶å»é‡
            sites = set()
            for row in sites_result:
                site = row['sites']
                if site:
                    # å¦‚æœç«™ç‚¹å­—æ®µåŒ…å«å¤šä¸ªç«™ç‚¹ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼Œåˆ™åˆ†å‰²å®ƒä»¬
                    if ',' in site:
                        site_list = site.split(',')
                        sites.update(s.strip() for s in site_list if s.strip())
                    else:
                        sites.add(site.strip())

            cursor.close()
            conn.close()

            sites_list = list(sites)
            log_iyuu_message(f"è·å–åˆ° {len(sites_list)} ä¸ªå·²å­˜åœ¨çš„ç«™ç‚¹", "INFO")
            log_iyuu_message(f"ç«™ç‚¹åˆ—è¡¨: {', '.join(sites_list)}", "INFO")

            return sites_list
        except Exception as e:
            logging.error(f"è·å–torrentsè¡¨ä¸­çš„ç«™ç‚¹ä¿¡æ¯æ—¶å‡ºé”™: {e}", exc_info=True)
            return []

    def _process_torrents(self, is_manual_trigger=False):
        """å¤„ç†ç§å­æ•°æ®ï¼ŒæŒ‰nameåˆ—è¿›è¡Œèšåˆ"""
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨æŸ¥è¯¢ï¼ˆä»…åœ¨è‡ªåŠ¨è§¦å‘æ—¶æ£€æŸ¥ï¼‰
        if not is_manual_trigger:
            config = self.config_manager.get()
            iyuu_settings = config.get("iyuu_settings", {})
            auto_query_enabled = iyuu_settings.get("auto_query_enabled", True)

            # å¦‚æœæœªå¯ç”¨è‡ªåŠ¨æŸ¥è¯¢ï¼Œåˆ™è·³è¿‡
            if not auto_query_enabled:
                log_iyuu_message("IYUUè‡ªåŠ¨æŸ¥è¯¢å·²ç¦ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡æŸ¥è¯¢ä»»åŠ¡", "INFO")
                return

        log_iyuu_message(f"[{current_time}] å¼€å§‹æ‰§è¡ŒIYUUç§å­èšåˆä»»åŠ¡", "INFO")
        conn = None
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)

            # æŸ¥è¯¢æ‰€æœ‰ç§å­æ•°æ®ï¼Œåªç­›é€‰ä½“ç§¯å¤§äº1GBçš„ç§å­ï¼ˆ1GB = 1073741824å­—èŠ‚ï¼‰
            cursor.execute(
                "SELECT hash, name, sites, size FROM torrents WHERE name IS NOT NULL AND name != '' AND size > 207374182"
            )
            torrents_raw = [dict(row) for row in cursor.fetchall()]

            # è·å–é…ç½®çš„ç«™ç‚¹åˆ—è¡¨ï¼ˆç”¨äºè¿‡æ»¤æ”¯æŒçš„ç«™ç‚¹ï¼‰
            configured_sites = self._get_configured_sites()

            # æŒ‰ç§å­åç§°è¿›è¡Œèšåˆï¼Œè®°å½•æ‰€æœ‰åŒåç§å­ï¼ˆåŒ…æ‹¬ä¸æ”¯æŒIYUUçš„ç«™ç‚¹ï¼‰
            all_torrents = defaultdict(list)
            for t in torrents_raw:
                torrent_name = t['name']
                site = t.get('sites', None)
                all_torrents[torrent_name].append({
                    'hash': t['hash'],
                    'sites': site,
                    'size': t.get('size', 0)
                })

            # ä¸ºèšåˆåˆ›å»ºä¸€ä¸ªåªåŒ…å«æ”¯æŒç«™ç‚¹çš„ç‰ˆæœ¬ï¼ˆç”¨äºé€‰æ‹©hashè¿›è¡ŒæŸ¥è¯¢ï¼‰
            agg_torrents = defaultdict(list)
            for t in torrents_raw:
                torrent_name = t['name']
                site = t.get('sites', None)
                # åªæœ‰å½“ç«™ç‚¹æ˜¯IYUUæ”¯æŒçš„ç«™ç‚¹æ—¶æ‰æ·»åŠ åˆ°èšåˆåˆ—è¡¨ä¸­ï¼ˆç”¨äºé€‰æ‹©hashï¼‰
                # è¿‡æ»¤æ‰é’è›™å’ŒæŸ æª¬ä¸¤ä¸ªç«™ç‚¹
                if site and site in configured_sites and site not in [
                        'é’è›™', 'æŸ æª¬ä¸ç”œ'
                ]:
                    agg_torrents[torrent_name].append({
                        'hash': t['hash'],
                        'sites': site,
                        'size': t.get('size', 0)
                    })

            # å‡†å¤‡å†™å…¥æ–‡ä»¶çš„å†…å®¹
            output_lines = []
            output_lines.append("=== IYUUç§å­èšåˆç»“æœ ===\n")
            output_lines.append(
                f"èšåˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            output_lines.append(f"å…±èšåˆäº† {len(agg_torrents)} ä¸ªå”¯ä¸€ç§å­ç»„\n")
            output_lines.append("=" * 50 + "\n")

            # ç”Ÿæˆèšåˆç»“æœ
            for name, torrents in agg_torrents.items():
                # é€‰æ‹©ä¸€ä¸ªhashç”¨äºåç»­çš„IYUUæœç´¢
                selected_hash = torrents[0]['hash'] if torrents else None
                sites_list = [t['sites'] for t in torrents if t['sites']]

                # æ·»åŠ èšåˆä¿¡æ¯åˆ°è¾“å‡ºå†…å®¹
                output_lines.append(f"[IYUU] ç§å­ç»„: {name}\n")
                output_lines.append(f"  - åŒ…å« {len(torrents)} ä¸ªç§å­\n")
                output_lines.append(f"  - é€‰æ‹©çš„hash: {selected_hash}\n")
                output_lines.append(
                    f"  - å­˜åœ¨äºç«™ç‚¹: {', '.join(sites_list) if sites_list else 'æ— '}\n"
                )
                output_lines.append("---\n")

            output_lines.append("=" * 50 + "\n")
            output_lines.append("=== IYUUç§å­èšåˆä»»åŠ¡æ‰§è¡Œå®Œæˆ ===\n")

            # è·å–å·²é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            configured_sites = self._get_configured_sites()
            log_iyuu_message(f"æ•°æ®åº“ä¸­å­˜åœ¨ {len(configured_sites)} ä¸ªé…ç½®ç«™ç‚¹", "INFO")

            # æ‰§è¡ŒIYUUæœç´¢é€»è¾‘ï¼Œä¼ é€’å·²é…ç½®çš„ç«™ç‚¹åˆ—è¡¨å’Œæ‰€æœ‰ç§å­ä¿¡æ¯
            self._perform_iyuu_search(agg_torrents, configured_sites,
                                      all_torrents)

            log_iyuu_message("=== IYUUç§å­èšåˆä»»åŠ¡æ‰§è¡Œå®Œæˆ ===", "INFO")

        except Exception as e:
            logging.error(f"å¤„ç†ç§å­æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
        finally:
            if conn:
                if 'cursor' in locals() and cursor:
                    cursor.close()
                conn.close()

    def _get_existing_sites(self):
        """è·å–æ•°æ®åº“ä¸­é…ç½®çš„ç«™ç‚¹ä¿¡æ¯"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)

            # æŸ¥è¯¢æ‰€æœ‰å·²é…ç½®çš„ç«™ç‚¹
            cursor.execute("SELECT nickname, base_url, site FROM sites")
            sites = {}
            for row in cursor.fetchall():
                # ä»¥æ˜µç§°ä¸ºé”®å­˜å‚¨ç«™ç‚¹ä¿¡æ¯
                site_data = dict(row)
                sites[site_data['nickname']] = site_data

            cursor.close()
            conn.close()

            return sites
        except Exception as e:
            logging.error(f"è·å–æ•°æ®åº“ç«™ç‚¹ä¿¡æ¯æ—¶å‡ºé”™: {e}", exc_info=True)
            return {}

    def _get_priority_hash_for_torrent_group(self, torrent_name,
                                             all_torrents_for_name,
                                             configured_sites):
        """ä¸ºç§å­ç»„è·å–ä¼˜å…ˆä½¿ç”¨çš„hashï¼Œä¼˜å…ˆé€‰æ‹©IYUUæ”¯æŒç«™ç‚¹çš„hash
        
        Args:
            torrent_name: ç§å­åç§°
            all_torrents_for_name: åŒåç§å­çš„æ‰€æœ‰è®°å½•
            configured_sites: é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            
        Returns:
            tuple: (ä¼˜å…ˆhashåˆ—è¡¨, è¿‡æ»¤åçš„ç§å­åˆ—è¡¨)
        """
        # è·å–IYUUæ”¯æŒçš„ç«™ç‚¹åˆ—è¡¨
        try:
            config = self.config_manager.get()
            iyuu_token = config.get("iyuu_token", "")
            if not iyuu_token:
                return [], []

            # è·å–IYUUæ”¯æŒçš„ç«™ç‚¹ä¿¡æ¯
            sid_sha1, all_sites = get_filtered_sid_sha1_and_sites(
                iyuu_token, self.db_manager)

            # åˆ›å»ºIYUUç«™ç‚¹æ˜ å°„
            iyuu_supported_sites = set()
            for site in all_sites:
                iyuu_nickname = site.get('nickname')
                if iyuu_nickname:
                    iyuu_supported_sites.add(iyuu_nickname)

            # è¿‡æ»¤å‡ºIYUUæ”¯æŒçš„ç«™ç‚¹ç§å­ï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§æ’åº
            iyuu_supported_torrents = []
            other_torrents = []

            for torrent in all_torrents_for_name:
                site_name = torrent.get('sites')
                if (site_name and site_name in configured_sites
                        and site_name in iyuu_supported_sites
                        and site_name not in ['é’è›™', 'æŸ æª¬ä¸ç”œ']):
                    iyuu_supported_torrents.append(torrent)
                elif site_name and site_name in configured_sites and site_name not in [
                        'é’è›™', 'æŸ æª¬ä¸ç”œ'
                ]:
                    other_torrents.append(torrent)

            # åˆå¹¶åˆ—è¡¨ï¼ŒIYUUæ”¯æŒçš„ç«™ç‚¹åœ¨å‰
            priority_torrents = iyuu_supported_torrents + other_torrents

            # æå–hashåˆ—è¡¨
            priority_hashes = [t['hash'] for t in priority_torrents]

            log_iyuu_message(
                f"ç§å­ç»„ '{torrent_name}': æ‰¾åˆ° {len(iyuu_supported_torrents)} ä¸ªIYUUæ”¯æŒç«™ç‚¹ï¼Œ{len(other_torrents)} ä¸ªå…¶ä»–æ”¯æŒç«™ç‚¹",
                "INFO")

            return priority_hashes, priority_torrents

        except Exception as e:
            logging.error(f"è·å–ä¼˜å…ˆhashæ—¶å‡ºé”™: {e}", exc_info=True)
            # å‡ºé”™æ—¶è¿”å›æ‰€æœ‰æ”¯æŒç«™ç‚¹çš„hash
            filtered_torrents = [
                t for t in all_torrents_for_name
                if t.get('sites') and t['sites'] in configured_sites
                and t['sites'] not in ['é’è›™', 'æŸ æª¬ä¸ç”œ']
            ]
            return [t['hash'] for t in filtered_torrents], filtered_torrents

    def _perform_iyuu_search(self,
                             agg_torrents,
                             configured_sites,
                             all_torrents,
                             force_query=False,
                             return_stats=False):
        """æ‰§è¡ŒIYUUæœç´¢é€»è¾‘
        
        Args:
            agg_torrents: èšåˆçš„ç§å­æ•°æ®
            configured_sites: é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            all_torrents: æ‰€æœ‰ç§å­æ•°æ®
            force_query: æ˜¯å¦å¼ºåˆ¶æŸ¥è¯¢ï¼Œå¿½ç•¥æ—¶é—´é—´éš”é™åˆ¶ï¼ˆé»˜è®¤Falseï¼‰
            return_stats: æ˜¯å¦è¿”å›ç»Ÿè®¡ä¿¡æ¯ï¼ˆé»˜è®¤Falseï¼‰
            
        Returns:
            dict: å¦‚æœreturn_statsä¸ºTrueï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯ï¼›å¦åˆ™è¿”å›None
        """
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        result_stats = {
            'total_found': 0,
            'new_records': 0,
            'updated_records': 0,
            'sites_found': []
        }

        try:
            # è·å–IYUU token
            config = self.config_manager.get()
            iyuu_token = config.get("iyuu_token", "")

            if not iyuu_token:
                logging.warning("IYUU Tokenæœªé…ç½®ï¼Œè·³è¿‡IYUUæœç´¢ã€‚")
                return result_stats if return_stats else None

            print(f"å¼€å§‹æ‰§è¡ŒIYUUæœç´¢ï¼Œå…± {len(agg_torrents)} ä¸ªç§å­ç»„")

            # è·å–è¿‡æ»¤åçš„sid_sha1å’Œç«™ç‚¹åˆ—è¡¨ï¼ŒåªåŒ…å«åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹
            sid_sha1, all_sites = get_filtered_sid_sha1_and_sites(
                iyuu_token, self.db_manager)

            # åˆ›å»ºç«™ç‚¹æ˜ å°„
            sites_map = {site['id']: site for site in all_sites}

            # ä»æ•°æ®åº“åŠ¨æ€åˆ›å»º IYUU 'site' å­—æ®µåˆ°æœ¬åœ° 'nickname' çš„æ˜ å°„
            try:
                conn = self.db_manager._get_connection()
                cursor = self.db_manager._get_cursor(conn)
                cursor.execute(
                    "SELECT site, nickname FROM sites WHERE site IS NOT NULL AND site != '' AND nickname IS NOT NULL AND nickname != ''"
                )
                # ä¸»è¦æ˜ å°„ï¼šIYUU API 'site' field -> local 'nickname'
                iyuu_site_to_db_nickname_map = {
                    row['site']: row['nickname']
                    for row in cursor.fetchall()
                }
                cursor.close()
                conn.close()
            except Exception as e:
                logging.error(f"ä»æ•°æ®åº“è·å–ç«™ç‚¹æ˜ å°„æ—¶å‡ºé”™: {e}", exc_info=True)
                iyuu_site_to_db_nickname_map = {}

            # è·å–æ•°æ®åº“ä¸­ç°æœ‰çš„ç«™ç‚¹ä¿¡æ¯ (keyed by nickname)
            existing_sites = self._get_existing_sites()
            print(f"æ•°æ®åº“ä¸­å­˜åœ¨ {len(existing_sites)} ä¸ªé…ç½®ç«™ç‚¹")

            # å¤„ç†æ‰€æœ‰ç§å­ç»„
            test_torrents = list(agg_torrents.items())

            # è·å–æ€»ç§å­ç»„æ•°
            total_torrents = len(test_torrents)

            for i, (name, torrents) in enumerate(test_torrents):
                if not self._is_running:  # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦åº”è¯¥åœæ­¢
                    break

                # å¦‚æœä¸æ˜¯å¼ºåˆ¶æŸ¥è¯¢ï¼Œåˆ™æ£€æŸ¥æ—¶é—´é—´éš”
                if not force_query:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡ŒIYUUæŸ¥è¯¢ï¼ˆè·ç¦»ä¸Šæ¬¡æŸ¥è¯¢è¶…è¿‡è®¾ç½®çš„æ—¶é—´é—´éš”æˆ–ä»æœªæŸ¥è¯¢è¿‡ï¼‰
                    # è·å–è®¾ç½®çš„æŸ¥è¯¢é—´éš”æ—¶é—´ï¼ˆé»˜è®¤ä¸º72å°æ—¶ï¼‰
                    config = self.config_manager.get()
                    iyuu_settings = config.get("iyuu_settings", {})
                    query_interval_hours = iyuu_settings.get(
                        "query_interval_hours", 72)

                    if not self._should_query_iyuu(name, query_interval_hours):
                        skip_message = f"[{i+1}/{total_torrents}] ğŸ”„ ç§å­ç»„ '{name}' è·ç¦»ä¸Šæ¬¡æŸ¥è¯¢ä¸è¶³{query_interval_hours}å°æ—¶ï¼Œè·³è¿‡æŸ¥è¯¢"
                        log_iyuu_message(skip_message, "INFO")
                        continue

                print(f"[{i+1}/{total_torrents}] ğŸ” æ­£åœ¨å¤„ç†ç§å­ç»„: {name}")

                # è·å–ä¼˜å…ˆhashåˆ—è¡¨å’Œè¿‡æ»¤åçš„ç§å­åˆ—è¡¨
                priority_hashes, filtered_torrents = self._get_priority_hash_for_torrent_group(
                    name, all_torrents.get(name, []), configured_sites)

                # å¦‚æœæ²¡æœ‰æ”¯æŒçš„ç«™ç‚¹ï¼Œåˆ™è·³è¿‡
                if not filtered_torrents:
                    log_iyuu_message(
                        f"[{i+1}/{total_torrents}] âš ï¸ ç§å­ç»„ '{name}' æ²¡æœ‰æ”¯æŒçš„ç«™ç‚¹ï¼Œè·³è¿‡æŸ¥è¯¢",
                        "INFO")
                    # æ›´æ–°æ‰€æœ‰åŒåç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´ï¼ˆåŒ…æ‹¬ä¸æ”¯æŒIYUUçš„ç«™ç‚¹ï¼‰
                    self._update_iyuu_last_check(name, [],
                                                 all_torrents.get(name, []))
                    continue

                # å°è¯•æœ€å¤š3ä¸ªä¸åŒçš„hashè¿›è¡ŒæŸ¥è¯¢
                max_attempts = 3
                results = None
                selected_hash = None

                for attempt in range(min(max_attempts, len(priority_hashes))):
                    if attempt >= len(priority_hashes):
                        break

                    selected_hash = priority_hashes[attempt]
                    # æ‰¾åˆ°å¯¹åº”çš„ç§å­ä¿¡æ¯ç”¨äºæ—¥å¿—è®°å½•
                    torrent_info = next((t for t in filtered_torrents
                                         if t['hash'] == selected_hash), None)
                    site_name = torrent_info['sites'] if torrent_info else 'æœªçŸ¥'

                    log_iyuu_message(
                        f"ä½¿ç”¨çš„hash [{attempt+1}/{min(max_attempts, len(priority_hashes))}]: {selected_hash} (ç«™ç‚¹: {site_name})",
                        "INFO")

                    try:
                        # æ‰§è¡Œæœç´¢
                        results = query_cross_seed(iyuu_token, selected_hash,
                                                   sid_sha1)
                        # å¦‚æœæˆåŠŸæŸ¥è¯¢åˆ°ç»“æœï¼Œåˆ™è·³å‡ºå¾ªç¯
                        log_iyuu_message(
                            f"[{i+1}/{total_torrents}] âœ… Hash {selected_hash[:8]}... æŸ¥è¯¢æˆåŠŸï¼Œåœæ­¢å°è¯•å…¶ä»–hash",
                            "INFO")
                        break
                    except Exception as e:
                        error_msg = str(e)
                        # å¦‚æœæ˜¯"æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®"é”™è¯¯ï¼Œåˆ™å°è¯•ä¸‹ä¸€ä¸ªhash
                        if "æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®" in error_msg or "400" in error_msg:
                            log_iyuu_message(
                                f"[{i+1}/{total_torrents}] âš ï¸  Hash {selected_hash[:8]}... æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªhash...",
                                "INFO")
                            continue
                        else:
                            # å…¶ä»–é”™è¯¯åˆ™é‡æ–°æŠ›å‡º
                            raise e

                # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
                if results is None:
                    log_iyuu_message(
                        f"[{i+1}/{total_torrents}] âŒ ç§å­ç»„ '{name}' æ‰€æœ‰hashéƒ½æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®",
                        "INFO")
                    # æ›´æ–°æ‰€æœ‰åŒåç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´ï¼ˆåŒ…æ‹¬ä¸æ”¯æŒIYUUçš„ç«™ç‚¹ï¼‰
                    self._update_iyuu_last_check(name, [],
                                                 all_torrents.get(name, []))
                    continue

                # å¦‚æœæˆåŠŸæŸ¥è¯¢åˆ°ç»“æœï¼Œç»§ç»­å¤„ç†
                # æ‰“å°æœç´¢ç»“æœå¹¶ç­›é€‰ç°æœ‰ç«™ç‚¹
                if not results:
                    print(
                        f"[{i+1}/{total_torrents}] ç§å­ {selected_hash[:8]}... æœªåœ¨å…¶ä»–ç«™ç‚¹å‘ç°ã€‚"
                    )
                else:
                    # ç­›é€‰å‡ºç°åœ¨æ•°æ®åº“ä¸­çš„ç«™ç‚¹
                    matched_sites = []
                    for item in results:
                        sid = item.get("sid")
                        site_info = sites_map.get(sid)

                        if not site_info:
                            continue

                        scheme = "https" if site_info.get(
                            "is_https") != 0 else "http"
                        details_page = site_info.get(
                            "details_page", "details.php?id={}").replace(
                                "{}", str(item.get("torrent_id")))
                        full_url = f"{scheme}://{site_info.get('base_url', '')}/{details_page}"

                        # å°†é“¾æ¥ä¸­çš„ api æ›¿æ¢ä¸º kpï¼ˆä¾‹å¦‚ï¼šapi.m-team.cc -> kp.m-team.ccï¼‰
                        full_url = full_url.replace("://api.", "://kp.")

                        iyuu_site_field = site_info.get("site")
                        iyuu_nickname = site_info.get("nickname")
                        db_site_name = None

                        # ä¼˜å…ˆä½¿ç”¨ 'site' å­—æ®µè¿›è¡Œæ˜ å°„
                        if iyuu_site_field and iyuu_site_field in iyuu_site_to_db_nickname_map:
                            db_site_name = iyuu_site_to_db_nickname_map[
                                iyuu_site_field]
                        # å¦åˆ™ï¼Œç›´æ¥ä½¿ç”¨ IYUU çš„ nickname ä½œä¸ºåå¤‡ (å‡è®¾å®ƒå¯èƒ½ä¸ torrents.sites åŒ¹é…)
                        elif iyuu_nickname:
                            db_site_name = iyuu_nickname

                        # æ£€æŸ¥æ˜ å°„åˆ°çš„ç«™ç‚¹åç§°æ˜¯å¦åœ¨ configured_sites (æ¥è‡ª torrents è¡¨) ä¸­
                        if db_site_name and db_site_name in configured_sites:
                            # è·å–åŸå§‹çš„IYUUç«™ç‚¹åç§°ç”¨äºæ—¥å¿—è®°å½•
                            iyuu_display_name = iyuu_nickname or iyuu_site_field or f"SID {sid}"
                            # å¦‚æœç«™ç‚¹åœ¨æ•°æ®åº“ä¸­ä¹Ÿæœ‰é…ç½®ä¿¡æ¯ï¼Œåˆ™ä½¿ç”¨å®ƒ
                            site_info_dict = existing_sites.get(
                                db_site_name, {})
                            matched_sites.append({
                                'iyuu_name': iyuu_display_name,
                                'db_name': db_site_name,
                                'url': full_url,
                                'site_info': site_info_dict
                            })

                    # ç»Ÿè®¡æ‰¾åˆ°çš„ç«™ç‚¹
                    if matched_sites:
                        result_stats['total_found'] += len(matched_sites)
                        result_stats['sites_found'].extend(
                            [site['db_name'] for site in matched_sites])

                        log_iyuu_message(
                            f"[{i+1}/{total_torrents}] ç§å­ {selected_hash[:8]}... åœ¨ {len(matched_sites)} ä¸ªå·²å­˜åœ¨çš„ç«™ç‚¹å‘ç°ï¼",
                            "INFO")
                        for site in matched_sites:
                            iyuu_site_name = site['iyuu_name']
                            db_site_name = site['db_name']
                            full_url = site['url']

                            if iyuu_site_name != db_site_name:
                                log_iyuu_message(
                                    f"âœ… åŒ¹é…ç«™ç‚¹: {iyuu_site_name} -> {db_site_name}",
                                    "INFO")
                            else:
                                log_iyuu_message(f"âœ… åŒ¹é…ç«™ç‚¹: {iyuu_site_name}",
                                                 "INFO")
                            log_iyuu_message(f"   é“¾æ¥: {full_url}", "INFO")
                    else:
                        log_iyuu_message(
                            f"[{i+1}/{total_torrents}] ç§å­ {selected_hash[:8]}... æœªåœ¨ä»»ä½•å·²å­˜åœ¨çš„ç«™ç‚¹å‘ç°ã€‚",
                            "INFO")

                    log_iyuu_message(
                        f"åœ¨torrentsè¡¨ä¸­æ‰¾åˆ° {len(matched_sites)} ä¸ªå·²å­˜åœ¨çš„ç«™ç‚¹", "INFO")

                    # ä¸ºç¼ºå¤±ç«™ç‚¹æ·»åŠ ç§å­è®°å½•
                    if matched_sites:
                        torrent_data = {
                            'hash': selected_hash,
                            'name': name,
                            'save_path':
                            filtered_torrents[0].get('save_path', ''),
                            'size': filtered_torrents[0].get('size', 0),
                        }

                        # ç»Ÿè®¡æ–°å¢å’Œæ›´æ–°çš„è®°å½•æ•°
                        new_count, updated_count = self._add_missing_site_torrents(
                            name,
                            torrent_data,
                            matched_sites,
                            return_count=True)
                        result_stats['new_records'] += new_count
                        result_stats['updated_records'] += updated_count

                    # æ›´æ–°æ‰€æœ‰åŒåç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´ï¼ˆåŒ…æ‹¬ä¸æ”¯æŒIYUUçš„ç«™ç‚¹ï¼‰
                    self._update_iyuu_last_check(name, matched_sites,
                                                 all_torrents.get(name, []))

                # æ¯æ¬¡æŸ¥è¯¢ä¹‹é—´é—´éš”5ç§’ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
                if i < len(test_torrents) - 1:
                    log_iyuu_message(
                        f"[{i+1}/{total_torrents}] ç­‰å¾…5ç§’åè¿›è¡Œä¸‹ä¸€æ¬¡æŸ¥è¯¢...", "INFO")
                    for _ in range(5):  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦åœæ­¢
                        if not self._is_running:
                            return result_stats if return_stats else None
                        time.sleep(1)

            return result_stats if return_stats else None

        except Exception as e:
            logging.error(f"IYUUæœç´¢æ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
            return result_stats if return_stats else None

    def _should_query_iyuu(self, torrent_name, query_interval_hours=72):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡ŒIYUUæŸ¥è¯¢ï¼ˆæ ¹æ®è®¾ç½®çš„æ—¶é—´é—´éš”æˆ–ä»æœªæŸ¥è¯¢è¿‡ï¼‰"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)
            ph = self.db_manager.get_placeholder()

            # æŸ¥è¯¢è¯¥ç§å­æœ€è¿‘ä¸€æ¬¡çš„iyuu_last_checkæ—¶é—´
            if self.db_manager.db_type == "postgresql":
                cursor.execute(
                    f"SELECT MAX(iyuu_last_check) as last_check FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            else:
                cursor.execute(
                    f"SELECT MAX(iyuu_last_check) as last_check FROM torrents WHERE name = {ph}",
                    (torrent_name, ))

            result = cursor.fetchone()
            last_check_str = result['last_check'] if isinstance(
                result, dict) else (result[0] if result else None)

            # å¦‚æœä»æœªæŸ¥è¯¢è¿‡ï¼Œåˆ™åº”è¯¥æŸ¥è¯¢
            if not last_check_str:
                return True

            # è§£æä¸Šæ¬¡æŸ¥è¯¢æ—¶é—´
            from datetime import datetime, timedelta
            # å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
            try:
                if isinstance(last_check_str, str):
                    # å°è¯•è§£æå¸¸è§çš„æ—¥æœŸæ—¶é—´æ ¼å¼
                    last_check = datetime.strptime(last_check_str,
                                                   "%Y-%m-%d %H:%M:%S")
                else:
                    last_check = last_check_str
            except ValueError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå‡è®¾éœ€è¦é‡æ–°æŸ¥è¯¢
                return True

            # è®¡ç®—è·ç¦»ç°åœ¨çš„æ—¶é—´å·®
            now = datetime.now()
            time_diff = now - last_check

            # å¦‚æœè¶…è¿‡è®¾ç½®çš„æ—¶é—´é—´éš”ï¼Œåˆ™åº”è¯¥æŸ¥è¯¢
            return time_diff > timedelta(hours=query_interval_hours)

        except Exception as e:
            logging.error(f"æ£€æŸ¥IYUUæŸ¥è¯¢æ¡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)
            # å‡ºé”™æ—¶é»˜è®¤è¿›è¡ŒæŸ¥è¯¢
            return True
        finally:
            if 'cursor' in locals() and cursor:
                cursor.close()
            if 'conn' in locals() and conn:
                conn.close()

    def _update_iyuu_last_check(self, torrent_name, matched_sites,
                                all_torrents_for_name):
        """æ›´æ–°æ‰€æœ‰åŒåç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´ï¼Œå¹¶ä¸ºæ²¡æœ‰detailså†…å®¹çš„è®°å½•å¡«å…¥è¯¦æƒ…é“¾æ¥"""
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)
            ph = self.db_manager.get_placeholder()

            # è·å–å½“å‰æ—¶é—´
            from datetime import datetime
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # è·å–æ•°æ®åº“ä¸­è¯¥ç§å­çš„æ‰€æœ‰ç°æœ‰è®°å½•
            if self.db_manager.db_type == "postgresql":
                cursor.execute(
                    f"SELECT hash, sites, details FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            else:
                cursor.execute(
                    f"SELECT hash, sites, details FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            existing_records = [dict(row) for row in cursor.fetchall()]

            updated_count = 0
            filled_details_count = 0

            # ä¸ºæ¯æ¡è®°å½•æ›´æ–°iyuu_last_checkæ—¶é—´ï¼Œå¹¶ä¸ºæ²¡æœ‰detailsçš„è®°å½•å¡«å…¥è¯¦æƒ…é“¾æ¥
            for record in existing_records:
                site_name = record['sites']
                current_details = record['details']
                hash_value = record['hash']

                # æ„å»ºæ›´æ–°å‚æ•°
                update_params = [current_time]  # iyuu_last_checkæ—¶é—´
                update_fields = [f"iyuu_last_check = {ph}"]

                # æŸ¥æ‰¾è¯¥ç«™ç‚¹åœ¨matched_sitesä¸­çš„è¯¦æƒ…é“¾æ¥
                matched_site = next(
                    (s for s in matched_sites if s['db_name'] == site_name),
                    None)

                # å¦‚æœå½“å‰è®°å½•æ²¡æœ‰detailsä¸”IYUUè¿”å›äº†è¯¦æƒ…é“¾æ¥ï¼Œåˆ™å¡«å…¥
                if (not current_details
                        or current_details.strip() == '') and matched_site:
                    update_params.append(matched_site['url'])
                    update_fields.append(f"details = {ph}")
                    filled_details_count += 1

                # æ·»åŠ WHEREæ¡ä»¶å‚æ•°
                update_params.extend([hash_value, torrent_name])

                # æ‰§è¡Œæ›´æ–°
                if self.db_manager.db_type == "postgresql":
                    cursor.execute(
                        f"UPDATE torrents SET {', '.join(update_fields)} WHERE hash = {ph} AND name = {ph}",
                        update_params)
                else:
                    cursor.execute(
                        f"UPDATE torrents SET {', '.join(update_fields)} WHERE hash = {ph} AND name = {ph}",
                        update_params)

                updated_count += cursor.rowcount

            conn.commit()
            print(f"ğŸ”„ å·²æ›´æ–° {updated_count} æ¡ç§å­è®°å½•çš„iyuu_last_checkæ—¶é—´")
            if filled_details_count > 0:
                print(f"âœ… å·²ä¸º {filled_details_count} æ¡ç§å­è®°å½•å¡«å…¥è¯¦æƒ…é“¾æ¥")

        except Exception as e:
            logging.error(f"æ›´æ–°ç§å­è®°å½•iyuu_last_checkæ—¶é—´å’Œè¯¦æƒ…é“¾æ¥æ—¶å‡ºé”™: {e}",
                          exc_info=True)
        finally:
            if 'cursor' in locals() and cursor:
                cursor.close()
            if 'conn' in locals() and conn:
                conn.close()

    def _add_missing_site_torrents(self,
                                   torrent_name,
                                   torrent_data,
                                   matched_sites,
                                   return_count=False):
        """ä¸ºç¼ºå¤±ç«™ç‚¹æ·»åŠ ç§å­è®°å½•
        
        Args:
            torrent_name: ç§å­åç§°
            torrent_data: ç§å­æ•°æ®
            matched_sites: åŒ¹é…çš„ç«™ç‚¹åˆ—è¡¨
            return_count: æ˜¯å¦è¿”å›æ–°å¢å’Œæ›´æ–°çš„è®°å½•æ•°
            
        Returns:
            tuple: å¦‚æœreturn_countä¸ºTrueï¼Œè¿”å›(new_count, updated_count)ï¼›å¦åˆ™è¿”å›None
        """
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)
            ph = self.db_manager.get_placeholder()

            # è·å–æ•°æ®åº“ä¸­è¯¥ç§å­å·²å­˜åœ¨çš„æ‰€æœ‰ç«™ç‚¹è®°å½•
            if self.db_manager.db_type == "postgresql":
                cursor.execute(
                    f"SELECT hash, sites, save_path, size, \"group\", details, downloader_id, progress, state FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            else:
                cursor.execute(
                    f"SELECT hash, sites, save_path, size, `group`, details, downloader_id, progress, state FROM torrents WHERE name = {ph}",
                    (torrent_name, ))
            existing_torrents = [dict(row) for row in cursor.fetchall()]

            # æå–å·²å­˜åœ¨çš„ç«™ç‚¹åˆ—è¡¨
            existing_sites = set()
            for t in existing_torrents:
                site = t['sites']
                if site:
                    if ',' in site:
                        site_list = site.split(',')
                        existing_sites.update(s.strip() for s in site_list
                                              if s.strip())
                    else:
                        existing_sites.add(site.strip())

            # è·å–IYUUè¿”å›çš„ç«™ç‚¹åˆ—è¡¨
            iyuu_sites = {site['db_name'] for site in matched_sites}

            # æ‰¾å‡ºç¼ºå¤±çš„ç«™ç‚¹
            missing_sites = iyuu_sites - existing_sites

            print(
                f"å‘ç° {len(missing_sites)} ä¸ªç¼ºå¤±çš„ç«™ç‚¹: {', '.join(missing_sites)}")

            # ä¸ºæ¯ä¸ªç¼ºå¤±çš„ç«™ç‚¹æ·»åŠ è®°å½•
            for site_name in missing_sites:
                # æ‰¾åˆ°è¯¥ç«™ç‚¹çš„åŒ¹é…ä¿¡æ¯
                matched_site = next(
                    (s for s in matched_sites if s['db_name'] == site_name),
                    None)
                if not matched_site:
                    continue

                # ä½¿ç”¨ç°æœ‰ç§å­ä¿¡æ¯åˆ›å»ºæ–°è®°å½•
                existing_torrent = existing_torrents[
                    0] if existing_torrents else torrent_data

                # è·å–å½“å‰æ—¶é—´
                from datetime import datetime
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                # ä¸ºç¼ºå¤±ç«™ç‚¹çš„ç§å­è®°å½•ç”Ÿæˆå”¯ä¸€hash
                # ä½¿ç”¨åŸå§‹hash+ç«™ç‚¹åç§°+æ—¶é—´æˆ³çš„ç»„åˆæ¥ç”Ÿæˆæ–°çš„å”¯ä¸€hash
                import hashlib
                unique_string = f"{torrent_data['hash']}_{site_name}_{current_time}"
                new_hash = hashlib.sha1(
                    unique_string.encode('utf-8')).hexdigest()

                if self.db_manager.db_type == "postgresql":
                    cursor.execute(
                        f"INSERT INTO torrents (hash, name, save_path, size, progress, state, sites, \"group\", details, downloader_id, last_seen, iyuu_last_check) VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})",
                        (
                            new_hash,  # ä½¿ç”¨æ–°ç”Ÿæˆçš„å”¯ä¸€hash
                            torrent_name,
                            '',  # è·¯å¾„ç•™ç©º
                            existing_torrent.get('size', 0),
                            0.0,  # è¿›åº¦è®¾ä¸º0ï¼Œè¡¨ç¤ºæœªä¸‹è½½
                            'æœªåšç§',  # çŠ¶æ€è®¾ä¸ºæœªåšç§ï¼Œè¡¨ç¤ºæœªåœ¨å®¢æˆ·ç«¯ä¸­
                            site_name,
                            existing_torrent.get('group', ''),
                            matched_site['url'],  # ä½¿ç”¨IYUUæä¾›çš„è¯¦æƒ…é“¾æ¥
                            existing_torrent.get('downloader_id', None),
                            current_time,  # last_seenè®¾ä¸ºå½“å‰æ—¶é—´
                            current_time  # iyuu_last_checkè®¾ä¸ºå½“å‰æ—¶é—´
                        ))
                else:
                    cursor.execute(
                        f"INSERT INTO torrents (hash, name, save_path, size, progress, state, sites, `group`, details, downloader_id, last_seen, iyuu_last_check) VALUES ({ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph}, {ph})",
                        (
                            new_hash,  # ä½¿ç”¨æ–°ç”Ÿæˆçš„å”¯ä¸€hash
                            torrent_name,
                            existing_torrent.get('save_path', ''),
                            existing_torrent.get('size', 0),
                            0.0,  # è¿›åº¦è®¾ä¸º0ï¼Œè¡¨ç¤ºæœªä¸‹è½½
                            'æœªåšç§',  # çŠ¶æ€è®¾ä¸ºæœªåšç§ï¼Œè¡¨ç¤ºæœªåœ¨å®¢æˆ·ç«¯ä¸­
                            site_name,
                            existing_torrent.get('group', ''),
                            matched_site['url'],  # ä½¿ç”¨IYUUæä¾›çš„è¯¦æƒ…é“¾æ¥
                            existing_torrent.get('downloader_id', None),
                            current_time,  # last_seenè®¾ä¸ºå½“å‰æ—¶é—´
                            current_time  # iyuu_last_checkè®¾ä¸ºå½“å‰æ—¶é—´
                        ))
                print(f"âœ… å·²ä¸ºç«™ç‚¹ '{site_name}' æ·»åŠ ç§å­è®°å½•")

            conn.commit()
            print(f"æˆåŠŸå¤„ç† {len(missing_sites)} ä¸ªç¼ºå¤±ç«™ç‚¹çš„ç§å­è®°å½•")

            # è¿”å›ç»Ÿè®¡ä¿¡æ¯
            if return_count:
                return len(missing_sites), 0  # æ–°å¢è®°å½•æ•°ï¼Œæ›´æ–°è®°å½•æ•°ï¼ˆè¿™é‡Œåªæœ‰æ–°å¢ï¼‰
            return None

        except Exception as e:
            logging.error(f"å¤„ç†ç¼ºå¤±ç«™ç‚¹ç§å­è®°å½•æ—¶å‡ºé”™: {e}", exc_info=True)
            if return_count:
                return 0, 0  # å‡ºé”™æ—¶è¿”å›0
            return None
        finally:
            if 'cursor' in locals() and cursor:
                cursor.close()
            if 'conn' in locals() and conn:
                conn.close()

    def _process_single_torrent(self,
                                torrent_name,
                                torrent_size,
                                force_query=True):
        """å¤„ç†å•ä¸ªç§å­çš„IYUUæŸ¥è¯¢
        
        Args:
            torrent_name: ç§å­åç§°
            torrent_size: ç§å­å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            force_query: æ˜¯å¦å¼ºåˆ¶æŸ¥è¯¢ï¼Œå¿½ç•¥æ—¶é—´é—´éš”é™åˆ¶ï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            dict: æŸ¥è¯¢ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        log_iyuu_message(
            f"[{current_time}] å¼€å§‹æ‰§è¡Œå•ä¸ªç§å­çš„IYUUæŸ¥è¯¢: {torrent_name} (å¤§å°: {torrent_size} å­—èŠ‚)",
            "INFO")
        if force_query:
            log_iyuu_message("å¼ºåˆ¶æŸ¥è¯¢æ¨¡å¼ï¼šå¿½ç•¥æ—¶é—´é—´éš”é™åˆ¶", "INFO")

        # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
        result_stats = {
            'total_found': 0,
            'new_records': 0,
            'updated_records': 0,
            'sites_found': []
        }

        conn = None
        try:
            conn = self.db_manager._get_connection()
            cursor = self.db_manager._get_cursor(conn)
            ph = self.db_manager.get_placeholder()

            # æŸ¥è¯¢æŒ‡å®šåç§°å’Œå¤§å°çš„ç§å­æ•°æ®ï¼Œåªç­›é€‰ä½“ç§¯å¤§äº200MBçš„ç§å­
            cursor.execute(
                f"SELECT hash, name, sites, size, save_path FROM torrents WHERE name = {ph} AND size = {ph} AND size > 207374182",
                (torrent_name, torrent_size))
            torrents_raw = [dict(row) for row in cursor.fetchall()]

            if not torrents_raw:
                log_iyuu_message(f"æœªæ‰¾åˆ°ç§å­: {torrent_name} (å¤§å°: {torrent_size})",
                                 "WARNING")
                return result_stats

            # è·å–é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            configured_sites = self._get_configured_sites()

            # æŒ‰ç§å­åç§°è¿›è¡Œèšåˆ
            all_torrents = defaultdict(list)

            for t in torrents_raw:
                site = t.get('sites', None)
                torrent_info = {
                    'hash': t['hash'],
                    'sites': site,
                    'size': t.get('size', 0),
                    'save_path': t.get('save_path', '')
                }
                all_torrents[torrent_name].append(torrent_info)

            # è·å–ä¼˜å…ˆhashåˆ—è¡¨å’Œè¿‡æ»¤åçš„ç§å­åˆ—è¡¨
            priority_hashes, filtered_torrents = self._get_priority_hash_for_torrent_group(
                torrent_name, all_torrents[torrent_name], configured_sites)

            if not filtered_torrents:
                log_iyuu_message(f"ç§å­ '{torrent_name}' æ²¡æœ‰æ”¯æŒçš„ç«™ç‚¹å¯ç”¨äºIYUUæŸ¥è¯¢",
                                 "WARNING")
                return result_stats

            log_iyuu_message(
                f"æ‰¾åˆ°ç§å­ '{torrent_name}'ï¼ŒåŒ…å« {len(filtered_torrents)} ä¸ªæ”¯æŒçš„ç«™ç‚¹",
                "INFO")

            # è·å–å·²é…ç½®çš„ç«™ç‚¹åˆ—è¡¨
            log_iyuu_message(f"æ•°æ®åº“ä¸­å­˜åœ¨ {len(configured_sites)} ä¸ªé…ç½®ç«™ç‚¹", "INFO")

            # åˆ›å»ºagg_torrentsç”¨äº_perform_iyuu_search
            agg_torrents = defaultdict(list)
            agg_torrents[torrent_name] = filtered_torrents

            # æ‰§è¡ŒIYUUæœç´¢é€»è¾‘ï¼Œä¼ å…¥force_queryå‚æ•°å’Œç»“æœç»Ÿè®¡
            result_stats = self._perform_iyuu_search(agg_torrents,
                                                     configured_sites,
                                                     all_torrents,
                                                     force_query=force_query,
                                                     return_stats=True)

            log_iyuu_message(f"=== ç§å­ '{torrent_name}' çš„IYUUæŸ¥è¯¢ä»»åŠ¡æ‰§è¡Œå®Œæˆ ===",
                             "INFO")
            log_iyuu_message(
                f"æŸ¥è¯¢ç»“æœç»Ÿè®¡: æ‰¾åˆ° {result_stats['total_found']} æ¡è®°å½•ï¼Œæ–°å¢ {result_stats['new_records']} æ¡ï¼Œæ›´æ–° {result_stats['updated_records']} æ¡",
                "INFO")

            return result_stats

        except Exception as e:
            logging.error(f"å¤„ç†å•ä¸ªç§å­æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            log_iyuu_message(f"å¤„ç†ç§å­æ—¶å‡ºé”™: {str(e)}", "ERROR")
            return result_stats
        finally:
            if conn:
                if 'cursor' in locals() and cursor:
                    cursor.close()
                conn.close()

    def stop(self):
        """åœæ­¢çº¿ç¨‹"""
        print("æ­£åœ¨åœæ­¢ IYUUThread çº¿ç¨‹...")
        self._is_running = False


# --- IYUU API é…ç½® ---
API_BASE = "https://2025.iyuu.cn"
CLIENT_VERSION = "8.2.0"

# --- è¯·æ±‚é¢‘ç‡æ§åˆ¶ ---
_last_request_time = 0
_rate_limit_delay = 5.0  # è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰


# --- IYUU ç¼“å­˜ç®¡ç†ç±» ---
class IYUUSiteCache:
    """IYUUç«™ç‚¹æ•°æ®ç¼“å­˜ç®¡ç†ç±»"""

    CACHE_EXPIRY_DAYS = 7  # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå¤©ï¼‰

    def __init__(self, cache_dir):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜æ–‡ä»¶å­˜å‚¨ç›®å½•
        """
        self.cache_dir = cache_dir
        self.cache_file = os.path.join(cache_dir, "iyuu_site_cache.json")
        os.makedirs(cache_dir, exist_ok=True)

    def _is_cache_valid(self, cache_data):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆæœªè¿‡æœŸï¼‰
        
        Args:
            cache_data: ç¼“å­˜æ•°æ®å­—å…¸
            
        Returns:
            bool: ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        if not cache_data or "timestamp" not in cache_data:
            return False

        try:
            cache_time = datetime.fromisoformat(cache_data["timestamp"])
            current_time = datetime.now()
            time_diff = current_time - cache_time

            return time_diff < timedelta(days=self.CACHE_EXPIRY_DAYS)
        except (ValueError, TypeError):
            return False

    def _sites_list_changed(self, cached_sites, current_sites):
        """æ£€æŸ¥ç«™ç‚¹åˆ—è¡¨æ˜¯å¦å‘ç”Ÿå˜åŒ–
        
        Args:
            cached_sites: ç¼“å­˜çš„ç«™ç‚¹åˆ—è¡¨
            current_sites: å½“å‰çš„ç«™ç‚¹åˆ—è¡¨
            
        Returns:
            bool: ç«™ç‚¹åˆ—è¡¨æ˜¯å¦å‘ç”Ÿå˜åŒ–
        """
        cached_set = set(cached_sites) if cached_sites else set()
        current_set = set(current_sites) if current_sites else set()

        return cached_set != current_set

    def load_cache(self, current_sites_list):
        """åŠ è½½ç¼“å­˜æ•°æ®
        
        Args:
            current_sites_list: å½“å‰torrentsè¡¨ä¸­çš„ç«™ç‚¹åˆ—è¡¨
            
        Returns:
            tuple: (sid_sha1, supported_sites, needs_update)
                   å¦‚æœç¼“å­˜æœ‰æ•ˆä¸”ç«™ç‚¹æœªå˜åŒ–ï¼Œè¿”å›ç¼“å­˜çš„æ•°æ®å’ŒFalse
                   å¦åˆ™è¿”å›None, None, True
        """
        try:
            if not os.path.exists(self.cache_file):
                log_iyuu_message("æœªæ‰¾åˆ°IYUUç¼“å­˜æ–‡ä»¶ï¼Œéœ€è¦é‡æ–°è·å–", "INFO")
                return None, None, True

            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if not self._is_cache_valid(cache_data):
                log_iyuu_message("IYUUç¼“å­˜å·²è¿‡æœŸï¼ˆè¶…è¿‡7å¤©ï¼‰ï¼Œéœ€è¦é‡æ–°è·å–", "INFO")
                return None, None, True

            # æ£€æŸ¥ç«™ç‚¹åˆ—è¡¨æ˜¯å¦å˜åŒ–
            cached_sites_list = cache_data.get("sites_list", [])
            if self._sites_list_changed(cached_sites_list, current_sites_list):
                log_iyuu_message("ç«™ç‚¹åˆ—è¡¨å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°è·å–sid_sha1", "INFO")
                return None, None, True

            # ç¼“å­˜æœ‰æ•ˆï¼Œè¿”å›ç¼“å­˜çš„æ•°æ®
            sid_sha1 = cache_data.get("sid_sha1")
            supported_sites = cache_data.get("supported_sites", [])

            log_iyuu_message(
                f"ä½¿ç”¨IYUUç¼“å­˜æ•°æ®ï¼ˆç¼“å­˜æ—¶é—´: {cache_data.get('timestamp')}ï¼‰", "INFO")
            log_iyuu_message(f"ç¼“å­˜çš„sid_sha1: {sid_sha1}", "INFO")
            log_iyuu_message(f"ç¼“å­˜çš„æ”¯æŒç«™ç‚¹æ•°é‡: {len(supported_sites)}", "INFO")

            return sid_sha1, supported_sites, False

        except Exception as e:
            logging.error(f"åŠ è½½IYUUç¼“å­˜æ—¶å‡ºé”™: {e}", exc_info=True)
            return None, None, True

    def save_cache(self, sid_sha1, supported_sites, sites_list):
        """ä¿å­˜ç¼“å­˜æ•°æ®
        
        Args:
            sid_sha1: ç«™ç‚¹æ ¡éªŒå“ˆå¸Œå€¼
            supported_sites: æ”¯æŒçš„ç«™ç‚¹åˆ—è¡¨
            sites_list: å½“å‰torrentsè¡¨ä¸­çš„ç«™ç‚¹åˆ—è¡¨
        """
        try:
            cache_data = {
                "timestamp": datetime.now().isoformat(),
                "sid_sha1": sid_sha1,
                "supported_sites": supported_sites,
                "sites_list": sites_list
            }

            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)

            log_iyuu_message(f"IYUUç¼“å­˜å·²ä¿å­˜åˆ°: {self.cache_file}", "INFO")
            log_iyuu_message(f"ç¼“å­˜åŒ…å« {len(supported_sites)} ä¸ªæ”¯æŒçš„ç«™ç‚¹", "INFO")

        except Exception as e:
            logging.error(f"ä¿å­˜IYUUç¼“å­˜æ—¶å‡ºé”™: {e}", exc_info=True)


# --- IYUU API è¾…åŠ©å‡½æ•° ---


def get_sha1_hex(text: str) -> str:
    """è®¡ç®—å­—ç¬¦ä¸²çš„ SHA-1 å“ˆå¸Œå€¼"""
    return hashlib.sha1(text.encode('utf-8')).hexdigest()


def make_api_request(method: str,
                     url: str,
                     token: str,
                     max_retries: int = 3,
                     **kwargs) -> dict:
    """
    å°è£… API è¯·æ±‚ï¼Œç»Ÿä¸€å¤„ç† headers å’Œé”™è¯¯ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶ã€‚
    
    Args:
        method: HTTPæ–¹æ³• (GET, POST)
        url: è¯·æ±‚URL
        token: IYUU Token
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
        **kwargs: å…¶ä»–è¯·æ±‚å‚æ•°
    
    Returns:
        dict: APIå“åº”æ•°æ®
    """
    global _last_request_time, _rate_limit_delay

    for attempt in range(max_retries):
        try:
            # è¯·æ±‚é¢‘ç‡æ§åˆ¶ - ç¡®ä¿è¯·æ±‚ä¹‹é—´æœ‰é€‚å½“çš„å»¶è¿Ÿ
            current_time = time.time()
            time_since_last_request = current_time - _last_request_time
            if time_since_last_request < _rate_limit_delay:
                sleep_time = _rate_limit_delay - time_since_last_request
                print(f"è¯·æ±‚é¢‘ç‡æ§åˆ¶: ç­‰å¾… {sleep_time:.2f} ç§’")
                time.sleep(sleep_time)

            # æ›´æ–°æœ€åè¯·æ±‚æ—¶é—´
            _last_request_time = time.time()

            # åŸºç¡€ headersï¼ŒåŒ…å« Token
            final_headers = {'Token': token}

            # å¦‚æœè°ƒç”¨æ—¶ä¼ å…¥äº†é¢å¤–çš„ headers (å¦‚ Content-Type)ï¼Œåˆ™è¿›è¡Œåˆå¹¶
            if 'headers' in kwargs:
                # ä½¿ç”¨ update æ–¹æ³•å°†ä¼ å…¥çš„ headers åˆå¹¶è¿›æ¥
                final_headers.update(kwargs.pop('headers'))

            if method.upper() == 'GET':
                response = requests.get(url,
                                        headers=final_headers,
                                        timeout=20,
                                        **kwargs)
            elif method.upper() == 'POST':
                response = requests.post(url,
                                         headers=final_headers,
                                         timeout=20,
                                         **kwargs)
            else:
                raise ValueError("Unsupported HTTP method")

            response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯ 2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

            data = response.json()
            if data.get("code") != 0:
                error_msg = data.get("msg", "æœªçŸ¥ API é”™è¯¯")
                raise Exception(
                    f"API é”™è¯¯: {error_msg} (ä»£ç : {data.get('code')})")

            return data

        except requests.exceptions.RequestException as e:
            error_msg = f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}"
            if attempt < max_retries - 1:
                wait_time = 5 * (attempt + 1)  # é€’å¢ç­‰å¾…æ—¶é—´ï¼š5ç§’ã€10ç§’ã€15ç§’
                log_iyuu_message(
                    f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}",
                    "WARNING")
                log_iyuu_message(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...", "INFO")
                time.sleep(wait_time)
            else:
                raise Exception(error_msg)
        except json.JSONDecodeError as e:
            error_msg = f"æ— æ³•è§£ææœåŠ¡å™¨è¿”å›çš„ JSON æ•°æ®: {e}"
            if attempt < max_retries - 1:
                wait_time = 5 * (attempt + 1)
                log_iyuu_message(
                    f"JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}",
                    "WARNING")
                log_iyuu_message(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...", "INFO")
                time.sleep(wait_time)
            else:
                raise Exception(error_msg)
        except Exception as e:
            error_msg = str(e)
            # å¯¹äºAPIé”™è¯¯ï¼ˆå¦‚tokenæ— æ•ˆç­‰ï¼‰ï¼Œä¸è¿›è¡Œé‡è¯•ï¼Œç›´æ¥æŠ›å‡º
            if "API é”™è¯¯" in error_msg or "Token" in error_msg:
                raise e
            # å¯¹äºå…¶ä»–é”™è¯¯ï¼Œè¿›è¡Œé‡è¯•
            if attempt < max_retries - 1:
                wait_time = 5 * (attempt + 1)
                log_iyuu_message(
                    f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}",
                    "WARNING")
                log_iyuu_message(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...", "INFO")
                time.sleep(wait_time)
            else:
                raise e


def get_supported_sites(token: str) -> list:
    """è·å– IYUU æ”¯æŒçš„æ‰€æœ‰å¯è¾…ç§ç«™ç‚¹åˆ—è¡¨"""
    print("æ­£åœ¨è·å– IYUU æ”¯æŒçš„å¯è¾…ç§ç«™ç‚¹åˆ—è¡¨...")
    url = f"{API_BASE}/reseed/sites/index"
    response_data = make_api_request("GET", url, token)
    sites = response_data.get("data", {}).get("sites", [])
    print(f"ä»APIè·å–åˆ°çš„å¯è¾…ç§ç«™ç‚¹æ•°é‡: {len(sites) if response_data.get('data') else 0}")
    if not sites:
        raise Exception("æœªèƒ½è·å–åˆ°å¯è¾…ç§ç«™ç‚¹åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ Token æˆ– IYUU æœåŠ¡çŠ¶æ€ã€‚")
    print(f"æˆåŠŸè·å–åˆ° {len(sites)} ä¸ªå¯è¾…ç§ç«™ç‚¹ä¿¡æ¯ã€‚")
    return sites


def get_sid_sha1(token: str, all_sites: list) -> str:
    """æ ¹æ®ç«™ç‚¹åˆ—è¡¨ä¸ŠæŠ¥å¹¶è·å– sid_sha1"""
    print("æ­£åœ¨ç”Ÿæˆç«™ç‚¹æ ¡éªŒå“ˆå¸Œ (sid_sha1)...")
    print(f"æ¥æ”¶åˆ°çš„ç«™ç‚¹æ•°é‡: {len(all_sites)}")

    # æ‰“å°å‰å‡ ä¸ªç«™ç‚¹çš„ä¿¡æ¯ç”¨äºè°ƒè¯•
    for i, site in enumerate(all_sites[:5]):
        print(f"ç«™ç‚¹ {i+1}: ID={site.get('id')}, åç§°={site.get('nickname')}")

    site_ids = [site['id'] for site in all_sites]
    print(f"æå–çš„ç«™ç‚¹IDæ•°é‡: {len(site_ids)}")

    if not site_ids:
        raise Exception("ç«™ç‚¹IDåˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆsid_sha1")

    payload = {"sid_list": site_ids}
    print(f"å‘é€çš„payload: {payload}")

    url = f"{API_BASE}/reseed/sites/reportExisting"

    # è¿™é‡Œçš„ headers ä¼šè¢«æ­£ç¡®åˆå¹¶
    headers = {'Content-Type': 'application/json'}
    response_data = make_api_request("POST",
                                     url,
                                     token,
                                     json=payload,
                                     headers=headers)

    sid_sha1 = response_data.get("data", {}).get("sid_sha1")
    if not sid_sha1:
        raise Exception("æœªèƒ½ä» API è·å– sid_sha1ã€‚")
    print("æˆåŠŸç”Ÿæˆ sid_sha1ã€‚")
    return sid_sha1


def get_filtered_sid_sha1_and_sites(token: str, db_manager) -> tuple:
    """è·å–è¿‡æ»¤åçš„sid_sha1å’Œç«™ç‚¹åˆ—è¡¨ï¼ŒåªåŒ…å«åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹"""
    print("=== å¼€å§‹è·å–è¿‡æ»¤åçš„sid_sha1å’Œç«™ç‚¹åˆ—è¡¨ ===")

    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    from config import DATA_DIR
    cache = IYUUSiteCache(DATA_DIR)

    # 1. è·å–torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹åˆ—è¡¨
    try:
        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)

        # æŸ¥è¯¢torrentsè¡¨ä¸­æ‰€æœ‰ä¸åŒçš„ç«™ç‚¹
        cursor.execute(
            "SELECT DISTINCT sites FROM torrents WHERE sites IS NOT NULL AND sites != ''"
        )
        sites_result = cursor.fetchall()

        # æå–ç«™ç‚¹åç§°å¹¶å»é‡
        torrent_sites = set()
        for row in sites_result:
            site = row['sites'] if isinstance(row, dict) else row[0]
            if site:
                # å¦‚æœç«™ç‚¹å­—æ®µåŒ…å«å¤šä¸ªç«™ç‚¹ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼Œåˆ™åˆ†å‰²å®ƒä»¬
                if ',' in site:
                    site_list = site.split(',')
                    torrent_sites.update(s.strip() for s in site_list
                                         if s.strip())
                else:
                    torrent_sites.add(site.strip())

        cursor.close()
        conn.close()

        torrent_sites_list = list(torrent_sites)
        print(f"torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹æ•°é‡: {len(torrent_sites_list)}")
        print(f"ç«™ç‚¹åˆ—è¡¨: {', '.join(torrent_sites_list)}")

    except Exception as e:
        logging.error(f"è·å–torrentsè¡¨ä¸­çš„ç«™ç‚¹ä¿¡æ¯æ—¶å‡ºé”™: {e}", exc_info=True)
        raise

    # 2. å°è¯•ä»ç¼“å­˜åŠ è½½æ•°æ®
    cached_sid_sha1, cached_sites, needs_update = cache.load_cache(
        torrent_sites_list)

    if not needs_update:
        # ç¼“å­˜æœ‰æ•ˆï¼Œç›´æ¥è¿”å›ç¼“å­˜çš„æ•°æ®
        return cached_sid_sha1, cached_sites

    # 3. ç¼“å­˜æ— æ•ˆæˆ–éœ€è¦æ›´æ–°ï¼Œé‡æ–°è·å–IYUUæ”¯æŒçš„æ‰€æœ‰å¯è¾…ç§ç«™ç‚¹
    try:
        supported_sites = get_supported_sites(token)
        print(f"è·å–åˆ° {len(supported_sites)} ä¸ªIYUUæ”¯æŒçš„å¯è¾…ç§ç«™ç‚¹")
    except Exception as e:
        logging.error(f"è·å–IYUUæ”¯æŒç«™ç‚¹åˆ—è¡¨å¤±è´¥: {e}")
        raise

    # åœ¨è·å–ç«™ç‚¹åˆ—è¡¨å’Œåç»­è¯·æ±‚ä¹‹é—´æ·»åŠ é¢å¤–å»¶è¿Ÿ
    print("ç­‰å¾…é¢å¤–å»¶è¿Ÿä»¥é¿å…è¯·æ±‚é¢‘ç‡è¿‡å¿«...")
    time.sleep(2)

    # 4. ä»æ•°æ®åº“è·å– site -> nickname æ˜ å°„
    try:
        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)
        cursor.execute(
            "SELECT site, nickname FROM sites WHERE site IS NOT NULL AND site != '' AND nickname IS NOT NULL AND nickname != ''"
        )
        db_site_to_nickname_map = {
            row['site']: row['nickname']
            for row in cursor.fetchall()
        }
        cursor.close()
        conn.close()
    except Exception as e:
        logging.error(f"ä»æ•°æ®åº“è·å–ç«™ç‚¹æ˜ å°„æ—¶å‡ºé”™: {e}", exc_info=True)
        db_site_to_nickname_map = {}

    # 5. è¿‡æ»¤å‡ºIYUUæ”¯æŒä¸”åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„ç«™ç‚¹
    filtered_site_ids = []
    filtered_sites = []

    # ä½¿ç”¨é›†åˆä»¥ä¼˜åŒ–æŸ¥æ‰¾æ€§èƒ½
    processed_site_ids = set()

    for site in supported_sites:
        iyuu_id = site.get('id')
        if not iyuu_id or iyuu_id in processed_site_ids:
            continue

        # æ ¹æ®ç”¨æˆ·åé¦ˆï¼šIYUU APIçš„'site'å­—æ®µä¸æ•°æ®åº“'sites'è¡¨çš„'site'åˆ—ç›¸åŒ
        iyuu_site_field = site.get('site')

        if iyuu_site_field and iyuu_site_field in db_site_to_nickname_map:
            # æ ¹æ® 'sites.site' -> 'sites.nickname' çš„æ˜ å°„å…³ç³»ï¼Œæ‰¾åˆ°å¯¹åº”çš„æœ¬åœ°æ˜µç§°
            db_nickname = db_site_to_nickname_map[iyuu_site_field]

            # æ ¹æ®ç”¨æˆ·åé¦ˆï¼š'torrents.sites'åˆ—çš„å†…å®¹ä¸'sites.nickname'åˆ—ç›¸åŒ
            # æ‰€ä»¥æ£€æŸ¥è¿™ä¸ªæ˜µç§°æ˜¯å¦åœ¨ torrents è¡¨ä¸­å­˜åœ¨
            if db_nickname in torrent_sites:
                filtered_site_ids.append(iyuu_id)
                filtered_sites.append(site)
                processed_site_ids.add(iyuu_id)
                continue

        # ä½œä¸ºåå¤‡æ–¹æ¡ˆï¼Œå¦‚æœä¸Šè¿°é€»è¾‘ä¸åŒ¹é…ï¼Œç›´æ¥ç”¨IYUUçš„nicknameåŒ¹é…torrentsè¡¨ä¸­çš„siteså­—æ®µ
        # (å› ä¸ºtorrents.sites å°±æ˜¯ nickname)
        iyuu_nickname = site.get('nickname')
        if iyuu_nickname and iyuu_nickname in torrent_sites:
            filtered_site_ids.append(iyuu_id)
            filtered_sites.append(site)
            processed_site_ids.add(iyuu_id)

    print(f"è¿‡æ»¤åå¾—åˆ° {len(filtered_site_ids)} ä¸ªæ”¯æŒçš„ç«™ç‚¹ID")
    print(f"ç«™ç‚¹IDåˆ—è¡¨: {filtered_site_ids}")

    if not filtered_site_ids:
        raise Exception("æ²¡æœ‰æ‰¾åˆ°åœ¨torrentsè¡¨ä¸­å­˜åœ¨çš„IYUUæ”¯æŒç«™ç‚¹")

    # åœ¨å‘é€reportExistingè¯·æ±‚å‰æ·»åŠ é¢å¤–å»¶è¿Ÿ
    print("å‡†å¤‡å‘é€reportExistingè¯·æ±‚ï¼Œç­‰å¾…é¢å¤–å»¶è¿Ÿ...")
    time.sleep(2)

    # 5. æ„å»ºsid_sha1
    try:
        payload = {"sid_list": filtered_site_ids}
        url = f"{API_BASE}/reseed/sites/reportExisting"

        headers = {'Content-Type': 'application/json'}
        response_data = make_api_request("POST",
                                         url,
                                         token,
                                         json=payload,
                                         headers=headers)

        sid_sha1 = response_data.get("data", {}).get("sid_sha1")
        if not sid_sha1:
            raise Exception("æœªèƒ½ä» API è·å– sid_sha1ã€‚")
        print(f"æˆåŠŸç”Ÿæˆè¿‡æ»¤åçš„ sid_sha1: {sid_sha1}")

        # 6. ä¿å­˜ç¼“å­˜
        cache.save_cache(sid_sha1, filtered_sites, torrent_sites_list)

        return sid_sha1, filtered_sites
    except Exception as e:
        logging.error(f"ç”Ÿæˆsid_sha1æ—¶å‡ºé”™: {e}")
        raise


def query_cross_seed(token: str,
                     infohash: str,
                     sid_sha1: str,
                     max_retries: int = 3) -> list:
    """æŸ¥è¯¢æŒ‡å®š infohash çš„è¾…ç§ä¿¡æ¯ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
    
    Args:
        token: IYUU Token
        infohash: ç§å­å“ˆå¸Œå€¼
        sid_sha1: ç«™ç‚¹æ ¡éªŒå“ˆå¸Œå€¼
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
    
    Returns:
        list: è¾…ç§ä¿¡æ¯åˆ—è¡¨
    """
    print(f"æ­£åœ¨ä¸ºç§å­ {infohash[:8]}... æŸ¥è¯¢è¾…ç§ä¿¡æ¯...")
    url = f"{API_BASE}/reseed/index/index"

    for attempt in range(max_retries):
        try:
            hashes_json_str = json.dumps([infohash.lower()])
            form_data = {
                "hash": hashes_json_str,
                "sha1": get_sha1_hex(hashes_json_str),
                "sid_sha1": sid_sha1,
                "timestamp": str(int(time.time())),
                "version": CLIENT_VERSION
            }

            # è¿™é‡Œçš„ headers ä¹Ÿä¼šè¢«æ­£ç¡®åˆå¹¶
            headers = {'Content-Type': 'application/x-www-form-urlencoded'}
            response_data = make_api_request("POST",
                                             url,
                                             token,
                                             data=form_data,
                                             headers=headers)

            data = response_data.get("data", {})
            if not data or infohash.lower() not in data:
                return []

            results = data[infohash.lower()].get("torrent", [])
            return results

        except Exception as e:
            error_msg = str(e)
            # å¦‚æœæ˜¯"æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®"é”™è¯¯ï¼Œä¸è¿›è¡Œé‡è¯•ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
            if "æœªæŸ¥è¯¢åˆ°å¯è¾…ç§æ•°æ®" in error_msg or "400" in error_msg:
                return []

            # å¯¹äºAPIé”™è¯¯ï¼ˆå¦‚tokenæ— æ•ˆç­‰ï¼‰ï¼Œä¸è¿›è¡Œé‡è¯•ï¼Œç›´æ¥æŠ›å‡º
            if "API é”™è¯¯" in error_msg or "Token" in error_msg:
                raise e

            # å¯¹äºå…¶ä»–é”™è¯¯ï¼Œè¿›è¡Œé‡è¯•
            if attempt < max_retries - 1:
                wait_time = 5 * (attempt + 1)  # é€’å¢ç­‰å¾…æ—¶é—´ï¼š5ç§’ã€10ç§’ã€15ç§’
                log_iyuu_message(
                    f"æŸ¥è¯¢è¾…ç§ä¿¡æ¯å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}",
                    "WARNING")
                log_iyuu_message(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...", "INFO")
                time.sleep(wait_time)
            else:
                log_iyuu_message(f"æŸ¥è¯¢è¾…ç§ä¿¡æ¯å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {error_msg}", "ERROR")
                raise e

    return []  # ç†è®ºä¸Šä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§


# å…¨å±€å˜é‡
iyuu_thread = None

# IYUUæ—¥å¿—å­˜å‚¨
iyuu_logs = []


def log_iyuu_message(message, level="INFO"):
    """è®°å½•IYUUæ—¥å¿—æ¶ˆæ¯"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = {"timestamp": timestamp, "level": level, "message": message}
    iyuu_logs.append(log_entry)

    # é™åˆ¶æ—¥å¿—æ•°é‡ï¼Œåªä¿ç•™æœ€è¿‘100æ¡
    if len(iyuu_logs) > 100:
        iyuu_logs.pop(0)

    # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
    print(f"[IYUU-{level}] {timestamp} {message}")


def start_iyuu_thread(db_manager, config_manager):
    """åˆå§‹åŒ–å¹¶å¯åŠ¨å…¨å±€ IYUUThread çº¿ç¨‹å®ä¾‹ã€‚"""
    global iyuu_thread
    # æ£€æŸ¥æ˜¯å¦åœ¨è°ƒè¯•æ¨¡å¼ä¸‹è¿è¡Œï¼Œé¿å…é‡å¤å¯åŠ¨
    import os
    if os.environ.get('WERKZEUG_RUN_MAIN') != 'true':
        # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼Œè¿™æ˜¯ç›‘æ§è¿›ç¨‹ï¼Œä¸éœ€è¦å¯åŠ¨çº¿ç¨‹
        print("æ£€æµ‹åˆ°è°ƒè¯•ç›‘æ§è¿›ç¨‹ï¼Œè·³è¿‡IYUUçº¿ç¨‹å¯åŠ¨ã€‚")
        return iyuu_thread

    if iyuu_thread is None or not iyuu_thread.is_alive():
        iyuu_thread = IYUUThread(db_manager, config_manager)
        iyuu_thread.start()
        print("å·²åˆ›å»ºå¹¶å¯åŠ¨æ–°çš„ IYUUThread å®ä¾‹ã€‚")
    return iyuu_thread


def stop_iyuu_thread():
    """åœæ­¢å¹¶æ¸…ç†å½“å‰çš„ IYUUThread çº¿ç¨‹å®ä¾‹ã€‚"""
    global iyuu_thread
    if iyuu_thread and iyuu_thread.is_alive():
        iyuu_thread.stop()
        iyuu_thread.join(timeout=10)
        print("IYUUThread çº¿ç¨‹å·²åœæ­¢ã€‚")
    iyuu_thread = None
