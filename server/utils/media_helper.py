# utils/media_helper.py

import base64
import logging
import mimetypes
import re
import os
import shutil
import subprocess
import tempfile
import requests
import json
import time
import random
import cloudscraper
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from pymediainfo import MediaInfo
from config import TEMP_DIR, config_manager
from qbittorrentapi import Client as qbClient
from transmission_rpc import Client as TrClient
from utils import ensure_scheme
from PIL import Image


def translate_path(downloader_id: str, remote_path: str) -> str:
    """
    å°†ä¸‹è½½å™¨çš„è¿œç¨‹è·¯å¾„è½¬æ¢ä¸º PT Nexus å®¹å™¨å†…çš„æœ¬åœ°è·¯å¾„ã€‚

    :param downloader_id: ä¸‹è½½å™¨ID
    :param remote_path: ä¸‹è½½å™¨ä¸­çš„è¿œç¨‹è·¯å¾„
    :return: PT Nexus å®¹å™¨å†…å¯è®¿é—®çš„æœ¬åœ°è·¯å¾„
    """
    if not downloader_id or not remote_path:
        return remote_path

    # è·å–ä¸‹è½½å™¨é…ç½®
    config = config_manager.get()
    downloaders = config.get("downloaders", [])

    for downloader in downloaders:
        if downloader.get("id") == downloader_id:
            path_mappings = downloader.get("path_mappings", [])
            if not path_mappings:
                # æ²¡æœ‰é…ç½®è·¯å¾„æ˜ å°„ï¼Œç›´æ¥è¿”å›åŸè·¯å¾„
                return remote_path

            # æŒ‰è¿œç¨‹è·¯å¾„é•¿åº¦é™åºæ’åºï¼Œä¼˜å…ˆåŒ¹é…æœ€é•¿çš„è·¯å¾„ï¼ˆæ›´ç²¾ç¡®ï¼‰
            sorted_mappings = sorted(path_mappings,
                                     key=lambda x: len(x.get('remote', '')),
                                     reverse=True)

            for mapping in sorted_mappings:
                remote = mapping.get('remote', '')
                local = mapping.get('local', '')

                if not remote or not local:
                    continue

                # ç¡®ä¿è·¯å¾„æ¯”è¾ƒæ—¶ç»Ÿä¸€å¤„ç†æœ«å°¾çš„æ–œæ 
                remote = remote.rstrip('/')
                remote_path_normalized = remote_path.rstrip('/')

                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼ˆå®Œå…¨åŒ¹é…æˆ–å‰ç¼€åŒ¹é…ï¼‰
                if remote_path_normalized == remote:
                    # å®Œå…¨åŒ¹é…
                    return local
                elif remote_path_normalized.startswith(remote + '/'):
                    # å‰ç¼€åŒ¹é…ï¼Œæ›¿æ¢è·¯å¾„
                    relative_path = remote_path_normalized[len(remote
                                                               ):].lstrip('/')
                    return os.path.join(local, relative_path)

            # æ²¡æœ‰åŒ¹é…çš„æ˜ å°„ï¼Œè¿”å›åŸè·¯å¾„
            return remote_path

    # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ä¸‹è½½å™¨ï¼Œè¿”å›åŸè·¯å¾„
    return remote_path


def _upload_to_pixhost(image_path: str):
    """
    å°†å•ä¸ªå›¾ç‰‡æ–‡ä»¶ä¸Šä¼ åˆ° Pixhost.toã€‚

    :param image_path: æœ¬åœ°å›¾ç‰‡æ–‡ä»¶çš„è·¯å¾„ã€‚
    :return: æˆåŠŸæ—¶è¿”å›å›¾ç‰‡çš„å±•ç¤ºURLï¼Œå¤±è´¥æ—¶è¿”å›Noneã€‚
    """
    api_url = 'https://api.pixhost.to/images'
    params = {'content_type': 0}
    headers = {
        'User-Agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    print(f"å‡†å¤‡ä¸Šä¼ å›¾ç‰‡: {image_path}")

    if not os.path.exists(image_path):
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {image_path}")
        return None

    # ç›´æ¥ä¸Šä¼ ï¼Œä¸ä½¿ç”¨å…¨å±€ä»£ç†
    return _upload_to_pixhost_direct(image_path, api_url, params, headers)


def _get_agsv_auth_token():
    """ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é‚®ç®±å’Œå¯†ç è·å– æœ«æ—¥å›¾åºŠ çš„æˆæƒ Tokenã€‚"""
    config = config_manager.get().get("cross_seed", {})
    email = config.get("agsv_email")
    password = config.get("agsv_password")

    if not email or not password:
        logging.warning("æœ«æ—¥å›¾åºŠ é‚®ç®±æˆ–å¯†ç æœªé…ç½®ï¼Œæ— æ³•è·å– Tokenã€‚")
        return None

    token_url = "https://img.seedvault.cn/api/v1/tokens"
    payload = {"email": email, "password": password}
    headers = {"Accept": "application/json"}
    print("æ­£åœ¨ä¸º æœ«æ—¥å›¾åºŠ è·å–æˆæƒ Token...")
    try:
        response = requests.post(token_url,
                                 headers=headers,
                                 json=payload,
                                 timeout=30)
        if response.status_code == 200 and response.json().get("status"):
            token = response.json().get("data", {}).get("token")
            if token:
                print("   âœ… æˆåŠŸè·å– æœ«æ—¥å›¾åºŠ Tokenï¼")
                return token

        logging.error(
            f"è·å– æœ«æ—¥å›¾åºŠ Token å¤±è´¥ã€‚çŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}"
        )
        print(f"   âŒ è·å– æœ«æ—¥å›¾åºŠ Token å¤±è´¥: {response.text}")
        return None
    except requests.exceptions.RequestException as e:
        logging.error(f"è·å– æœ«æ—¥å›¾åºŠ Token æ—¶ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        print(f"   âŒ è·å– æœ«æ—¥å›¾åºŠ Token æ—¶ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return None


def _upload_to_agsv(image_path: str, token: str):
    """ä½¿ç”¨ç»™å®šçš„ Token ä¸Šä¼ å•ä¸ªå›¾ç‰‡åˆ° æœ«æ—¥å›¾åºŠã€‚"""
    upload_url = "https://img.seedvault.cn/api/v1/upload"
    headers = {
        "Authorization":
        f"Bearer {token}",
        "Accept":
        "application/json",
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    }

    mime_type = mimetypes.guess_type(
        image_path)[0] or 'application/octet-stream'
    image_name = os.path.basename(image_path)

    print(f"å‡†å¤‡ä¸Šä¼ å›¾ç‰‡åˆ° æœ«æ—¥å›¾åºŠ: {image_name}")
    try:
        with open(image_path, 'rb') as f:
            files = {'file': (image_name, f, mime_type)}
            response = requests.post(upload_url,
                                     headers=headers,
                                     files=files,
                                     timeout=180)

        data = response.json()
        if response.status_code == 200 and data.get("status"):
            image_url = data.get("data", {}).get("links", {}).get("url")
            print(f"   âœ… æœ«æ—¥å›¾åºŠ ä¸Šä¼ æˆåŠŸï¼URL: {image_url}")
            return image_url
        else:
            message = data.get('message', 'æ— è¯¦ç»†ä¿¡æ¯')
            logging.error(f"æœ«æ—¥å›¾åºŠ ä¸Šä¼ å¤±è´¥ã€‚API æ¶ˆæ¯: {message}")
            print(f"   âŒ æœ«æ—¥å›¾åºŠ ä¸Šä¼ å¤±è´¥: {message}")
            return None
    except (requests.exceptions.RequestException,
            requests.exceptions.JSONDecodeError) as e:
        logging.error(f"ä¸Šä¼ åˆ° æœ«æ—¥å›¾åºŠ æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(f"   âŒ ä¸Šä¼ åˆ° æœ«æ—¥å›¾åºŠ æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def _get_smart_screenshot_points(video_path: str,
                                 num_screenshots: int = 5) -> list[float]:
    """
    [ä¼˜åŒ–ç‰ˆ] ä½¿ç”¨ ffprobe æ™ºèƒ½åˆ†æè§†é¢‘å­—å¹•ï¼Œé€‰æ‹©æœ€ä½³çš„æˆªå›¾æ—¶é—´ç‚¹ã€‚
    - é€šè¿‡ `-read_intervals` å‚æ•°å®ç°åˆ†æ®µè¯»å–ï¼Œé¿å…å…¨æ–‡ä»¶æ‰«æï¼Œå¤§å¹…æå‡å¤§æ–‡ä»¶å¤„ç†é€Ÿåº¦ã€‚
    - ä¼˜å…ˆé€‰æ‹© ASS > SRT > PGS æ ¼å¼çš„å­—å¹•ã€‚
    - ä¼˜å…ˆåœ¨è§†é¢‘çš„ 30%-80% "é»„é‡‘æ—¶æ®µ" å†…éšæœºé€‰æ‹©ã€‚
    - åœ¨æ‰€æœ‰æ™ºèƒ½åˆ†æå¤±è´¥æ—¶ï¼Œä¼˜é›…åœ°å›é€€åˆ°æŒ‰ç™¾åˆ†æ¯”é€‰æ‹©ã€‚
    """
    print("\n--- å¼€å§‹æ™ºèƒ½æˆªå›¾æ—¶é—´ç‚¹åˆ†æ (å¿«é€Ÿæ‰«ææ¨¡å¼) ---")
    if not shutil.which("ffprobe"):
        print("è­¦å‘Š: æœªæ‰¾åˆ° ffprobeï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½åˆ†æã€‚")
        return []

    try:
        cmd_duration = [
            "ffprobe", "-v", "error", "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1", video_path
        ]
        result = subprocess.run(cmd_duration,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        duration = float(result.stdout.strip())
        print(f"è§†é¢‘æ€»æ—¶é•¿: {duration:.2f} ç§’")
    except Exception as e:
        print(f"é”™è¯¯ï¼šä½¿ç”¨ ffprobe è·å–è§†é¢‘æ—¶é•¿å¤±è´¥ã€‚{e}")
        return []

    # æ¢æµ‹å­—å¹•æµçš„éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒæœ¬èº«é€Ÿåº¦å¾ˆå¿«
    try:
        cmd_probe_subs = [
            "ffprobe", "-v", "quiet", "-print_format", "json", "-show_entries",
            "stream=index,codec_name,disposition", "-select_streams", "s",
            video_path
        ]
        result = subprocess.run(cmd_probe_subs,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        sub_data = json.loads(result.stdout)

        best_ass, best_srt, best_pgs = None, None, None
        for stream in sub_data.get("streams", []):
            disposition = stream.get("disposition", {})
            is_normal = not any([
                disposition.get("comment"),
                disposition.get("hearing_impaired"),
                disposition.get("visual_impaired")
            ])
            if is_normal:
                codec_name = stream.get("codec_name")
                if codec_name == "ass" and not best_ass: best_ass = stream
                elif codec_name == "subrip" and not best_srt: best_srt = stream
                elif codec_name == "hdmv_pgs_subtitle" and not best_pgs:
                    best_pgs = stream

        chosen_sub_stream = best_ass or best_srt or best_pgs
        if not chosen_sub_stream:
            print("æœªæ‰¾åˆ°åˆé€‚çš„æ­£å¸¸å­—å¹•æµã€‚")
            return []

        sub_index, sub_codec = chosen_sub_stream.get(
            "index"), chosen_sub_stream.get("codec_name")
        print(f"   âœ… æ‰¾åˆ°æœ€ä¼˜å­—å¹•æµ (æ ¼å¼: {sub_codec.upper()})ï¼Œæµç´¢å¼•: {sub_index}")

    except Exception as e:
        print(f"æ¢æµ‹å­—å¹•æµå¤±è´¥: {e}")
        return []

    subtitle_events = []
    try:
        # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ ---
        # 1. å®šä¹‰æˆ‘ä»¬è¦æ¢æµ‹çš„æ—¶é—´ç‚¹ï¼ˆä¾‹å¦‚ï¼Œè§†é¢‘çš„20%, 40%, 60%, 80%ä½ç½®ï¼‰
        probe_points = [0.2, 0.4, 0.6, 0.8]
        # 2. å®šä¹‰åœ¨æ¯ä¸ªæ¢æµ‹ç‚¹é™„è¿‘æ‰«æå¤šé•¿æ—¶é—´ï¼ˆä¾‹å¦‚ï¼Œ60ç§’ï¼‰ï¼Œæ—¶é—´è¶Šé•¿ï¼Œæ‰¾åˆ°å­—å¹•äº‹ä»¶è¶Šå¤šï¼Œä½†è€—æ—¶ä¹Ÿè¶Šé•¿
        probe_duration = 60

        # 3. æ„å»º -read_intervals å‚æ•°
        # æ ¼å¼ä¸º "start1%+duration1,start2%+duration2,..."
        intervals = []
        for point in probe_points:
            start_time = duration * point
            end_time = start_time + probe_duration
            if end_time > duration:
                end_time = duration  # ç¡®ä¿ä¸è¶…è¿‡è§†é¢‘æ€»é•¿
            intervals.append(f"{start_time}%{end_time}")

        read_intervals_arg = ",".join(intervals)
        print(f"   ğŸš€ å°†åªæ‰«æä»¥ä¸‹æ—¶é—´æ®µæ¥å¯»æ‰¾å­—å¹•: {read_intervals_arg}")

        # 4. å°† -read_intervals å‚æ•°æ·»åŠ åˆ° ffprobe å‘½ä»¤ä¸­
        cmd_extract = [
            "ffprobe",
            "-v",
            "quiet",
            "-read_intervals",
            read_intervals_arg,  # <--- æ–°å¢çš„å‚æ•°
            "-print_format",
            "json",
            "-show_packets",
            "-select_streams",
            str(sub_index),
            video_path
        ]

        # æ‰§è¡Œå‘½ä»¤ï¼Œç°åœ¨å®ƒä¼šå¿«éå¸¸å¤š
        result = subprocess.run(cmd_extract,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        # --- ã€æ ¸å¿ƒä¿®æ”¹ç»“æŸã€‘ ---

        events_data = json.loads(result.stdout)
        packets = events_data.get("packets", [])

        # åç»­å¤„ç†é€»è¾‘åŸºæœ¬ä¸å˜
        if sub_codec in ["ass", "subrip"]:
            for packet in packets:
                try:
                    start, dur = float(packet.get("pts_time")), float(
                        packet.get("duration_time"))
                    if dur > 0.1:
                        subtitle_events.append({
                            "start": start,
                            "end": start + dur
                        })
                except (ValueError, TypeError):
                    continue
        elif sub_codec == "hdmv_pgs_subtitle":
            for i in range(0, len(packets) - 1, 2):
                try:
                    start, end = float(packets[i].get("pts_time")), float(
                        packets[i + 1].get("pts_time"))
                    if end > start and (end - start) > 0.1:
                        subtitle_events.append({"start": start, "end": end})
                except (ValueError, TypeError):
                    continue

        if not subtitle_events: raise ValueError("åœ¨æŒ‡å®šåŒºé—´å†…æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æ—¶é—´äº‹ä»¶ã€‚")
        print(f"   âœ… æˆåŠŸä»æŒ‡å®šåŒºé—´æå–åˆ° {len(subtitle_events)} æ¡æœ‰æ•ˆå­—å¹•äº‹ä»¶ã€‚")
    except Exception as e:
        print(f"æ™ºèƒ½æå–æ—¶é—´äº‹ä»¶å¤±è´¥: {e}")
        return []

    # åç»­çš„éšæœºé€‰æ‹©é€»è¾‘ä¿æŒä¸å˜
    if len(subtitle_events) < num_screenshots:
        print("æœ‰æ•ˆå­—å¹•æ•°é‡ä¸è¶³ï¼Œæ— æ³•å¯åŠ¨æ™ºèƒ½é€‰æ‹©ã€‚")
        return []

    golden_start_time, golden_end_time = duration * 0.30, duration * 0.80
    golden_events = [
        e for e in subtitle_events
        if e["start"] >= golden_start_time and e["end"] <= golden_end_time
    ]
    print(
        f"   -> åœ¨è§†é¢‘ä¸­éƒ¨ ({(golden_start_time):.2f}s - {(golden_end_time):.2f}s) æ‰¾åˆ° {len(golden_events)} ä¸ªé»„é‡‘å­—å¹•äº‹ä»¶ã€‚"
    )

    target_events = golden_events
    if len(target_events) < num_screenshots:
        print("   -> é»„é‡‘å­—å¹•æ•°é‡ä¸è¶³ï¼Œå°†ä»æ‰€æœ‰å­—å¹•äº‹ä»¶ä¸­éšæœºé€‰æ‹©ã€‚")
        target_events = subtitle_events

    chosen_events = random.sample(target_events,
                                  min(num_screenshots, len(target_events)))

    screenshot_points = []
    for event in chosen_events:
        event_duration = event["end"] - event["start"]
        random_offset = event_duration * 0.1 + random.random() * (
            event_duration * 0.8)
        random_point = event["start"] + random_offset
        screenshot_points.append(random_point)
        print(
            f"   -> é€‰ä¸­æ—¶é—´æ®µ [{(event['start']):.2f}s - {(event['end']):.2f}s], éšæœºæˆªå›¾ç‚¹: {(random_point):.2f}s"
        )

    return sorted(screenshot_points)


def _find_target_video_file(path: str) -> tuple[str | None, bool]:
    """
    æ ¹æ®è·¯å¾„æ™ºèƒ½æŸ¥æ‰¾ç›®æ ‡è§†é¢‘æ–‡ä»¶ï¼Œå¹¶æ£€æµ‹æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶ã€‚
    - ä¼˜å…ˆæ£€æŸ¥ç§å­åç§°åŒ¹é…çš„æ–‡ä»¶ï¼ˆå¤„ç†ç”µå½±ç›´æ¥æ”¾åœ¨ä¸‹è½½ç›®å½•æ ¹ç›®å½•çš„æƒ…å†µï¼‰
    - å¦‚æœæ˜¯ç”µå½±ç›®å½•ï¼Œè¿”å›æœ€å¤§çš„è§†é¢‘æ–‡ä»¶ã€‚
    - å¦‚æœæ˜¯å‰§é›†ç›®å½•ï¼Œè¿”å›æŒ‰åç§°æ’åºçš„ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚
    - æ£€æµ‹æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶ï¼ˆæ£€æŸ¥ BDMV/CERTIFICATE ç›®å½•ï¼‰

    :param path: è¦æœç´¢çš„ç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ã€‚
    :return: å…ƒç»„ (ç›®æ ‡è§†é¢‘æ–‡ä»¶çš„å®Œæ•´è·¯å¾„, æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶)
    """
    print(f"å¼€å§‹åœ¨è·¯å¾„ '{path}' ä¸­æŸ¥æ‰¾ç›®æ ‡è§†é¢‘æ–‡ä»¶...")
    VIDEO_EXTENSIONS = {
        ".mkv", ".mp4", ".ts", ".avi", ".wmv", ".mov", ".flv", ".m2ts"
    }

    if not os.path.exists(path):
        print(f"é”™è¯¯ï¼šæä¾›çš„è·¯å¾„ä¸å­˜åœ¨: {path}")
        return None, False

    # å¦‚æœæä¾›çš„è·¯å¾„æœ¬èº«å°±æ˜¯ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
    if os.path.isfile(path) and os.path.splitext(
            path)[1].lower() in VIDEO_EXTENSIONS:
        print(f"è·¯å¾„ç›´æ¥æŒ‡å‘ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå°†ä½¿ç”¨: {path}")
        return path, False

    if not os.path.isdir(path):
        print(f"é”™è¯¯ï¼šè·¯å¾„ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç›®å½•æˆ–è§†é¢‘æ–‡ä»¶: {path}")
        return None, False

    # æ£€æŸ¥æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶ï¼ˆæ£€æŸ¥ BDMV/CERTIFICATE ç›®å½•ï¼‰
    is_bluray_disc = False
    bdmv_path = os.path.join(path, "BDMV")
    certificate_path = os.path.join(path, "CERTIFICATE")

    if os.path.exists(bdmv_path) and os.path.isdir(bdmv_path):
        print(f"æ£€æµ‹åˆ° BDMV ç›®å½•: {bdmv_path}")
        if certificate_path and os.path.exists(
                certificate_path) and os.path.isdir(certificate_path):
            print(f"æ£€æµ‹åˆ° CERTIFICATE ç›®å½•: {certificate_path}")
            is_bluray_disc = True
            print("ç¡®è®¤ï¼šæ£€æµ‹åˆ°åŸç›˜æ–‡ä»¶ç»“æ„ (BDMV/CERTIFICATE)")
        else:
            print("è­¦å‘Šï¼šæ£€æµ‹åˆ° BDMV ç›®å½•ä½†æœªæ‰¾åˆ° CERTIFICATE ç›®å½•ï¼Œå¯èƒ½ä¸æ˜¯æ ‡å‡†åŸç›˜")

    # ä¼˜å…ˆæ£€æŸ¥ç§å­åç§°åŒ¹é…çš„æ–‡ä»¶ï¼ˆå¤„ç†ç”µå½±ç›´æ¥æ”¾åœ¨æ ¹ç›®å½•çš„æƒ…å†µï¼‰
    # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨æ²¡æœ‰æ–‡ä»¶å¤¹åŒ…è£¹çš„ç”µå½±æ–‡ä»¶
    parent_dir = os.path.dirname(path)
    file_name = os.path.basename(path)

    # æ£€æŸ¥çˆ¶ç›®å½•ä¸­æ˜¯å¦æœ‰åŒ¹é…çš„æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    if parent_dir != path:  # ç¡®ä¿è¿™ä¸æ˜¯æ ¹ç›®å½•çš„æƒ…å†µ
        try:
            for file in os.listdir(parent_dir):
                if not file.startswith('.') and not os.path.isdir(
                        os.path.join(parent_dir, file)):
                    if os.path.splitext(file)[1].lower() in VIDEO_EXTENSIONS:
                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…ï¼ˆå¿½ç•¥æ‰©å±•åï¼‰
                        file_name_without_ext = os.path.splitext(file)[0]
                        if (file_name in file_name_without_ext
                                or file_name_without_ext in file_name
                                or file_name.replace(' ', '')
                                in file_name_without_ext.replace(' ', '')
                                or file_name_without_ext.replace(
                                    ' ', '') in file_name.replace(' ', '')):
                            full_path = os.path.join(parent_dir, file)
                            print(f"æ‰¾åˆ°åŒ¹é…çš„è§†é¢‘æ–‡ä»¶: {full_path}")
                            return full_path, is_bluray_disc
        except OSError as e:
            print(f"è¯»å–çˆ¶ç›®å½•å¤±è´¥: {e}")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼Œç»§ç»­åŸæ¥çš„æŸ¥æ‰¾é€»è¾‘
    video_files = []
    for root, _, files in os.walk(path):
        for file in files:
            if os.path.splitext(file)[1].lower() in VIDEO_EXTENSIONS:
                video_files.append(os.path.join(root, file))

    if not video_files:
        print(f"åœ¨ç›®å½• '{path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶ã€‚")
        return None, is_bluray_disc

    # å¦‚æœåªæœ‰ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨
    if len(video_files) == 1:
        print(f"æ‰¾åˆ°å”¯ä¸€çš„è§†é¢‘æ–‡ä»¶: {video_files[0]}")
        return video_files[0], is_bluray_disc

    # å¦‚æœæœ‰å¤šä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå°è¯•æ‰¾åˆ°æœ€åŒ¹é…çš„æ–‡ä»¶å
    best_match = ""
    best_score = -1
    for video_file in video_files:
        base_name = os.path.basename(video_file).lower()
        path_name = file_name.lower()

        # è®¡ç®—åŒ¹é…åº¦
        score = 0
        if path_name in base_name:
            score += 10
        if base_name in path_name:
            score += 5

        # é•¿åº¦è¶Šæ¥è¿‘ï¼Œå¾—åˆ†è¶Šé«˜
        if abs(len(base_name) - len(path_name)) < 5:
            score += 3

        if score > best_score:
            best_score = score
            best_match = video_file

    if best_match and best_score > 0:
        print(f"é€‰æ‹©æœ€ä½³åŒ¹é…çš„è§†é¢‘æ–‡ä»¶: {best_match} (åŒ¹é…åº¦: {best_score})")
        return best_match, is_bluray_disc

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¥½çš„åŒ¹é…ï¼Œé€‰æ‹©æœ€å¤§çš„æ–‡ä»¶
    largest_file = ""
    max_size = -1
    for f in video_files:
        try:
            size = os.path.getsize(f)
            if size > max_size:
                max_size = size
                largest_file = f
        except OSError as e:
            print(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° '{f}': {e}")
            continue

    if largest_file:
        print(f"å·²é€‰æ‹©æœ€å¤§æ–‡ä»¶ ({(max_size / 1024**3):.2f} GB): {largest_file}")
        return largest_file, is_bluray_disc
    else:
        print("æ— æ³•ç¡®å®šæœ€å¤§çš„æ–‡ä»¶ã€‚")
        return None, is_bluray_disc


# --- [ä¿®æ”¹] ä¸»å‡½æ•°ï¼Œæ•´åˆäº†æ–°çš„æ–‡ä»¶æŸ¥æ‰¾é€»è¾‘ ---
def upload_data_mediaInfo(mediaInfo: str,
                          save_path: str,
                          content_name: str = None,
                          downloader_id: str = None,
                          torrent_name: str = None,
                          force_refresh: bool = False):
    """
    æ£€æŸ¥ä¼ å…¥çš„æ–‡æœ¬æ˜¯æœ‰æ•ˆçš„ MediaInfo è¿˜æ˜¯ BDInfo æ ¼å¼ã€‚
    å¦‚æœæ²¡æœ‰ MediaInfo æˆ– BDInfo åˆ™å°è¯•ä» save_path æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶æå– MediaInfoã€‚
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ torrent_name (å®é™…æ–‡ä»¶å¤¹å) æˆ– content_name (è§£æåçš„æ ‡é¢˜) æ¥æ„å»ºæ›´ç²¾ç¡®çš„æœç´¢è·¯å¾„ã€‚
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ downloader_id æ¥åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä»£ç†è·å– MediaInfo
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ force_refresh å¼ºåˆ¶é‡æ–°è·å– MediaInfoï¼Œå¿½ç•¥å·²æœ‰çš„æœ‰æ•ˆæ ¼å¼
    """
    print("å¼€å§‹æ£€æŸ¥ MediaInfo/BDInfo æ ¼å¼")
    print(f"æä¾›çš„ MediaInfo: {mediaInfo[:80]}...")  # æ‰“å°éƒ¨åˆ†MediaInfo

    # 1. (æ­¤éƒ¨åˆ†ä»£ç ä¸å˜) ...
    standard_mediainfo_keywords = [
        "General",
        "Video",
        "Audio",
        "Complete name",
        "File size",
        "Duration",
        "Width",
        "Height",
    ]
    bdinfo_required_keywords = ["DISC INFO", "PLAYLIST REPORT"]
    bdinfo_optional_keywords = [
        "VIDEO:",
        "AUDIO:",
        "SUBTITLES:",
        "FILES:",
        "Disc Label",
        "Disc Size",
        "BDInfo:",
        "Protection:",
        "Codec",
        "Bitrate",
        "Language",
        "Description",
    ]
    mediainfo_matches = sum(1 for keyword in standard_mediainfo_keywords
                            if keyword in mediaInfo)
    is_standard_mediainfo = mediainfo_matches >= 3
    bdinfo_required_matches = sum(1 for keyword in bdinfo_required_keywords
                                  if keyword in mediaInfo)
    bdinfo_optional_matches = sum(1 for keyword in bdinfo_optional_keywords
                                  if keyword in mediaInfo)
    is_bdinfo = (bdinfo_required_matches == len(bdinfo_required_keywords)) or \
                (bdinfo_required_matches >= 1 and bdinfo_optional_matches >= 2)

    if is_standard_mediainfo:
        if force_refresh:
            print(f"æ£€æµ‹åˆ°æ ‡å‡† MediaInfo æ ¼å¼ï¼Œä½†è®¾ç½®äº†å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡æ–°æå–ã€‚")
            # ä¸returnï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„æå–é€»è¾‘
        else:
            print(f"æ£€æµ‹åˆ°æ ‡å‡† MediaInfo æ ¼å¼ï¼ŒéªŒè¯é€šè¿‡ã€‚(åŒ¹é…å…³é”®å­—æ•°: {mediainfo_matches})")
            return mediaInfo
    elif is_bdinfo:
        if force_refresh:
            print(f"æ£€æµ‹åˆ° BDInfo æ ¼å¼ï¼Œä½†è®¾ç½®äº†å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡æ–°æå–ã€‚")
            # ä¸returnï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„æå–é€»è¾‘
        else:
            print(
                f"æ£€æµ‹åˆ° BDInfo æ ¼å¼ï¼ŒéªŒè¯é€šè¿‡ã€‚(å¿…è¦å…³é”®å­—: {bdinfo_required_matches}/{len(bdinfo_required_keywords)}, å¯é€‰å…³é”®å­—: {bdinfo_optional_matches})"
            )
            return mediaInfo
    elif not force_refresh:
        # åªæœ‰åœ¨ä¸æ˜¯å¼ºåˆ¶åˆ·æ–°æ—¶æ‰æ‰“å°è¿™ä¸ªæ¶ˆæ¯
        print("æä¾›çš„æ–‡æœ¬ä¸æ˜¯æœ‰æ•ˆçš„ MediaInfo/BDInfoï¼Œå°†å°è¯•ä»æœ¬åœ°æ–‡ä»¶æå–ã€‚")

    # å¦‚æœæ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜éœ€è¦é‡æ–°æå–ï¼ˆforce_refresh=True æˆ–è€…æ²¡æœ‰æœ‰æ•ˆæ ¼å¼ï¼‰
    if not save_path:
        print("é”™è¯¯ï¼šæœªæä¾› save_pathï¼Œæ— æ³•ä»æ–‡ä»¶æå– MediaInfoã€‚")
        return mediaInfo

    # --- ã€ä»£ç†æ£€æŸ¥å’Œå¤„ç†é€»è¾‘ã€‘ ---
    proxy_config = _get_downloader_proxy_config(downloader_id)

    if proxy_config:
        print(f"ä½¿ç”¨ä»£ç†å¤„ç† MediaInfo: {proxy_config['proxy_base_url']}")
        # æ„å»ºå®Œæ•´è·¯å¾„å‘é€ç»™ä»£ç†
        remote_path = save_path
        if torrent_name:
            remote_path = os.path.join(save_path, torrent_name)
            print(f"å·²æä¾› torrent_nameï¼Œå°†ä½¿ç”¨å®Œæ•´è·¯å¾„: '{remote_path}'")
        elif content_name:
            remote_path = os.path.join(save_path, content_name)
            print(f"å·²æä¾› content_nameï¼Œå°†ä½¿ç”¨æ‹¼æ¥è·¯å¾„: '{remote_path}'")

        try:
            response = requests.post(
                f"{proxy_config['proxy_base_url']}/api/media/mediainfo",
                json={"remote_path": remote_path},
                timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            response.raise_for_status()
            result = response.json()
            if result.get("success"):
                print("é€šè¿‡ä»£ç†è·å– MediaInfo æˆåŠŸ")
                proxy_mediainfo = result.get("mediainfo", mediaInfo)
                # å¤„ç†ä»£ç†è¿”å›çš„ MediaInfoï¼Œåªä¿ç•™ Complete name ä¸­çš„æ–‡ä»¶å
                proxy_mediainfo = re.sub(
                    r'(Complete name\s*:\s*)(.+)', lambda m:
                    f"{m.group(1)}{os.path.basename(m.group(2).strip())}",
                    proxy_mediainfo)
                return proxy_mediainfo
            else:
                print(f"é€šè¿‡ä»£ç†è·å– MediaInfo å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        except Exception as e:
            print(f"é€šè¿‡ä»£ç†è·å– MediaInfo å¤±è´¥: {e}")

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä»¿ç…§æˆªå›¾é€»è¾‘ï¼Œæ„å»ºç²¾ç¡®çš„æœç´¢è·¯å¾„ ---
    # é¦–å…ˆåº”ç”¨è·¯å¾„æ˜ å°„è½¬æ¢
    translated_save_path = translate_path(downloader_id, save_path)
    if translated_save_path != save_path:
        print(f"è·¯å¾„æ˜ å°„: {save_path} -> {translated_save_path}")

    path_to_search = translated_save_path  # ä½¿ç”¨è½¬æ¢åçš„è·¯å¾„
    # ä¼˜å…ˆä½¿ç”¨ torrent_name (å®é™…æ–‡ä»¶å¤¹å)ï¼Œå¦‚æœä¸å­˜åœ¨å†ä½¿ç”¨ content_name (è§£æåçš„æ ‡é¢˜)
    if torrent_name:
        path_to_search = os.path.join(translated_save_path, torrent_name)
        print(f"å·²æä¾› torrent_nameï¼Œå°†åœ¨ç²¾ç¡®è·¯å¾„ä¸­æœç´¢: '{path_to_search}'")
    elif content_name:
        # å¦‚æœæä¾›äº†å…·ä½“çš„å†…å®¹åç§°ï¼ˆä¸»æ ‡é¢˜ï¼‰ï¼Œåˆ™æ‹¼æ¥æˆä¸€ä¸ªæ›´ç²¾ç¡®çš„è·¯å¾„
        path_to_search = os.path.join(translated_save_path, content_name)
        print(f"å·²æä¾› content_nameï¼Œå°†åœ¨ç²¾ç¡®è·¯å¾„ä¸­æœç´¢: '{path_to_search}'")

    # ä½¿ç”¨æ–°æ„å»ºçš„è·¯å¾„æ¥æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
    target_video_file, is_bluray_disc = _find_target_video_file(path_to_search)

    if not target_video_file:
        print("æœªèƒ½åœ¨æŒ‡å®šè·¯å¾„ä¸­æ‰¾åˆ°åˆé€‚çš„è§†é¢‘æ–‡ä»¶ï¼Œæå–å¤±è´¥ã€‚")
        return mediaInfo

    # æ£€æŸ¥æ˜¯å¦ä¸ºåŸç›˜æ–‡ä»¶
    if is_bluray_disc:
        print("æ£€æµ‹åˆ°åŸç›˜æ–‡ä»¶ç»“æ„ (BDMV/CERTIFICATE)ï¼Œè¿”å›æŒ‡å®šæ¶ˆæ¯")
        return "bdinfoæå–æš‚æœªå®ç°ï¼Œè¯·æ‰‹åŠ¨è·å–ã€‚"

    try:
        print(f"å‡†å¤‡ä½¿ç”¨ MediaInfo å·¥å…·ä» '{target_video_file}' æå–...")
        media_info_parsed = MediaInfo.parse(target_video_file,
                                            output="text",
                                            full=False)
        # å¤„ç† Complete nameï¼Œåªä¿ç•™æœ€åä¸€ä¸ª / ä¹‹åçš„å†…å®¹
        media_info_str = str(media_info_parsed)
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ Complete name è¡Œä¸­çš„å®Œæ•´è·¯å¾„ä¸ºæ–‡ä»¶å
        media_info_str = re.sub(
            r'(Complete name\s*:\s*)(.+)',
            lambda m: f"{m.group(1)}{os.path.basename(m.group(2).strip())}",
            media_info_str)
        print("ä»æ–‡ä»¶é‡æ–°æå– MediaInfo æˆåŠŸã€‚")
        return media_info_str
    except Exception as e:
        print(f"ä»æ–‡ä»¶ '{target_video_file}' å¤„ç†æ—¶å‡ºé”™: {e}ã€‚å°†è¿”å›åŸå§‹ mediainfoã€‚")
        return mediaInfo


def upload_data_title(title: str, torrent_filename: str = ""):
    """
    ä»ç§å­ä¸»æ ‡é¢˜ä¸­æå–æ‰€æœ‰å‚æ•°ï¼Œå¹¶å¯é€‰åœ°ä»ç§å­æ–‡ä»¶åä¸­è¡¥å……ç¼ºå¤±å‚æ•°ã€‚
    """
    print(f"å¼€å§‹ä»ä¸»æ ‡é¢˜è§£æå‚æ•°: {title}")

    # 1. é¢„å¤„ç†
    original_title_str = title.strip()
    params = {}
    unrecognized_parts = []

    # æ³¨é‡Šæ‰åŸæœ‰çš„ç²—æš´æˆªæ–­é€»è¾‘ï¼Œå› ä¸ºå®ƒä¼šé”™è¯¯åœ°ç§»é™¤åˆ¶ä½œç»„åç§°ä¸­çš„ä¸­æ–‡éƒ¨åˆ†
    # ä¾‹å¦‚ "-FFans@wsæ—å°å‡¡" ä¼šè¢«é”™è¯¯æˆªæ–­
    # ç°åœ¨åªåœ¨æ ‡é¢˜ä¸­å­˜åœ¨ç‹¬ç«‹çš„ä¸­æ–‡ç‰‡æ®µï¼ˆä¸ä¸åˆ¶ä½œç»„ç›¸è¿ï¼‰æ—¶æ‰æ ‡è®°ä¸ºæ— æ³•è¯†åˆ«
    # chinese_junk_match = re.search(r"([\u4e00-\u9fa5].*)$", original_title_str)
    # if chinese_junk_match:
    #     unrecognized_parts.append(chinese_junk_match.group(1).strip())
    #     title = original_title_str[:chinese_junk_match.start()].strip()
    # else:
    #     title = original_title_str

    # ä¿æŒåŸå§‹æ ‡é¢˜ï¼Œè®©åç»­çš„åˆ¶ä½œç»„æå–é€»è¾‘æ¥å¤„ç†
    title = original_title_str

    title = re.sub(r"[ï¿¡â‚¬]", "", title)
    title = re.sub(r"\s*å‰©é¤˜æ™‚é–“.*$", "", title)
    title = re.sub(r"[\s\.]*(mkv|mp4)$", "", title,
                   flags=re.IGNORECASE).strip()
    title = re.sub(r"\[.*?\]|ã€.*?ã€‘", "", title).strip()
    title = title.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
    title = title.replace("'", "")
    title = re.sub(r"(\d+[pi])([A-Z])", r"\1 \2", title)

    # 2. ä¼˜å…ˆæå–åˆ¶ä½œç»„ä¿¡æ¯
    release_group = ""
    main_part = title

    # æ£€æŸ¥ç‰¹æ®Šåˆ¶ä½œç»„ï¼ˆå®Œæ•´åŒ¹é…ï¼‰
    special_groups = ["mUHD-FRDS", "MNHD-FRDS", "DMG&VCB-Studio", "VCB-Studio"]
    found_special_group = False
    for group in special_groups:
        if title.endswith(f" {group}") or title.endswith(f"-{group}"):
            release_group = group
            main_part = title[:-len(group) - 1].strip()
            found_special_group = True
            break

    # å¦‚æœä¸æ˜¯ç‰¹æ®Šåˆ¶ä½œç»„ï¼Œå…ˆå°è¯•åŒ¹é… VCB-Studio å˜ä½“
    if not found_special_group:
        # åŒ¹é…ç±»ä¼¼ -Nekomoe kissaten&VCB-Studio, -LoliHouse&VCB-Studio ç­‰æ ¼å¼
        # è¿™ä¸ªæ­£åˆ™ä¼šåŒ¹é… - å¼€å¤´ï¼Œä¸­é—´å¯èƒ½æœ‰å¤šä¸ªå•è¯ï¼ˆåŒ…å«ç©ºæ ¼ï¼‰ã€&ç¬¦å·ï¼Œæœ€åä»¥ VCB-Studio ç»“å°¾
        vcb_variant_pattern = re.compile(
            r"^(?P<main_part>.+?)[-](?P<release_group>[\w\s]+&VCB-Studio)$",
            re.IGNORECASE)
        vcb_match = vcb_variant_pattern.match(title)
        if vcb_match:
            main_part = vcb_match.group("main_part").strip()
            release_group = vcb_match.group("release_group")
            found_special_group = True
            print(f"æ£€æµ‹åˆ° VCB-Studio å˜ä½“åˆ¶ä½œç»„: {release_group}")

    # å¦‚æœè¿˜ä¸æ˜¯ç‰¹æ®Šåˆ¶ä½œç»„ï¼Œä½¿ç”¨é€šç”¨æ¨¡å¼åŒ¹é…
    if not found_special_group:
        # ä¿®å¤ï¼šæ”¹ç”¨è´ªå©ªåŒ¹é…ï¼Œç›´æ¥æå–æœ€åä¸€ä¸ª - æˆ– @ ä¹‹åçš„æ‰€æœ‰å†…å®¹ä½œä¸ºåˆ¶ä½œç»„
        # è¿™æ ·å¯ä»¥æ­£ç¡®å¤„ç†åŒ…å«ä¸­æ–‡ã€@ã€- ç­‰å¤æ‚å­—ç¬¦çš„åˆ¶ä½œç»„åç§°
        general_regex = re.compile(
            r"^(?P<main_part>.+?)[-@](?P<release_group>[^\s]+)$",
            re.IGNORECASE,
        )
        print(f"[è°ƒè¯•] å°è¯•åŒ¹é…åˆ¶ä½œç»„ï¼Œæ ‡é¢˜: {title}")
        match = general_regex.match(title)
        if match:
            main_part = match.group("main_part").strip()
            release_group = match.group("release_group").strip()
            print(f"[è°ƒè¯•] æ­£åˆ™åŒ¹é…æˆåŠŸ!")
            print(f"[è°ƒè¯•]   - main_part: {main_part}")
            print(f"[è°ƒè¯•]   - release_group: {release_group}")
            print(f"[è°ƒè¯•] æœ€ç»ˆåˆ¶ä½œç»„: {release_group}")
        else:
            # æ£€æŸ¥æ˜¯å¦ä»¥-NOGROUPç»“å°¾
            if title.upper().endswith("-NOGROUP"):
                release_group = "NOGROUP"
                main_part = title[:-8].strip()
            else:
                release_group = "N/A (æ— å‘å¸ƒç»„)"

    # 3. å­£é›†ã€å¹´ä»½ã€å‰ªè¾‘ç‰ˆæœ¬æå–
    season_match = re.search(
        r"(?<!\w)(S\d{1,2}(?:(?:[-â€“~]\s*S?\d{1,2})?|(?:\s*E\d{1,3}(?:[-â€“~]\s*(?:S\d{1,2})?E?\d{1,3})*)?))(?!\w)",
        main_part,
        re.I,
    )
    if season_match:
        season_str = season_match.group(1)
        main_part = main_part.replace(season_str, " ").strip()
        params["season_episode"] = re.sub(r"\s", "", season_str.upper())

    title_part = main_part
    year_match = re.search(r"[\s\.\(]((?:19|20)\d{2})([\s\.\)]|$)", title_part)
    if year_match:
        params["year"] = year_match.group(1)
        title_part = title_part.replace(year_match.group(0), " ", 1).strip()

    # 4.1 æå–å‰ªè¾‘ç‰ˆæœ¬å¹¶æ‹¼æ¥åˆ°å¹´ä»½
    cut_version_pattern = re.compile(
        r"(?<!\w)(Theatrical[\s\.]?Cut|Directors?[\s\.]?Cut|DC|Extended[\s\.]?(?:Cut|Edition)|Special[\s\.]?Edition|SE|Final[\s\.]?Cut|Anniversary[\s\.]?Edition|Restored|Remastered|Criterion[\s\.]?(?:Edition|Collection)|Ultimate[\s\.]?Cut|IMAX[\s\.]?Edition|Open[\s\.]?Matte|Unrated[\s\.]?Cut)(?!\w)",
        re.IGNORECASE)
    cut_version_match = cut_version_pattern.search(title_part)
    if cut_version_match:
        cut_version = re.sub(r'[\s\.]+', ' ',
                             cut_version_match.group(1).strip())
        # å°†å‰ªè¾‘ç‰ˆæœ¬æ‹¼æ¥åˆ°å¹´ä»½
        if "year" in params:
            params["year"] = f"{params['year']} {cut_version}"
        else:
            # å¦‚æœæ²¡æœ‰å¹´ä»½ï¼Œå•ç‹¬ä½œä¸ºå¹´ä»½å­—æ®µ
            params["year"] = cut_version
        # ä»æ ‡é¢˜éƒ¨åˆ†ç§»é™¤å‰ªè¾‘ç‰ˆæœ¬
        title_part = title_part.replace(cut_version_match.group(0), " ",
                                        1).strip()
        print(f"æ£€æµ‹åˆ°å‰ªè¾‘ç‰ˆæœ¬: {cut_version}ï¼Œå·²æ‹¼æ¥åˆ°å¹´ä»½")

    # 4. é¢„å¤„ç†æ ‡é¢˜ï¼šä¿®å¤éŸ³é¢‘å‚æ•°æ ¼å¼
    # å…ˆå¤„ç†ç¼ºå°‘ç‚¹çš„æƒ…å†µï¼Œå¦‚ FLAC 20 -> FLAC 2.0, FLAC 2 0 -> FLAC 2.0
    title_part = re.sub(r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))\s*(\d)\s*(\d)",
                        r"\1 \2.\3",
                        title_part,
                        flags=re.I)
    # å†å¤„ç†æ²¡æœ‰ç©ºæ ¼çš„æƒ…å†µï¼Œå¦‚ FLAC2.0 -> FLAC 2.0, DTS5.1 -> DTS 5.1
    title_part = re.sub(r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))(\d(?:\.\d)?)",
                        r"\1 \2",
                        title_part,
                        flags=re.I)

    # æŠ€æœ¯æ ‡ç­¾æå–ï¼ˆæ’é™¤å·²è¯†åˆ«çš„åˆ¶ä½œç»„åç§°ï¼‰
    tech_patterns_definitions = {
        "medium":
        r"UHDTV|UHD\s*Blu-?ray|Blu-?ray\s+DIY|Blu-ray|BluRay\s+DIY|BluRay|BDrip|BD-?rip|WEB-DL|WEBrip|TVrip|DVDRip|HDTV",
        "audio":
        r"DTS-HD(?:\s*MA)?(?:\s*\d\.\d)?|(?:Dolby\s*)?TrueHD(?:\s*Atmos)?(?:\s*\d\.\d)?|Atmos(?:\s*TrueHD)?(?:\s*\d\.\d)?|DTS(?:\s*\d\.\d)?|DDP(?:\s*\d\.\d)?|DD\+(?:\s*\d\.\d)?|DD(?:\s*\d\.\d)?|AC3(?:\s*\d\.\d)?|FLAC(?:\s*\d\.\d)?|AAC(?:\s*\d\.\d)?|LPCM(?:\s*\d\.\d)?|AV3A\s*\d\.\d|\d+\s*Audios?|MP2|DUAL",
        "hdr_format":
        r"Dolby Vision|DoVi|HDR10\+|HDRVivid|HDR10|HLG|HDR|SDR|DV|Vivid",
        "resolution": r"\d{3,4}[pi]|4K",
        "video_codec":
        r"HEVC|AVC|x265|H\s*[\s\.]?\s*265|x264|H\s*[\s\.]?\s*264|VC-1|AV1|MPEG-2",
        "source_platform":
        r"Apple TV\+|ViuTV|MyTVSuper|MyVideo|AMZN|Netflix|NF|DSNP|MAX|ATVP|iTunes|friDay|USA|EUR|JPN|CEE|FRA|LINETV|EDR|PCOK|Hami|GBR|NowPlayer|CR|SEEZN|GER|CHN|MA|Viu|Baha|KKTV|IQ|HKG|ITA|ESP",
        "bit_depth": r"\b(?:8|10)bit\b",
        "framerate": r"\d{2,3}fps",
        "completion_status": r"Complete|COMPLETE",
        "video_format": r"3D|HSBS",
        "release_version": r"REMASTERED|REPACK|RERIP|PROPER|REPOST|V\d+",
        "cut_version":
        r"Theatrical[\s\.]?Cut|Directors?[\s\.]?Cut|DC|Extended[\s\.]?(?:Cut|Edition)|Special[\s\.]?Edition|SE|Final[\s\.]?Cut|Anniversary[\s\.]?Edition|Restored|Remastered|Criterion[\s\.]?(?:Edition|Collection)|Ultimate[\s\.]?Cut|IMAX[\s\.]?Edition|Open[\s\.]?Matte|Unrated[\s\.]?Cut",
        "quality_modifier": r"MAXPLUS|HQ|EXTENDED|REMUX|EE|MiniBD",
    }
    priority_order = [
        "completion_status",
        "release_version",
        "cut_version",
        "medium",
        "resolution",
        "video_codec",
        "bit_depth",
        "hdr_format",
        "video_format",
        "framerate",
        "source_platform",
        "audio",
        "quality_modifier",
    ]

    title_candidate = title_part
    first_tech_tag_pos = len(title_candidate)
    all_found_tags = []

    # æ„å»ºåˆ¶ä½œç»„çš„å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºåç»­è¿‡æ»¤
    release_group_keywords = []
    if release_group and release_group != "N/A (æ— å‘å¸ƒç»„)":
        # å°†åˆ¶ä½œç»„åç§°æŒ‰@å’Œå…¶ä»–åˆ†éš”ç¬¦æ‹†åˆ†ï¼Œè·å–æ‰€æœ‰ç»„æˆéƒ¨åˆ†
        # ä¾‹å¦‚ "DIY@Audies" -> ["DIY", "Audies"]
        release_group_keywords = re.split(r'[@\-\s]+', release_group)
        release_group_keywords = [
            kw.strip() for kw in release_group_keywords if kw.strip()
        ]
        print(f"[è°ƒè¯•] åˆ¶ä½œç»„å…³é”®è¯åˆ—è¡¨: {release_group_keywords}")

    for key in priority_order:
        pattern = tech_patterns_definitions[key]
        search_pattern = (re.compile(r"(?<!\w)(" + pattern + r")(?!\w)",
                                     re.IGNORECASE) if r"\b" not in pattern
                          else re.compile(pattern, re.IGNORECASE))
        matches = list(search_pattern.finditer(title_candidate))
        if not matches:
            continue

        first_tech_tag_pos = min(first_tech_tag_pos, matches[0].start())
        raw_values = [
            m.group(0).strip() if r"\b" in pattern else m.group(1).strip()
            for m in matches
        ]

        # è¿‡æ»¤æ‰å±äºåˆ¶ä½œç»„åç§°çš„éƒ¨åˆ†
        filtered_values = []
        for val in raw_values:
            # æ£€æŸ¥è¿™ä¸ªå€¼æ˜¯å¦æ˜¯åˆ¶ä½œç»„å…³é”®è¯ä¹‹ä¸€
            is_release_group_part = any(val.upper() == kw.upper()
                                        for kw in release_group_keywords)
            if is_release_group_part:
                print(f"[è°ƒè¯•] è¿‡æ»¤æ‰åˆ¶ä½œç»„å…³é”®è¯: {val} (å±äº {key})")
            if not is_release_group_part:
                filtered_values.append(val)

        all_found_tags.extend(filtered_values)
        if filtered_values:
            print(f"[è°ƒè¯•] '{key}' å­—æ®µæå–åˆ°æŠ€æœ¯æ ‡ç­¾: {filtered_values}")
        raw_values = filtered_values
        processed_values = (
            [re.sub(r"(DD)\+", r"\1+", val, flags=re.I)
             for val in raw_values] if key == "audio" else raw_values)
        if key == "audio":
            processed_values = [
                # å…ˆå¤„ç†ç¼ºå°‘ç‚¹çš„æƒ…å†µï¼Œå¦‚ FLAC 20 -> FLAC 2.0, FLAC 2 0 -> FLAC 2.0, DTS 51 -> DTS 5.1
                re.sub(r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))\s*(\d)\s*(\d)",
                       r"\1 \2.\3",
                       val,
                       flags=re.I) for val in processed_values
            ]
            processed_values = [
                # å†å¤„ç†æ²¡æœ‰ç©ºæ ¼çš„æƒ…å†µï¼Œå¦‚ FLAC2.0 -> FLAC 2.0, DTS5.1 -> DTS 5.1
                re.sub(r"((?:DTS|FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))(\d(?:\.\d)?)",
                       r"\1 \2",
                       val,
                       flags=re.I) for val in processed_values
            ]

        unique_processed = sorted(
            list(set(processed_values)),
            key=lambda x: title_candidate.find(x.replace(" ", "")))
        if unique_processed:
            params[key] = unique_processed[0] if len(
                unique_processed) == 1 else unique_processed

    # --- [æ–°å¢] å¼€å§‹: ä»ç§å­æ–‡ä»¶åè¡¥å……ç¼ºå¤±çš„å‚æ•° ---
    if torrent_filename:
        print(f"å¼€å§‹ä»ç§å­æ–‡ä»¶åè¡¥å……å‚æ•°: {torrent_filename}")
        # é¢„å¤„ç†æ–‡ä»¶åï¼šç§»é™¤åç¼€ï¼Œç”¨ç©ºæ ¼æ›¿æ¢ç‚¹å’Œå…¶ä»–å¸¸ç”¨åˆ†éš”ç¬¦
        filename_base = re.sub(r'(\.original)?\.torrent',
                               '',
                               torrent_filename,
                               flags=re.IGNORECASE)
        filename_candidate = re.sub(r'[\._\[\]\(\)]', ' ', filename_base)

        # å†æ¬¡éå†æ‰€æœ‰æŠ€æœ¯æ ‡ç­¾å®šä¹‰ï¼Œä»¥è¡¥å……ä¿¡æ¯
        for key in priority_order:
            # å¦‚æœä¸»æ ‡é¢˜ä¸­å·²è§£æå‡ºæ­¤å‚æ•°ï¼Œåˆ™è·³è¿‡ï¼Œä¼˜å…ˆä½¿ç”¨ä¸»æ ‡é¢˜çš„ç»“æœ
            if key in params and params.get(key):
                continue

            pattern = tech_patterns_definitions[key]
            search_pattern = (re.compile(r"(?<!\w)(" + pattern + r")(?!\w)",
                                         re.IGNORECASE) if r"\b" not in pattern
                              else re.compile(pattern, re.IGNORECASE))

            matches = list(search_pattern.finditer(filename_candidate))
            if matches:
                # æå–æ‰€æœ‰åŒ¹é…åˆ°çš„å€¼
                raw_values = [
                    m.group(0).strip()
                    if r"\b" in pattern else m.group(1).strip()
                    for m in matches
                ]

                # (å¤åˆ¶ä¸»è§£æé€»è¾‘ä¸­çš„ audio ç‰¹æ®Šå¤„ç†)
                processed_values = ([
                    re.sub(r"(DD)\\+", r"\1+", val, flags=re.I)
                    for val in raw_values
                ] if key == "audio" else raw_values)
                if key == "audio":
                    processed_values = [
                        # å…ˆå¤„ç†ç¼ºå°‘ç‚¹çš„æƒ…å†µï¼Œå¦‚ FLAC 20 -> FLAC 2.0, FLAC 2 0 -> FLAC 2.0
                        re.sub(
                            r"((?:FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))\s*(\d)\s*(\d)",
                            r"\1 \2.\3",
                            val,
                            flags=re.I) for val in processed_values
                    ]
                    processed_values = [
                        # å†å¤„ç†æ²¡æœ‰ç©ºæ ¼çš„æƒ…å†µï¼Œå¦‚ FLAC2.0 -> FLAC 2.0
                        re.sub(
                            r"((?:FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))(\d(?:\.\d)?)",
                            r"\1 \2",
                            val,
                            flags=re.I) for val in processed_values
                    ]

                # å–ç‹¬ä¸€æ— äºŒçš„å€¼å¹¶æŒ‰å‡ºç°é¡ºåºæ’åº
                unique_processed = sorted(
                    list(set(processed_values)),
                    key=lambda x: filename_candidate.find(x.replace(" ", "")))

                if unique_processed:
                    print(f"   [æ–‡ä»¶åè¡¥å……] æ‰¾åˆ°ç¼ºå¤±å‚æ•° '{key}': {unique_processed}")
                    # å°†è¡¥å……çš„å‚æ•°å­˜å…¥ params å­—å…¸
                    params[key] = unique_processed[0] if len(
                        unique_processed) == 1 else unique_processed
                    # å°†æ–°æ‰¾åˆ°çš„æ ‡ç­¾ä¹ŸåŠ å…¥ all_found_tagsï¼Œä»¥ä¾¿åç»­æ­£ç¡®è®¡ç®—"æ— æ³•è¯†åˆ«"éƒ¨åˆ†
                    all_found_tags.extend(unique_processed)
    # --- [æ–°å¢] ç»“æŸ ---

    # å°†åˆ¶ä½œç»„ä¿¡æ¯æ·»åŠ åˆ°æœ€åçš„å‚æ•°ä¸­
    params["release_info"] = release_group

    if "quality_modifier" in params:
        modifiers = params.pop("quality_modifier")
        if not isinstance(modifiers, list):
            modifiers = [modifiers]
        if "medium" in params:
            medium_str = (params["medium"] if isinstance(
                params["medium"], str) else params["medium"][0])
            params["medium"] = f"{medium_str} {' '.join(sorted(modifiers))}"

    # 5. æœ€ç»ˆæ ‡é¢˜å’Œæœªè¯†åˆ«å†…å®¹ç¡®å®š
    title_zone = title_part[:first_tech_tag_pos].strip()
    tech_zone = title_part[first_tech_tag_pos:].strip()
    params["title"] = re.sub(r"[\s\.]+", " ", title_zone).strip()

    print(f"[è°ƒè¯•] å¼€å§‹æ¸…ç†æŠ€æœ¯åŒºåŸŸï¼ŒåŸå§‹æŠ€æœ¯åŒº: '{tech_zone}'")
    print(f"[è°ƒè¯•] æ‰€æœ‰å·²è¯†åˆ«æ ‡ç­¾: {all_found_tags}")

    cleaned_tech_zone = tech_zone
    for tag in sorted(all_found_tags, key=len, reverse=True):
        # å¯¹äºåŒ…å«ä¸­æ–‡çš„æ ‡ç­¾ï¼Œä¸ä½¿ç”¨ \b è¯è¾¹ç•Œ
        # ä½¿ç”¨æ›´é€šç”¨çš„æ¨¡å¼ï¼š(?<!\w) å’Œ (?!\w) æ¥åŒ¹é…éå­—æ¯æ•°å­—è¾¹ç•Œ
        # ä½†ä¸­æ–‡å­—ç¬¦ä¸è¢« \w åŒ¹é…ï¼Œæ‰€ä»¥éœ€è¦ç‰¹æ®Šå¤„ç†
        if re.search(r'[\u4e00-\u9fa5]', tag):
            # åŒ…å«ä¸­æ–‡ï¼Œç›´æ¥ä½¿ç”¨å­—é¢åŒ¹é…
            pattern_to_remove = re.escape(tag)
            print(f"[è°ƒè¯•] æ¸…ç†ä¸­æ–‡æ ‡ç­¾: '{tag}' (ä½¿ç”¨å­—é¢åŒ¹é…)")
        else:
            # çº¯è‹±æ–‡/æ•°å­—ï¼Œä½¿ç”¨è¯è¾¹ç•Œ
            pattern_to_remove = r"\b" + re.escape(tag) + r"(?!\w)"
            print(f"[è°ƒè¯•] æ¸…ç†è‹±æ–‡æ ‡ç­¾: '{tag}' (ä½¿ç”¨è¯è¾¹ç•Œ)")

        before = cleaned_tech_zone
        cleaned_tech_zone = re.sub(pattern_to_remove,
                                   " ",
                                   cleaned_tech_zone,
                                   flags=re.IGNORECASE)
        if before != cleaned_tech_zone:
            print(f"[è°ƒè¯•]   å·²ä»æŠ€æœ¯åŒºç§»é™¤: '{tag}'")

    print(f"[è°ƒè¯•] æ¸…ç†åçš„æŠ€æœ¯åŒº: '{cleaned_tech_zone}'")
    remains = re.split(r"[\s\.]+", cleaned_tech_zone)
    unrecognized_parts.extend([part for part in remains if part])
    print(f"[è°ƒè¯•] æœ€ç»ˆæ— æ³•è¯†åˆ«éƒ¨åˆ†: {unrecognized_parts}")
    if unrecognized_parts:
        params["unrecognized"] = " ".join(sorted(list(
            set(unrecognized_parts))))

    english_params = {}
    key_order = [
        "title",
        "year",
        "season_episode",
        "completion_status",
        "release_version",
        "resolution",
        "medium",
        "source_platform",
        "video_codec",
        "video_format",
        "hdr_format",
        "bit_depth",
        "framerate",
        "audio",
        "release_info",
        "unrecognized",
    ]
    for key in key_order:
        if key in params and params[key]:
            if key == "audio" and isinstance(params[key], list):
                # å¤„ç†éŸ³é¢‘ç¼–ç åˆ—è¡¨ï¼Œå°† "æ•°å­—Audio" æ ¼å¼ç§»åˆ°ç¼–ç æ ¼å¼åé¢
                processed_audio = []
                for audio_item in params[key]:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å« "æ•°å­—Audio" æ¨¡å¼ï¼ˆå¦‚ "3Audio DTS" æˆ– "DTS 3Audio"ï¼‰
                    # åŒ¹é…æ¨¡å¼ï¼š(\d+)\s*(Audio[s]?)\s+(.+) æˆ– (.+)\s+(\d+)\s*(Audio[s]?)
                    match = re.match(r'^(\d+)\s*(Audio[s]?)\s+(.+)$', audio_item, re.IGNORECASE)
                    if match:
                        # å¦‚æœæ˜¯ "3Audio DTS" æ ¼å¼ï¼Œé‡æ’ä¸º "DTS 3Audio"
                        number = match.group(1)
                        audio_word = match.group(2)
                        codec = match.group(3)
                        processed_audio.append(f"{codec} {number}{audio_word}")
                    else:
                        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ­£ç¡®æ ¼å¼ "DTS 3Audio"
                        match_correct = re.match(r'^(.+?)\s+(\d+)\s*(Audio[s]?)$', audio_item, re.IGNORECASE)
                        if match_correct:
                            # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                            processed_audio.append(audio_item)
                        else:
                            # å…¶ä»–æ ¼å¼ä¸å˜
                            processed_audio.append(audio_item)
                
                # æ’åºï¼šå…ˆæŒ‰æ˜¯å¦ä»¥æ•°å­—Audioç»“å°¾ï¼Œå†æŒ‰é•¿åº¦
                sorted_audio = sorted(processed_audio,
                                      key=lambda s:
                                      (bool(re.search(r'\d+\s*Audio[s]?$', s, re.IGNORECASE)), -len(s)))
                english_params[key] = " ".join(sorted_audio)
            else:
                english_params[key] = params[key]

    if "source_platform" in english_params and "audio" in english_params:
        is_sp_list = isinstance(english_params["source_platform"], list)
        sp_values = (english_params["source_platform"]
                     if is_sp_list else [english_params["source_platform"]])
        if "MA" in sp_values and "MA" in str(english_params["audio"]):
            sp_values.remove("MA")
            if not sp_values:
                del english_params["source_platform"]
            elif len(sp_values) == 1 and not is_sp_list:
                english_params["source_platform"] = sp_values[0]
            elif is_sp_list:
                english_params["source_platform"] = sp_values

    # 6. æœ‰æ•ˆæ€§è´¨æ£€
    is_valid = bool(english_params.get("title"))
    if is_valid:
        if not any(
                key in english_params
                for key in ["resolution", "medium", "video_codec", "audio"]):
            is_valid = False
        release_info = english_params.get("release_info", "")
        if "N/A" in release_info and "NOGROUP" not in release_info:
            core_tech_keys = ["resolution", "medium", "video_codec"]
            if sum(1 for key in core_tech_keys if key in english_params) < 2:
                is_valid = False

    if not is_valid:
        print("ä¸»æ ‡é¢˜è§£æå¤±è´¥æˆ–æœªé€šè¿‡è´¨æ£€ã€‚")
        english_params = {"title": original_title_str, "unrecognized": "è§£æå¤±è´¥"}

    translation_map = {
        "title": "ä¸»æ ‡é¢˜",
        "year": "å¹´ä»½",
        "season_episode": "å­£é›†",
        "resolution": "åˆ†è¾¨ç‡",
        "medium": "åª’ä»‹",
        "source_platform": "ç‰‡æºå¹³å°",
        "video_codec": "è§†é¢‘ç¼–ç ",
        "hdr_format": "HDRæ ¼å¼",
        "bit_depth": "è‰²æ·±",
        "framerate": "å¸§ç‡",
        "audio": "éŸ³é¢‘ç¼–ç ",
        "release_info": "åˆ¶ä½œç»„",
        "completion_status": "å‰§é›†çŠ¶æ€",
        "unrecognized": "æ— æ³•è¯†åˆ«",
        "video_format": "è§†é¢‘æ ¼å¼",
        "release_version": "å‘å¸ƒç‰ˆæœ¬",
    }

    chinese_keyed_params = {}
    for key, value in english_params.items():
        chinese_key = translation_map.get(key)
        if chinese_key:
            chinese_keyed_params[chinese_key] = value

    # å®šä¹‰å‰ç«¯æ˜¾ç¤ºçš„å®Œæ•´å‚æ•°åˆ—è¡¨å’Œå›ºå®šé¡ºåº
    all_possible_keys_ordered = [
        "ä¸»æ ‡é¢˜",
        "å¹´ä»½",
        "å­£é›†",
        "å‰§é›†çŠ¶æ€",
        "å‘å¸ƒç‰ˆæœ¬",
        "åˆ†è¾¨ç‡",
        "åª’ä»‹",
        "ç‰‡æºå¹³å°",
        "è§†é¢‘ç¼–ç ",
        "è§†é¢‘æ ¼å¼",
        "HDRæ ¼å¼",
        "è‰²æ·±",
        "å¸§ç‡",
        "éŸ³é¢‘ç¼–ç ",
        "åˆ¶ä½œç»„",
        "æ— æ³•è¯†åˆ«",
    ]

    final_components_list = []
    for key in all_possible_keys_ordered:
        final_components_list.append({
            "key": key,
            "value": chinese_keyed_params.get(key, "")
        })

    print(f"ä¸»æ ‡é¢˜è§£ææˆåŠŸã€‚")
    return final_components_list


def upload_data_screenshot(source_info,
                           save_path,
                           torrent_name=None,
                           downloader_id=None):
    """
    [æœ€ç»ˆHDRä¼˜åŒ–ç‰ˆ] ä½¿ç”¨ mpv ä»è§†é¢‘æ–‡ä»¶ä¸­æˆªå–å¤šå¼ å›¾ç‰‡ï¼Œå¹¶ä¸Šä¼ åˆ°å›¾åºŠã€‚
    - æ–°å¢HDRè‰²è°ƒæ˜ å°„å‚æ•°ï¼Œç¡®ä¿HDRè§†é¢‘æˆªå›¾é¢œè‰²æ­£å¸¸ã€‚
    - æŒ‰é¡ºåºä¸€å¼ ä¸€å¼ å¤„ç†ï¼Œç®€åŒ–æµç¨‹ã€‚
    - é‡‡ç”¨æ™ºèƒ½æ—¶é—´ç‚¹åˆ†æã€‚
    """
    if Image is None:
        print("é”™è¯¯ï¼šPillow åº“æœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡Œæˆªå›¾ä»»åŠ¡ã€‚")
        return ""

    print("å¼€å§‹æ‰§è¡Œæˆªå›¾å’Œä¸Šä¼ ä»»åŠ¡ (å¼•æ“: mpv, è¾“å‡ºæ ¼å¼: JPEG, æ¨¡å¼: é¡ºåºæ‰§è¡Œ)...")
    config = config_manager.get()
    hoster = config.get("cross_seed", {}).get("image_hoster", "pixhost")
    num_screenshots = 5
    print(f"å·²é€‰æ‹©å›¾åºŠæœåŠ¡: {hoster}, æˆªå›¾æ•°é‡: {num_screenshots}")

    # é¦–å…ˆåº”ç”¨è·¯å¾„æ˜ å°„è½¬æ¢
    translated_save_path = translate_path(downloader_id, save_path)
    if translated_save_path != save_path:
        print(f"è·¯å¾„æ˜ å°„: {save_path} -> {translated_save_path}")

    if torrent_name:
        full_video_path = os.path.join(translated_save_path, torrent_name)
        print(f"ä½¿ç”¨å®Œæ•´è§†é¢‘è·¯å¾„: {full_video_path}")
    else:
        full_video_path = translated_save_path
        print(f"ä½¿ç”¨åŸå§‹è·¯å¾„: {full_video_path}")

    # --- ä»£ç†æ£€æŸ¥å’Œå¤„ç†é€»è¾‘ (æ­¤éƒ¨åˆ†ä¿æŒä¸å˜) ---
    use_proxy = False
    proxy_config = None
    if downloader_id:
        downloaders = config.get("downloaders", [])
        for downloader in downloaders:
            if downloader.get("id") == downloader_id:
                use_proxy = downloader.get("use_proxy", False)
                if use_proxy:
                    host_value = downloader.get('host', '')
                    proxy_port = downloader.get('proxy_port', 9090)
                    if host_value.startswith(('http://', 'https://')):
                        parsed_url = urlparse(host_value)
                    else:
                        parsed_url = urlparse(f"http://{host_value}")
                    proxy_ip = parsed_url.hostname
                    if not proxy_ip:
                        if '://' in host_value:
                            proxy_ip = host_value.split('://')[1].split(
                                ':')[0].split('/')[0]
                        else:
                            proxy_ip = host_value.split(':')[0]
                    proxy_config = {
                        "proxy_base_url": f"http://{proxy_ip}:{proxy_port}",
                    }
                break

    if use_proxy and proxy_config:
        print(f"ä½¿ç”¨ä»£ç†å¤„ç†æˆªå›¾: {proxy_config['proxy_base_url']}")
        try:
            response = requests.post(
                f"{proxy_config['proxy_base_url']}/api/media/screenshot",
                json={"remote_path": full_video_path},
                timeout=300)
            response.raise_for_status()
            result = response.json()
            if result.get("success"):
                print("ä»£ç†æˆªå›¾ä¸Šä¼ æˆåŠŸ")
                return result.get("bbcode", "")
            else:
                print(f"ä»£ç†æˆªå›¾ä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return ""
        except Exception as e:
            print(f"é€šè¿‡ä»£ç†è·å–æˆªå›¾å¤±è´¥: {e}")
            return ""

    # --- æœ¬åœ°æˆªå›¾é€»è¾‘ ---
    target_video_file, is_bluray_disc = _find_target_video_file(
        full_video_path)
    if not target_video_file:
        print("é”™è¯¯ï¼šåœ¨æŒ‡å®šè·¯å¾„ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ã€‚")
        return ""

    # å¯¹äºåŸç›˜æ–‡ä»¶ï¼Œä»ç„¶è¿›è¡Œæˆªå›¾å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    if is_bluray_disc:
        print("æ£€æµ‹åˆ°åŸç›˜æ–‡ä»¶ç»“æ„ï¼Œä½†ä»å°†è¿›è¡Œæˆªå›¾å¤„ç†")

    if not shutil.which("mpv"):
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ° mpvã€‚è¯·ç¡®ä¿å®ƒå·²å®‰è£…å¹¶å·²æ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ PATH ä¸­ã€‚")
        return ""

    screenshot_points = _get_smart_screenshot_points(target_video_file,
                                                     num_screenshots)
    if len(screenshot_points) < num_screenshots:
        print("è­¦å‘Š: æ™ºèƒ½åˆ†æå¤±è´¥æˆ–å­—å¹•ä¸è¶³ï¼Œå›é€€åˆ°æŒ‰ç™¾åˆ†æ¯”æˆªå›¾ã€‚")
        try:
            cmd_duration = [
                "ffprobe", "-v", "error", "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1", target_video_file
            ]
            result = subprocess.run(cmd_duration,
                                    capture_output=True,
                                    text=True,
                                    check=True,
                                    encoding='utf-8')
            duration = float(result.stdout.strip())
            screenshot_points = [
                duration * p for p in [0.15, 0.30, 0.50, 0.70, 0.85]
            ]
        except Exception as e:
            print(f"é”™è¯¯: è¿è·å–è§†é¢‘æ—¶é•¿éƒ½å¤±è´¥äº†ï¼Œæ— æ³•æˆªå›¾ã€‚{e}")
            return ""

    auth_token = _get_agsv_auth_token() if hoster == "agsv" else None
    if hoster == "agsv" and not auth_token:
        print("âŒ æ— æ³•è·å– æœ«æ—¥å›¾åºŠ Tokenï¼Œæˆªå›¾ä¸Šä¼ ä»»åŠ¡ç»ˆæ­¢ã€‚")
        return ""

    uploaded_urls = []
    temp_files_to_cleanup = []

    for i, screenshot_time in enumerate(screenshot_points):
        print(f"\n--- å¼€å§‹å¤„ç†ç¬¬ {i+1}/{len(screenshot_points)} å¼ æˆªå›¾ ---")

        safe_name = re.sub(r'[\\/*?:"<>|\'\s\.]+', '_',
                           source_info.get('main_title', f's_{i+1}'))  # æ›´çŸ­çš„æ–‡ä»¶å
        timestamp = f"{int(time.time()) % 1000000}"  # æ›´çŸ­çš„æ—¶é—´æˆ³
        intermediate_png_path = os.path.join(
            TEMP_DIR, f"s_{i+1}_{timestamp}.png")  # æ›´çŸ­çš„æ–‡ä»¶å
        final_jpeg_path = os.path.join(TEMP_DIR,
                                       f"s_{i+1}_{timestamp}.jpg")  # æ›´çŸ­çš„æ–‡ä»¶å
        temp_files_to_cleanup.extend([intermediate_png_path, final_jpeg_path])

        # --- [æ ¸å¿ƒä¿®æ”¹] ---
        # ä¸º mpv å‘½ä»¤æ·»åŠ  HDR è‰²è°ƒæ˜ å°„å‚æ•°
        cmd_screenshot = [
            "mpv",
            "--no-audio",
            f"--start={screenshot_time:.2f}",
            "--frames=1",

            # --- HDR è‰²è°ƒæ˜ å°„å‚æ•° ---
            # æŒ‡å®šè¾“å‡ºä¸ºæ ‡å‡†çš„sRGBè‰²å½©ç©ºé—´ï¼Œè¿™æ˜¯æ‰€æœ‰SDRå›¾ç‰‡çš„åŸºç¡€
            "--target-trc=srgb",
            # ä½¿ç”¨ 'hable' ç®—æ³•è¿›è¡Œè‰²è°ƒæ˜ å°„ï¼Œå®ƒèƒ½åœ¨ä¿ç•™é«˜å…‰å’Œé˜´å½±ç»†èŠ‚æ–¹é¢å–å¾—è‰¯å¥½å¹³è¡¡
            "--tone-mapping=hable",
            # å¦‚æœè‰²å½©ä¾ç„¶ä¸å‡†ï¼Œå¯ä»¥å°è¯•æ›´ç°ä»£çš„ 'bt.2390' ç®—æ³•
            # "--tone-mapping=bt.2390",
            f"--o={intermediate_png_path}",
            target_video_file
        ]
        # --- [æ ¸å¿ƒä¿®æ”¹ç»“æŸ] ---

        try:
            subprocess.run(cmd_screenshot,
                           check=True,
                           capture_output=True,
                           timeout=180)

            if not os.path.exists(intermediate_png_path):
                print(f"âŒ é”™è¯¯: mpv å‘½ä»¤æ‰§è¡ŒæˆåŠŸï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶ {intermediate_png_path}")
                continue

            print(
                f"   -> ä¸­é—´PNGå›¾ {os.path.basename(intermediate_png_path)} ç”ŸæˆæˆåŠŸã€‚"
            )

            try:
                with Image.open(intermediate_png_path) as img:
                    rgb_img = img.convert('RGB')
                    rgb_img.save(final_jpeg_path, 'jpeg', quality=85)
                print(
                    f"   -> JPEGå‹ç¼©æˆåŠŸ (è´¨é‡: 85) -> {os.path.basename(final_jpeg_path)}"
                )
            except Exception as e:
                print(f"   âŒ é”™è¯¯: å›¾ç‰‡ä»PNGè½¬æ¢ä¸ºJPEGå¤±è´¥: {e}")
                continue

            max_retries = 3
            image_url = None
            for attempt in range(max_retries):
                print(f"   -> æ­£åœ¨ä¸Šä¼  (ç¬¬ {attempt+1}/{max_retries} æ¬¡å°è¯•)...")
                try:
                    if hoster == "agsv":
                        image_url = _upload_to_agsv(final_jpeg_path,
                                                    auth_token)
                    else:
                        image_url = _upload_to_pixhost(final_jpeg_path)
                    if image_url:
                        uploaded_urls.append(image_url)
                        break
                    else:
                        time.sleep(2)
                except Exception as e:
                    print(f"   -> ä¸Šä¼ å°è¯• {attempt+1} å‡ºç°å¼‚å¸¸: {e}")
                    time.sleep(2)

            if not image_url:
                print(f"âš ï¸  ç¬¬ {i+1} å¼ å›¾ç‰‡ç»è¿‡ {max_retries} æ¬¡å°è¯•åä»ç„¶ä¸Šä¼ å¤±è´¥ã€‚")

        except subprocess.CalledProcessError as e:
            error_output = e.stderr.decode('utf-8', errors='ignore')
            print(f"âŒ é”™è¯¯: mpv æˆªå›¾å¤±è´¥ã€‚")
            print(f"   -> Stderr: {error_output}")
            continue
        except subprocess.TimeoutExpired:
            print(f"âŒ é”™è¯¯: mpv æˆªå›¾è¶…æ—¶ (è¶…è¿‡60ç§’)ã€‚")
            continue

    print("\n--- æ‰€æœ‰æˆªå›¾å¤„ç†å®Œæ¯• ---")
    print(f"æ­£åœ¨æ¸…ç†ä¸´æ—¶ç›®å½•ä¸­çš„ {len(temp_files_to_cleanup)} ä¸ªæˆªå›¾æ–‡ä»¶...")
    for item_path in temp_files_to_cleanup:
        try:
            if os.path.exists(item_path):
                os.remove(item_path)
        except OSError as e:
            print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {item_path} å¤±è´¥: {e}")

    if not uploaded_urls:
        print("ä»»åŠ¡å®Œæˆï¼Œä½†æ²¡æœ‰æˆåŠŸä¸Šä¼ ä»»ä½•å›¾ç‰‡ã€‚")
        return ""

    bbcode_links = []
    for url in sorted(uploaded_urls):
        if "pixhost.to/show/" in url:
            bbcode_links.append(
                f"[img]{url.replace('https://pixhost.to/show/', 'https://img1.pixhost.to/images/')}[/img]"
            )
        else:
            bbcode_links.append(f"[img]{url}[/img]")

    screenshots = "\n".join(bbcode_links)
    print("æ‰€æœ‰æˆªå›¾å·²æˆåŠŸä¸Šä¼ å¹¶å·²æ ¼å¼åŒ–ä¸ºBBCodeã€‚")
    return screenshots


def upload_data_poster(douban_link: str, imdb_link: str):
    """
    é€šè¿‡PT-Gen APIè·å–ç”µå½±ä¿¡æ¯çš„æµ·æŠ¥å’ŒIMDbé“¾æ¥ã€‚
    æ”¯æŒä»è±†ç“£é“¾æ¥æˆ–IMDbé“¾æ¥è·å–ä¿¡æ¯ã€‚
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨upload_data_movie_infoæ›¿ä»£ã€‚
    """
    # è°ƒç”¨æ–°çš„ç»Ÿä¸€å‡½æ•°è·å–æ‰€æœ‰ä¿¡æ¯
    status, poster, description, imdb_link_result = upload_data_movie_info(
        douban_link, imdb_link)

    if status:
        return True, poster, imdb_link_result
    else:
        return False, description, imdb_link_result


def upload_data_movie_info(douban_link: str, imdb_link: str):
    """
    é€šè¿‡å¤šä¸ªPT-Gen APIè·å–ç”µå½±ä¿¡æ¯çš„å®Œæ•´å†…å®¹ï¼ŒåŒ…æ‹¬æµ·æŠ¥ã€ç®€ä»‹å’ŒIMDbé“¾æ¥ã€‚
    æ”¯æŒä»è±†ç“£é“¾æ¥æˆ–IMDbé“¾æ¥è·å–ä¿¡æ¯ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢APIã€‚
    è¿”å›: (çŠ¶æ€, æµ·æŠ¥, ç®€ä»‹, IMDbé“¾æ¥)
    """
    # APIé…ç½®åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    api_configs = [
        {
            'name': 'ptn-ptgen.sqing33.dpdns.org',
            'base_url': 'https://ptn-ptgen.sqing33.dpdns.org',
            'type': 'url_format'
        },
        {
            'name': 'ptgen.tju.pt',
            'base_url': 'https://ptgen.tju.pt/infogen',
            'type': 'tju_format',
            'force_douban': True  # å¼ºåˆ¶ä½¿ç”¨site=doubanæ¨¡å¼
        },
        {
            'name': 'ptgen.homeqian.top',
            'base_url': 'https://ptgen.homeqian.top',
            'type': 'url_format'
        },
        {
            'name': 'api.iyuu.cn',
            'base_url': 'https://api.iyuu.cn/App.Movie.Ptgen',
            'type': 'iyuu_format'
        }
    ]

    # ç¡®å®šè¦ä½¿ç”¨çš„èµ„æºURLï¼ˆè±†ç“£ä¼˜å…ˆï¼‰
    if not douban_link and not imdb_link:
        return False, "", "", "æœªæä¾›è±†ç“£æˆ–IMDbé“¾æ¥ã€‚"

    # å°è¯•æ¯ä¸ªAPI
    last_error = ""
    for api_config in api_configs:
        try:
            print(f"å°è¯•ä½¿ç”¨API: {api_config['name']}")

            if api_config['type'] == 'tju_format':
                # TJUæ ¼å¼API (ptgen.tju.pt) - å¼ºåˆ¶ä½¿ç”¨è±†ç“£æ¨¡å¼
                success, poster, description, imdb_link_result = _call_tju_format_api(
                    api_config, douban_link, imdb_link)
            elif api_config['type'] == 'url_format':
                # URLæ ¼å¼API (workers.dev, homeqian.top)
                success, poster, description, imdb_link_result = _call_url_format_api(
                    api_config, douban_link, imdb_link)
            elif api_config['type'] == 'iyuu_format':
                # IYUUæ ¼å¼API (api.iyuu.cn)
                success, poster, description, imdb_link_result = _call_iyuu_format_api(
                    api_config, douban_link, imdb_link)
            else:
                continue

            if success:
                print(f"API {api_config['name']} è°ƒç”¨æˆåŠŸ")
                return True, poster, description, imdb_link_result
            else:
                last_error = description  # é”™è¯¯ä¿¡æ¯å­˜å‚¨åœ¨descriptionä¸­
                print(f"API {api_config['name']} è¿”å›å¤±è´¥: {last_error}")

        except Exception as e:
            last_error = f"API {api_config['name']} è¯·æ±‚å¼‚å¸¸: {e}"
            print(last_error)
            continue

    # æ‰€æœ‰APIéƒ½å¤±è´¥
    return False, "", "", f"æ‰€æœ‰PT-Gen APIéƒ½å¤±è´¥ã€‚æœ€åé”™è¯¯: {last_error}"


def _call_tju_format_api(api_config: dict, douban_link: str, imdb_link: str):
    """
    è°ƒç”¨TJUæ ¼å¼API (ptgen.tju.pt) - å¼ºåˆ¶ä½¿ç”¨site=doubanæ¨¡å¼
    """
    try:
        # å¼ºåˆ¶ä½¿ç”¨site=doubanï¼Œè¿™æ ·IMDbé“¾æ¥ä¹Ÿä¼šè¢«è½¬æ¢æŸ¥è¯¢è±†ç“£
        if douban_link:
            # ä»è±†ç“£é“¾æ¥æå–ID
            douban_id = _extract_douban_id(douban_link)
            if douban_id:
                url = f"{api_config['base_url']}?site=douban&sid={douban_id}"
            else:
                raise ValueError("æ— æ³•ä»è±†ç“£é“¾æ¥æå–ID")
        elif imdb_link:
            # ä»IMDbé“¾æ¥æå–IDï¼Œä½†å¼ºåˆ¶ä½¿ç”¨doubanæ¨¡å¼
            imdb_id = _extract_imdb_id(imdb_link)
            if imdb_id:
                url = f"{api_config['base_url']}?site=douban&sid={imdb_id}"
            else:
                raise ValueError("æ— æ³•ä»IMDbé“¾æ¥æå–ID")
        else:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„é“¾æ¥")

        response = requests.get(url, timeout=30)
        response.raise_for_status()

        data = response.json()

        if not data.get('success', False):
            error_msg = data.get('error', 'æœªçŸ¥é”™è¯¯')
            return False, "", f"APIè¿”å›å¤±è´¥: {error_msg}", ""

        format_data = data.get('format', '')
        if not format_data:
            return False, "", "APIæœªè¿”å›æœ‰æ•ˆçš„æ ¼å¼åŒ–å†…å®¹", ""

        # æå–ä¿¡æ¯
        extracted_imdb_link = data.get('imdb_link', '')
        poster = ""
        description = ""

        # æå–æµ·æŠ¥å›¾ç‰‡
        img_match = re.search(r'(\[img\].*?\[/img\])', format_data)
        if img_match:
            poster = re.sub(r'img1', 'img9', img_match.group(1))

        # æå–ç®€ä»‹å†…å®¹ï¼ˆå»é™¤æµ·æŠ¥éƒ¨åˆ†ï¼‰
        description = re.sub(r'\[img\].*?\[/img\]', '', format_data).strip()
        description = re.sub(r'\n{3,}', '\n\n', description)

        return True, poster, description, extracted_imdb_link

    except Exception as e:
        return False, "", f"TJUæ ¼å¼APIè°ƒç”¨å¤±è´¥: {e}", ""


def _call_url_format_api(api_config: dict, douban_link: str, imdb_link: str):
    """
    è°ƒç”¨URLæ ¼å¼API (workers.dev, homeqian.top)
    """
    try:
        resource_url = douban_link or imdb_link
        url = f"{api_config['base_url']}/?url={resource_url}"

        response = requests.get(url, timeout=30)
        response.raise_for_status()

        # å°è¯•è§£æä¸ºJSON
        try:
            data = response.json()
        except:
            # å¦‚æœä¸æ˜¯JSONï¼Œå¯èƒ½æ˜¯ç›´æ¥è¿”å›çš„æ–‡æœ¬æ ¼å¼
            text_content = response.text.strip()
            if text_content and ('[img]' in text_content
                                 or 'â—' in text_content):
                # ç›´æ¥è¿”å›æ–‡æœ¬å†…å®¹ä½œä¸ºformat
                return _parse_format_content(text_content)
            else:
                return False, "", "APIè¿”å›äº†æ— æ•ˆçš„å†…å®¹æ ¼å¼", ""

        # JSONæ ¼å¼å¤„ç†
        if isinstance(data, dict):
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if data.get('success') is False:
                error_msg = data.get('message', data.get('error', 'æœªçŸ¥é”™è¯¯'))
                return False, "", f"APIè¿”å›å¤±è´¥: {error_msg}", ""

            # è·å–æ ¼å¼åŒ–å†…å®¹
            format_data = data.get('format', data.get('content', ''))
            if format_data:
                return _parse_format_content(format_data,
                                             data.get('imdb_link', ''))
            else:
                return False, "", "APIæœªè¿”å›æœ‰æ•ˆçš„æ ¼å¼åŒ–å†…å®¹", ""
        else:
            return False, "", "APIè¿”å›äº†æ— æ•ˆçš„æ•°æ®æ ¼å¼", ""

    except Exception as e:
        return False, "", f"URLæ ¼å¼APIè°ƒç”¨å¤±è´¥: {e}", ""


def _call_iyuu_format_api(api_config: dict, douban_link: str, imdb_link: str):
    """
    è°ƒç”¨IYUUæ ¼å¼API (api.iyuu.cn)
    """
    try:
        resource_url = douban_link or imdb_link
        url = f"{api_config['base_url']}?url={resource_url}"

        response = requests.get(url, timeout=30)
        response.raise_for_status()

        data = response.json()

        # æ£€æŸ¥ä¸šåŠ¡çŠ¶æ€ç 
        if data.get('ret') != 200 and data.get('ret') != 0:
            error_msg = data.get('msg', 'æœªçŸ¥é”™è¯¯')
            return False, "", f"APIè¿”å›é”™è¯¯(çŠ¶æ€ç {data.get('ret')}): {error_msg}", ""

        format_data = data.get('format') or data.get('data', {}).get(
            'format', '')
        if not format_data:
            return False, "", "APIæœªè¿”å›æœ‰æ•ˆçš„ç®€ä»‹å†…å®¹", ""

        return _parse_format_content(format_data)

    except Exception as e:
        return False, "", f"IYUUæ ¼å¼APIè°ƒç”¨å¤±è´¥: {e}", ""


def _parse_format_content(format_data: str, provided_imdb_link: str = ""):
    """
    è§£ææ ¼å¼åŒ–å†…å®¹ï¼Œæå–æµ·æŠ¥ã€ç®€ä»‹å’ŒIMDbé“¾æ¥
    è‡ªåŠ¨å¯¹æµ·æŠ¥è¿›è¡Œæ™ºèƒ½éªŒè¯å’Œè½¬å­˜åˆ°pixhost
    """
    try:
        # æå–ä¿¡æ¯
        extracted_imdb_link = provided_imdb_link
        poster = ""
        description = ""

        # å¦‚æœæ²¡æœ‰æä¾›IMDbé“¾æ¥ï¼Œå°è¯•ä»æ ¼å¼åŒ–å†…å®¹ä¸­æå–
        if not extracted_imdb_link:
            imdb_match = re.search(
                r'â—IMDbé“¾æ¥\s*(https?://www\.imdb\.com/title/tt\d+/)',
                format_data)
            if imdb_match:
                extracted_imdb_link = imdb_match.group(1)

        # æå–æµ·æŠ¥å›¾ç‰‡å¹¶è¿›è¡Œæ™ºèƒ½éªŒè¯å’Œè½¬å­˜
        img_match = re.search(r'\[img\](.*?)\[/img\]', format_data)
        if img_match:
            original_poster_url = img_match.group(1)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯pixhostå›¾åºŠ
            if 'pixhost.to' in original_poster_url or 'img1.pixhost.to' in original_poster_url:
                # å·²ç»æ˜¯pixhostï¼Œç›´æ¥ä½¿ç”¨
                print(f"[*] æµ·æŠ¥å·²æ˜¯pixhostå›¾åºŠï¼Œç›´æ¥ä½¿ç”¨: {original_poster_url}")
                poster = f"[img]{original_poster_url}[/img]"
            else:
                # épixhostï¼Œè¿›è¡Œæ™ºèƒ½éªŒè¯å’Œè½¬å­˜
                print(f"[*] æµ·æŠ¥épixhostå›¾åºŠï¼Œæ‰§è¡Œæ™ºèƒ½éªŒè¯å’Œè½¬å­˜...")
                smart_poster_url = _get_smart_poster_url(original_poster_url)
                
                if smart_poster_url:
                    poster = f"[img]{smart_poster_url}[/img]"
                    print(f"[*] æ™ºèƒ½éªŒè¯å’Œè½¬å­˜æˆåŠŸ: {smart_poster_url}")
                else:
                    # æ™ºèƒ½è·å–å¤±è´¥ï¼Œä¿ç•™åŸURL
                    print(f"[*] æ™ºèƒ½éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹URL")
                    poster = f"[img]{original_poster_url}[/img]"

        # æå–ç®€ä»‹å†…å®¹ï¼ˆå»é™¤æµ·æŠ¥éƒ¨åˆ†ï¼‰
        description = re.sub(r'\[img\].*?\[/img\]', '', format_data).strip()
        description = re.sub(r'\n{3,}', '\n\n', description)

        return True, poster, description, extracted_imdb_link

    except Exception as e:
        return False, "", f"è§£ææ ¼å¼åŒ–å†…å®¹å¤±è´¥: {e}", ""


def _extract_douban_id(douban_link: str) -> str:
    """
    ä»è±†ç“£é“¾æ¥ä¸­æå–ID
    ä¾‹å¦‚: https://movie.douban.com/subject/34832354/ -> 34832354
    """
    match = re.search(r'/subject/(\d+)', douban_link)
    return match.group(1) if match else ""


def _extract_imdb_id(imdb_link: str) -> str:
    """
    ä»IMDbé“¾æ¥ä¸­æå–ID
    ä¾‹å¦‚: https://www.imdb.com/title/tt13721828/ -> tt13721828
    """
    match = re.search(r'/title/(tt\d+)', imdb_link)
    return match.group(1) if match else ""


# (ç¡®ä¿æ–‡ä»¶é¡¶éƒ¨æœ‰ import bencoder, import json)

# utils/media_helper.py


def add_torrent_to_downloader(detail_page_url: str, save_path: str,
                              downloader_id: str, db_manager, config_manager):
    """
    ä»ç§å­è¯¦æƒ…é¡µä¸‹è½½ .torrent æ–‡ä»¶å¹¶æ·»åŠ åˆ°æŒ‡å®šçš„ä¸‹è½½å™¨ã€‚
    [æœ€ç»ˆä¿®å¤ç‰ˆ] ä¿®æ­£äº†å‘ Transmission å‘é€æ•°æ®æ—¶çš„åŒé‡ç¼–ç é—®é¢˜ã€‚
    """
    logging.info(
        f"å¼€å§‹è‡ªåŠ¨æ·»åŠ ä»»åŠ¡: URL='{detail_page_url}', Path='{save_path}', DownloaderID='{downloader_id}'"
    )

    # 1. æŸ¥æ‰¾å¯¹åº”çš„ç«™ç‚¹é…ç½®
    conn = db_manager._get_connection()
    cursor = db_manager._get_cursor(conn)
    cursor.execute("SELECT nickname, base_url, cookie, speed_limit FROM sites")
    site_info = None
    for site in cursor.fetchall():
        # [ä¿®å¤] ç¡®ä¿ base_url å­˜åœ¨ä¸”ä¸ä¸ºç©º
        if site['base_url'] and site['base_url'] in detail_page_url:
            site_info = dict(site)  # [ä¿®å¤] å°† sqlite3.Row è½¬æ¢ä¸º dict
            break
    conn.close()

    if not site_info or not site_info.get("cookie"):
        msg = f"æœªèƒ½æ‰¾åˆ°ä¸URL '{detail_page_url}' åŒ¹é…çš„ç«™ç‚¹é…ç½®æˆ–è¯¥ç«™ç‚¹ç¼ºå°‘Cookieã€‚"
        logging.error(msg)
        return False, msg

    try:
        # 2. ä¸‹è½½ç§å­æ–‡ä»¶
        common_headers = {
            "Cookie":
            site_info["cookie"],
            "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        }
        scraper = cloudscraper.create_scraper()

        # ç«™ç‚¹çº§åˆ«çš„ä»£ç†å·²ä¸ä½¿ç”¨å…¨å±€ä»£ç†é…ç½®
        proxies = None

        # Add retry logic for network requests
        max_retries = 3
        for attempt in range(max_retries):
            try:
                details_response = scraper.get(detail_page_url,
                                               headers=common_headers,
                                               timeout=180,
                                               proxies=proxies)
                break  # Success, exit retry loop
            except Exception as e:
                if attempt < max_retries - 1:
                    logging.warning(
                        f"Attempt {attempt + 1} failed to fetch details page: {e}. Retrying..."
                    )
                    time.sleep(2**attempt)  # Exponential backoff
                else:
                    raise  # Re-raise the exception if all retries failed
        details_response.raise_for_status()

        soup = BeautifulSoup(details_response.text, "html.parser")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨ç‰¹æ®Šä¸‹è½½å™¨
        site_base_url = ensure_scheme(site_info['base_url'])
        full_download_url = None  # åˆå§‹åŒ–full_download_url

        print(f"ç«™ç‚¹åŸºç¡€URL: {site_base_url}")

        # æ£€æŸ¥æ˜¯å¦ä¸ºhaidanç«™ç‚¹
        if 'haidan' in site_base_url:
            # Haidanç«™ç‚¹éœ€è¦æå–torrent_idè€Œä¸æ˜¯id
            torrent_id_match = re.search(r"torrent_id=(\d+)", detail_page_url)
            if not torrent_id_match:
                raise ValueError("æ— æ³•ä»è¯¦æƒ…é¡µURLä¸­æå–ç§å­IDï¼ˆtorrent_idï¼‰ã€‚")
            torrent_id = torrent_id_match.group(1)
            # Haidanç«™ç‚¹çš„ç‰¹æ®Šé€»è¾‘
            download_link_tag = soup.find(
                'a', href=re.compile(r"download.php\?id="))

            if not download_link_tag: raise RuntimeError("åœ¨è¯¦æƒ…é¡µHTMLä¸­æœªèƒ½æ‰¾åˆ°ä¸‹è½½é“¾æ¥ï¼")

            download_url_part = str(download_link_tag['href'])  # æ˜¾å¼è½¬æ¢ä¸ºstr

            # æ›¿æ¢ä¸‹è½½é“¾æ¥ä¸­çš„idä¸ºä»detail_page_urlä¸­æå–çš„torrent_id
            download_url_part = re.sub(r"id=\d+", f"id={torrent_id}",
                                       download_url_part)

            full_download_url = f"{site_base_url}/{download_url_part}"
        else:
            # å…¶ä»–ç«™ç‚¹çš„é€šç”¨é€»è¾‘ - æå–idå‚æ•°
            torrent_id_match = re.search(r"id=(\d+)", detail_page_url)
            if not torrent_id_match: raise ValueError("æ— æ³•ä»è¯¦æƒ…é¡µURLä¸­æå–ç§å­IDã€‚")
            torrent_id = torrent_id_match.group(1)

            download_link_tag = soup.select_one(
                f'a.index[href^="download.php?id={torrent_id}"]')
            if not download_link_tag: raise RuntimeError("åœ¨è¯¦æƒ…é¡µHTMLä¸­æœªèƒ½æ‰¾åˆ°ä¸‹è½½é“¾æ¥ï¼")

            download_url_part = str(download_link_tag['href'])  # æ˜¾å¼è½¬æ¢ä¸ºstr
            full_download_url = f"{site_base_url}/{download_url_part}"

        # ç¡®ä¿full_download_urlå·²è¢«èµ‹å€¼
        if not full_download_url:
            raise RuntimeError("æœªèƒ½æˆåŠŸæ„å»ºç§å­ä¸‹è½½é“¾æ¥ï¼")

        print(f"ç§å­ä¸‹è½½é“¾æ¥: {full_download_url}")

        common_headers["Referer"] = detail_page_url
        # Add retry logic for torrent download
        for attempt in range(max_retries):
            try:
                torrent_response = scraper.get(full_download_url,
                                               headers=common_headers,
                                               timeout=180,
                                               proxies=proxies)
                torrent_response.raise_for_status()
                break  # Success, exit retry loop
            except Exception as e:
                if attempt < max_retries - 1:
                    logging.warning(
                        f"Attempt {attempt + 1} failed to download torrent: {e}. Retrying..."
                    )
                    time.sleep(2**attempt)  # Exponential backoff
                else:
                    raise  # Re-raise the exception if all retries failed

        torrent_content = torrent_response.content
        logging.info("å·²æˆåŠŸä¸‹è½½æœ‰æ•ˆçš„ç§å­æ–‡ä»¶å†…å®¹ã€‚")

    except Exception as e:
        msg = f"åœ¨ä¸‹è½½ç§å­æ–‡ä»¶æ­¥éª¤å‘ç”Ÿé”™è¯¯: {e}"
        logging.error(msg, exc_info=True)
        return False, msg

    # 3. æ‰¾åˆ°ä¸‹è½½å™¨é…ç½®
    config = config_manager.get()
    downloader_config = next(
        (d for d in config.get("downloaders", [])
         if d.get("id") == downloader_id and d.get("enabled")), None)

    if not downloader_config:
        msg = f"æœªæ‰¾åˆ°IDä¸º '{downloader_id}' çš„å·²å¯ç”¨ä¸‹è½½å™¨é…ç½®ã€‚"
        logging.error(msg)
        return False, msg

    # 4. æ·»åŠ åˆ°ä¸‹è½½å™¨ (æ ¸å¿ƒä¿®æ”¹åœ¨æ­¤ï¼) - æ·»åŠ é‡è¯•æœºåˆ¶
    max_retries = 3
    for attempt in range(max_retries):
        try:
            from core.services import _prepare_api_config

            api_config = _prepare_api_config(downloader_config)
            client_name = downloader_config['name']

            if downloader_config['type'] == 'qbittorrent':
                client = qbClient(**api_config)
                client.auth_log_in()

                # å‡†å¤‡ qBittorrent å‚æ•°
                qb_params = {
                    'torrent_files': torrent_content,
                    'save_path': save_path,
                    'is_paused': False,
                    'skip_checking': True
                }

                # å¦‚æœç«™ç‚¹è®¾ç½®äº†é€Ÿåº¦é™åˆ¶ï¼Œåˆ™æ·»åŠ é€Ÿåº¦é™åˆ¶å‚æ•°
                # æ•°æ®åº“ä¸­å­˜å‚¨çš„æ˜¯MB/sï¼Œéœ€è¦è½¬æ¢ä¸ºbytes/sä¼ é€’ç»™ä¸‹è½½å™¨API
                if site_info and site_info.get('speed_limit', 0) > 0:
                    speed_limit = int(
                        site_info['speed_limit']) * 1024 * 1024  # è½¬æ¢ä¸º bytes/s
                    qb_params['upload_limit'] = speed_limit
                    logging.info(
                        f"ä¸ºç«™ç‚¹ '{site_info['nickname']}' è®¾ç½®ä¸Šä¼ é€Ÿåº¦é™åˆ¶: {site_info['speed_limit']} MB/s"
                    )

                result = client.torrents_add(**qb_params)
                logging.info(f"å·²å°†ç§å­æ·»åŠ åˆ° qBittorrent '{client_name}': {result}")

            elif downloader_config['type'] == 'transmission':
                client = TrClient(**api_config)

                # å‡†å¤‡ Transmission å‚æ•°
                tr_params = {
                    'torrent': torrent_content,
                    'download_dir': save_path,
                    'paused': False
                }

                # å…ˆæ·»åŠ ç§å­
                result = client.add_torrent(**tr_params)
                logging.info(
                    f"å·²å°†ç§å­æ·»åŠ åˆ° Transmission '{client_name}': ID={result.id}")

                # å¦‚æœç«™ç‚¹è®¾ç½®äº†é€Ÿåº¦é™åˆ¶ï¼Œåˆ™åœ¨æ·»åŠ åè®¾ç½®é€Ÿåº¦é™åˆ¶
                # add_torrent æ–¹æ³•ä¸æ”¯æŒé€Ÿåº¦é™åˆ¶å‚æ•°ï¼Œéœ€è¦ä½¿ç”¨ change_torrent æ–¹æ³•
                if site_info and site_info.get('speed_limit', 0) > 0:
                    # è½¬æ¢ä¸º KBps: MB/s * 1024 = KBps
                    speed_limit_kbps = int(site_info['speed_limit']) * 1024
                    try:
                        client.change_torrent(result.id,
                                              upload_limit=speed_limit_kbps,
                                              upload_limited=True)
                        logging.info(
                            f"ä¸ºç«™ç‚¹ '{site_info['nickname']}' è®¾ç½®ä¸Šä¼ é€Ÿåº¦é™åˆ¶: {site_info['speed_limit']} MB/s ({speed_limit_kbps} KBps)"
                        )
                    except Exception as e:
                        logging.warning(f"è®¾ç½®é€Ÿåº¦é™åˆ¶å¤±è´¥ï¼Œä½†ç§å­å·²æ·»åŠ æˆåŠŸ: {e}")

            return True, f"æˆåŠŸæ·»åŠ åˆ° '{client_name}'"

        except Exception as e:
            logging.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æ·»åŠ ç§å­åˆ°ä¸‹è½½å™¨å¤±è´¥: {e}")

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            if attempt < max_retries - 1:
                wait_time = 2**attempt  # æŒ‡æ•°é€€é¿
                logging.info(f"ç­‰å¾… {wait_time} ç§’åè¿›è¡Œç¬¬ {attempt + 2} æ¬¡å°è¯•...")
                time.sleep(wait_time)
            else:
                msg = f"æ·»åŠ åˆ°ä¸‹è½½å™¨ '{downloader_config['name']}' æ—¶å¤±è´¥: {e}"
                logging.error(msg, exc_info=True)
                return False, msg


def extract_tags_from_title(title_components: list) -> list:
    """
    ä»æ ‡é¢˜å‚æ•°ä¸­æå–æ ‡ç­¾ï¼Œä¸»è¦ä»åª’ä»‹å’Œåˆ¶ä½œç»„å­—æ®µæå– DIY å’Œ VCB-Studio æ ‡ç­¾ã€‚
    
    è¿”å›åŸå§‹æ ‡ç­¾åç§°ï¼ˆå¦‚ "DIY", "VCB-Studio"ï¼‰ï¼Œè€Œä¸æ˜¯æ ‡å‡†åŒ–é”®ã€‚
    è¿™æ ·å¯ä»¥è¢« ParameterMapper æ­£ç¡®æ˜ å°„åˆ° global_mappings.yaml ä¸­å®šä¹‰çš„æ ‡å‡†åŒ–é”®ã€‚

    :param title_components: æ ‡é¢˜ç»„ä»¶åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"key": "ä¸»æ ‡é¢˜", "value": "..."}, ...]
    :return: ä¸€ä¸ªåŒ…å«åŸå§‹æ ‡ç­¾åç§°çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ ['DIY', 'VCB-Studio']
    """
    if not title_components:
        return []

    found_tags = set()

    # å°† title_components è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿æŸ¥æ‰¾
    title_dict = {
        item.get('key'): item.get('value', '')
        for item in title_components
    }

    # å®šä¹‰éœ€è¦æ£€æŸ¥çš„å­—æ®µå’Œå¯¹åº”çš„æ ‡ç­¾æ˜ å°„
    # æ ¼å¼ï¼šå­—æ®µå -> [(æ­£åˆ™æ¨¡å¼, åŸå§‹æ ‡ç­¾å), ...]
    # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯åŸå§‹æ ‡ç­¾åï¼ˆå¦‚ "DIY"ï¼‰ï¼Œè€Œä¸æ˜¯æ ‡å‡†åŒ–é”®ï¼ˆå¦‚ "tag.diy"ï¼‰
    tag_extraction_rules = {
        'åª’ä»‹': [
            (r'\bDIY\b', 'DIY'),
            (r'\bBlu-?ray\s+DIY\b', 'DIY'),
            (r'\bBluRay\s+DIY\b', 'DIY'),
        ],
        'åˆ¶ä½œç»„': [
            (r'\bDIY\b', 'DIY'),
            (r'\bVCB-Studio\b', 'VCB-Studio'),
            (r'\bVCB\b', 'VCB-Studio'),
        ]
    }

    # éå†éœ€è¦æ£€æŸ¥çš„å­—æ®µ
    for field_name, patterns in tag_extraction_rules.items():
        field_value = title_dict.get(field_name, '')

        if not field_value:
            continue

        # å¦‚æœå­—æ®µå€¼æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(field_value, list):
            field_value = ' '.join(str(v) for v in field_value)
        else:
            field_value = str(field_value)

        # æ£€æŸ¥æ¯ä¸ªæ­£åˆ™æ¨¡å¼
        for pattern, tag_name in patterns:
            if re.search(pattern, field_value, re.IGNORECASE):
                found_tags.add(tag_name)
                print(
                    f"ä»æ ‡é¢˜å‚æ•° '{field_name}' ä¸­æå–åˆ°æ ‡ç­¾: {tag_name} (åŒ¹é…: {pattern})")

    result_tags = list(found_tags)
    if result_tags:
        print(f"ä»æ ‡é¢˜å‚æ•°ä¸­æå–åˆ°çš„æ ‡ç­¾: {result_tags}")
    else:
        print("ä»æ ‡é¢˜å‚æ•°ä¸­æœªæå–åˆ°ä»»ä½•æ ‡ç­¾")

    return result_tags


def extract_tags_from_subtitle(subtitle: str) -> list:
    """
    ä»å‰¯æ ‡é¢˜ä¸­æå–æ ‡ç­¾ï¼Œç›®å‰ä¸»è¦æ£€æµ‹"ç‰¹æ•ˆ"å…³é”®è¯ã€‚
    
    :param subtitle: å‰¯æ ‡é¢˜æ–‡æœ¬
    :return: æ ‡ç­¾åˆ—è¡¨ï¼Œä¾‹å¦‚ ['ç‰¹æ•ˆ']
    """
    if not subtitle:
        return []

    found_tags = []

    # æ£€æŸ¥å‰¯æ ‡é¢˜ä¸­æ˜¯å¦åŒ…å«"ç‰¹æ•ˆ"å…³é”®è¯
    if "ç‰¹æ•ˆ" in subtitle:
        found_tags.append("tag.ç‰¹æ•ˆ")
        print(f"ä»å‰¯æ ‡é¢˜ä¸­æå–åˆ°æ ‡ç­¾: ç‰¹æ•ˆ")

    if found_tags:
        print(f"ä»å‰¯æ ‡é¢˜ä¸­æå–åˆ°çš„æ ‡ç­¾: {found_tags}")
    else:
        print("ä»å‰¯æ ‡é¢˜ä¸­æœªæå–åˆ°ä»»ä½•æ ‡ç­¾")

    return found_tags


def extract_tags_from_description(description_text: str) -> list:
    """
    ä»ç®€ä»‹æ–‡æœ¬çš„"ç±»åˆ«"å­—æ®µä¸­æå–æ ‡ç­¾ã€‚
    
    :param description_text: ç®€ä»‹æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬statementå’Œbodyï¼‰
    :return: æ ‡ç­¾åˆ—è¡¨ï¼Œä¾‹å¦‚ ['tag.å–œå‰§', 'tag.åŠ¨ç”»']
    """
    if not description_text:
        return []

    found_tags = []

    # ä»ç®€ä»‹ä¸­æå–ç±»åˆ«å­—æ®µ
    category_match = re.search(r"â—\s*ç±»\s*åˆ«\s*(.+?)(?:\n|$)", description_text)
    if category_match:
        category_text = category_match.group(1).strip()
        print(f"ä»ç®€ä»‹ä¸­æå–åˆ°ç±»åˆ«: {category_text}")

        # å®šä¹‰ç±»åˆ«å…³é”®è¯åˆ°æ ‡ç­¾çš„æ˜ å°„
        category_tag_map = {
            'å–œå‰§': 'tag.å–œå‰§',
            'Comedy': 'tag.å–œå‰§',
            'åŠ¨ç”»': 'tag.åŠ¨ç”»',
            'Animation': 'tag.åŠ¨ç”»',
            'åŠ¨ä½œ': 'tag.åŠ¨ä½œ',
            'Action': 'tag.åŠ¨ä½œ',
            'çˆ±æƒ…': 'tag.çˆ±æƒ…',
            'Romance': 'tag.çˆ±æƒ…',
            'ç§‘å¹»': 'tag.ç§‘å¹»',
            'Sci-Fi': 'tag.ç§‘å¹»',
            'ææ€–': 'tag.ææ€–',
            'Horror': 'tag.ææ€–',
            'æƒŠæ‚š': 'tag.æƒŠæ‚š',
            'Thriller': 'tag.æƒŠæ‚š',
            'æ‚¬ç–‘': 'tag.æ‚¬ç–‘',
            'Mystery': 'tag.æ‚¬ç–‘',
            'çŠ¯ç½ª': 'tag.çŠ¯ç½ª',
            'Crime': 'tag.çŠ¯ç½ª',
            'æˆ˜äº‰': 'tag.æˆ˜äº‰',
            'War': 'tag.æˆ˜äº‰',
            'å†’é™©': 'tag.å†’é™©',
            'Adventure': 'tag.å†’é™©',
            'å¥‡å¹»': 'tag.å¥‡å¹»',
            'Fantasy': 'tag.å¥‡å¹»',
            'å®¶åº­': 'tag.å®¶åº­',
            'Family': 'tag.å®¶åº­',
            'å‰§æƒ…': 'tag.å‰§æƒ…',
            'Drama': 'tag.å‰§æƒ…',
        }

        # æ£€æŸ¥ç±»åˆ«æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        for keyword, tag in category_tag_map.items():
            if keyword in category_text:
                found_tags.append(tag)
                print(f"   ä»ç±»åˆ«ä¸­æå–åˆ°æ ‡ç­¾: {tag} (åŒ¹é…å…³é”®è¯: {keyword})")

    if found_tags:
        print(f"ä»ç®€ä»‹ç±»åˆ«ä¸­æå–åˆ°çš„æ ‡ç­¾: {found_tags}")
    else:
        print("ä»ç®€ä»‹ç±»åˆ«ä¸­æœªæå–åˆ°ä»»ä½•æ ‡ç­¾")

    return found_tags


def check_animation_type_from_description(description_text: str) -> bool:
    """
    æ£€æŸ¥ç®€ä»‹çš„ç±»åˆ«å­—æ®µä¸­æ˜¯å¦åŒ…å«"åŠ¨ç”»"ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®æ­£ç±»å‹ä¸ºåŠ¨æ¼«ã€‚
    
    :param description_text: ç®€ä»‹æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬statementå’Œbodyï¼‰
    :return: å¦‚æœåŒ…å«"åŠ¨ç”»"è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not description_text:
        return False

    # ä»ç®€ä»‹ä¸­æå–ç±»åˆ«å­—æ®µ
    category_match = re.search(r"â—\s*ç±»\s*åˆ«\s*(.+?)(?:\n|$)", description_text)
    if category_match:
        category_text = category_match.group(1).strip()

        # æ£€æŸ¥ç±»åˆ«ä¸­æ˜¯å¦åŒ…å«"åŠ¨ç”»"å…³é”®è¯
        if "åŠ¨ç”»" in category_text or "Animation" in category_text:
            print(f"æ£€æµ‹åˆ°ç±»åˆ«ä¸­åŒ…å«'åŠ¨ç”»': {category_text}")
            return True

    return False


def extract_tags_from_mediainfo(mediainfo_text: str) -> list:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ ‡ç­¾åˆ—è¡¨ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: ä¸€ä¸ªåŒ…å«è¯†åˆ«å‡ºçš„æ ‡ç­¾å­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ ['tag.å›½è¯­', 'tag.ä¸­å­—', 'tag.HDR10']ã€‚
    """
    if not mediainfo_text:
        return []

    found_tags = set()
    lines = mediainfo_text.split('\n')  # ä¸è½¬å°å†™ï¼Œä¿æŒåŸå§‹å¤§å°å†™

    # å®šä¹‰å…³é”®è¯åˆ°æ ‡å‡†åŒ–æ ‡ç­¾çš„æ˜ å°„
    tag_keywords_map = {
        # è¯­è¨€æ ‡ç­¾
        'å›½è¯­': ['å›½è¯­', 'mandarin'],
        'ç²¤è¯­': ['ç²¤è¯­', 'cantonese'],
        'è‹±è¯­': ['è‹±è¯­', 'english'],
        'æ—¥è¯­': ['æ—¥è¯­', 'Japanese', 'japanese'],
        'éŸ©è¯­': ['éŸ©è¯­', 'korean'],
        'æ³•è¯­': ['æ³•è¯­', 'french'],
        'å¾·è¯­': ['å¾·è¯­', 'german'],
        'ä¿„è¯­': ['ä¿„è¯­', 'russian'],
        'å°åœ°è¯­': ['å°åœ°è¯­', 'hindi'],
        'è¥¿ç­ç‰™è¯­': ['è¥¿ç­ç‰™è¯­', 'spanish'],
        'è‘¡è„ç‰™è¯­': ['è‘¡è„ç‰™è¯­', 'portuguese'],
        'æ„å¤§åˆ©è¯­': ['æ„å¤§åˆ©è¯­', 'italian'],
        'æ³°è¯­': ['æ³°è¯­', 'thai'],
        'é˜¿æ‹‰ä¼¯è¯­': ['é˜¿æ‹‰ä¼¯è¯­', 'arabic'],
        'å¤–è¯­': ['å¤–è¯­', 'foreign'],
        # å­—å¹•æ ‡ç­¾
        'ä¸­å­—': ['ä¸­å­—', 'chinese', 'ç®€', 'ç¹'],
        'è‹±å­—': ['è‹±å­—', 'english'],
        # HDR æ ¼å¼æ ‡ç­¾
        'Dolby Vision': ['dolby vision', 'æœæ¯”è§†ç•Œ'],
        'HDR10+': ['hdr10+'],
        'HDR10': ['hdr10'],
        'HDR': ['hdr'],  # ä½œä¸ºé€šç”¨ HDR çš„å¤‡ç”¨é€‰é¡¹
        'HDRVivid': ['hdr vivid'],
    }

    # å®šä¹‰æ£€æŸ¥èŒƒå›´
    # current_section ç”¨äºè®°å½•å½“å‰ MediaInfo æ­£åœ¨å¤„ç†çš„ Section ç±»å‹ (General, Video, Audio, Text)
    current_section = None
    # ç”¨äºæ”¶é›†å½“å‰ Audio Section çš„æ‰€æœ‰è¡Œï¼Œä»¥ä¾¿åç»­è¯­è¨€æ£€æµ‹
    current_audio_section_lines = []
    # ç”¨äºæ”¶é›†å½“å‰ Video Section çš„æ‰€æœ‰è¡Œï¼Œä»¥ä¾¿åç»­è¯­è¨€æ£€æµ‹
    current_video_section_lines = []

    for line in lines:
        line_stripped = line.strip()
        line_lower = line_stripped.lower()

        # åˆ¤å®šå½“å‰å¤„äºå“ªä¸ªä¿¡æ¯å—
        if line_lower.startswith('general'):
            current_section = 'general'
            # åœ¨ General Section ç»“æŸæ—¶å¤„ç†ä¹‹å‰çš„ Audio/Video Section
            if current_audio_section_lines:
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
                current_audio_section_lines = []
            if current_video_section_lines:
                _process_video_section_languages(current_video_section_lines,
                                                 found_tags)
                current_video_section_lines = []
            continue
        elif line_lower.startswith('video'):
            current_section = 'video'
            if current_audio_section_lines:
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
                current_audio_section_lines = []
            current_video_section_lines = [line_stripped]  # å¼€å§‹æ–°çš„ Video å—
            continue
        elif line_lower.startswith('audio'):
            current_section = 'audio'
            if current_video_section_lines:
                _process_video_section_languages(current_video_section_lines,
                                                 found_tags)
                current_video_section_lines = []
            current_audio_section_lines = [line_stripped]  # å¼€å§‹æ–°çš„ Audio å—
            continue
        elif line_lower.startswith('text'):
            current_section = 'text'
            if current_audio_section_lines:
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
                current_audio_section_lines = []
            if current_video_section_lines:
                _process_video_section_languages(current_video_section_lines,
                                                 found_tags)
                current_video_section_lines = []
            continue
        # å…¶ä»– Section æš‚ä¸å¤„ç†ï¼Œç›´æ¥è·³è¿‡æˆ–è€…å¯ä»¥å®šä¹‰ä¸º 'other'
        elif not line_stripped:  # ç©ºè¡Œè¡¨ç¤ºä¸€ä¸ªSectionçš„ç»“æŸï¼Œå¯ä»¥è§¦å‘å¤„ç†
            if current_audio_section_lines and current_section != 'audio':  # å¦‚æœæ˜¯ç©ºè¡Œä¸”ä¹‹å‰æ˜¯éŸ³é¢‘å—ï¼Œåˆ™å¤„ç†
                _process_audio_section_languages(current_audio_section_lines,
                                                 found_tags)
                current_audio_section_lines = []
            if current_video_section_lines and current_section != 'video':  # å¦‚æœæ˜¯ç©ºè¡Œä¸”ä¹‹å‰æ˜¯è§†é¢‘å—ï¼Œåˆ™å¤„ç†
                _process_video_section_languages(current_video_section_lines,
                                                 found_tags)
                current_video_section_lines = []
            current_section = None  # é‡ç½®å½“å‰section
            continue

        # æ”¶é›†å½“å‰ Section çš„è¡Œ
        if current_section == 'audio':
            current_audio_section_lines.append(line_stripped)
        elif current_section == 'video':
            current_video_section_lines.append(line_stripped)
        elif current_section == 'text':
            # ä»…åœ¨ Text å—ä¸­æ£€æŸ¥å­—å¹•æ ‡ç­¾
            if 'ä¸­å­—' in tag_keywords_map and any(
                    kw in line_lower for kw in tag_keywords_map['ä¸­å­—']):
                found_tags.add('ä¸­å­—')
            if 'è‹±å­—' in tag_keywords_map and any(
                    kw in line_lower for kw in tag_keywords_map['è‹±å­—']):
                found_tags.add('è‹±å­—')

        # æ£€æŸ¥ HDR æ ¼å¼æ ‡ç­¾ (å…¨å±€æ£€æŸ¥)
        # æ³¨æ„ï¼šè¿™é‡Œä¿æŒå…¨å±€æ£€æŸ¥æ˜¯å› ä¸º HDR æ ¼å¼å¯èƒ½å‡ºç°åœ¨ General/Video ç­‰å¤šä¸ªåœ°æ–¹
        if 'dolby vision' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['Dolby Vision']):
            found_tags.add('Dolby Vision')
        if 'hdr10+' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR10+']):
            found_tags.add('HDR10+')
        if 'hdr10' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR10']):
            found_tags.add('HDR10')
        elif 'hdr' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR']):
            if not any(hdr_tag in found_tags
                       for hdr_tag in ['Dolby Vision', 'HDR10+', 'HDR10']):
                found_tags.add('HDR')
        if 'hdrvivid' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDRVivid']):
            found_tags.add('HDRVivid')

    # å¤„ç†æ–‡ä»¶æœ«å°¾å¯èƒ½å­˜åœ¨çš„ Audio/Video Section
    if current_audio_section_lines:
        _process_audio_section_languages(current_audio_section_lines,
                                         found_tags)
    if current_video_section_lines:
        _process_video_section_languages(current_video_section_lines,
                                         found_tags)

    # ä¸ºæ‰€æœ‰æ ‡ç­¾æ·»åŠ  tag. å‰ç¼€
    prefixed_tags = set()
    for tag in found_tags:
        if not tag.startswith('tag.'):  # é¿å…é‡å¤æ·»åŠ  tag.
            prefixed_tags.add(f'tag.{tag}')
        else:
            prefixed_tags.add(tag)

    print(f"ä» MediaInfo ä¸­æå–åˆ°çš„æ ‡ç­¾: {list(prefixed_tags)}")
    return list(prefixed_tags)


def _process_audio_section_languages(audio_lines, found_tags):
    """è¾…åŠ©å‡½æ•°ï¼šå¤„ç†éŸ³é¢‘å—ä¸­çš„è¯­è¨€æ£€æµ‹"""
    language = _check_language_in_section(audio_lines)
    if language:
        if language == 'å›½è¯­':
            found_tags.add('å›½è¯­')
        elif language == 'ç²¤è¯­':
            found_tags.add('ç²¤è¯­')
        else:  # å…¶ä»–è¯­è¨€
            found_tags.add(language)
            found_tags.add('å¤–è¯­')
        print(f"   -> ä»éŸ³é¢‘å—ä¸­æå–åˆ°è¯­è¨€: {language}")


def _process_video_section_languages(video_lines, found_tags):
    """è¾…åŠ©å‡½æ•°ï¼šå¤„ç†è§†é¢‘å—ä¸­çš„è¯­è¨€æ£€æµ‹"""
    language = _check_language_in_section(video_lines)
    if language:
        if language == 'å›½è¯­':
            found_tags.add('å›½è¯­')
        elif language == 'ç²¤è¯­':
            found_tags.add('ç²¤è¯­')
        else:  # å…¶ä»–è¯­è¨€
            found_tags.add(language)
            found_tags.add('å¤–è¯­')
        print(f"   -> ä»è§†é¢‘å—ä¸­æå–åˆ°è¯­è¨€: {language}")


def _check_language_in_section(section_lines) -> str | None:
    """
    é€šç”¨å‡½æ•°ï¼šæ£€æŸ¥æŒ‡å®š Section å—ä¸­æ˜¯å¦åŒ…å«è¯­è¨€ç›¸å…³æ ‡è¯†ã€‚

    :param section_lines: Section å—çš„æ‰€æœ‰è¡Œ
    :return: å¦‚æœæ£€æµ‹åˆ°è¯­è¨€è¿”å›å…·ä½“è¯­è¨€åç§°ï¼Œå¦åˆ™è¿”å›None
    """
    language_keywords_map = {
        'å›½è¯­': ['ä¸­æ–‡', 'chinese', 'mandarin', 'å›½è¯­'],
        'ç²¤è¯­': ['cantonese', 'ç²¤è¯­'],
        'è‹±è¯­': ['english', 'è‹±è¯­'],
        'æ—¥è¯­': ['japanese', 'æ—¥è¯­'],
        'éŸ©è¯­': ['korean', 'éŸ©è¯­'],
        'æ³•è¯­': ['french', 'æ³•è¯­'],
        'å¾·è¯­': ['german', 'å¾·è¯­'],
        'ä¿„è¯­': ['russian', 'ä¿„è¯­'],
        'å°åœ°è¯­': ['hindi', 'å°åœ°è¯­'],
        'è¥¿ç­ç‰™è¯­': ['spanish', 'è¥¿ç­ç‰™è¯­', 'latin america'],  # æ·»åŠ  Latin America
        'è‘¡è„ç‰™è¯­': ['portuguese', 'è‘¡è„ç‰™è¯­', 'br'],  # æ·»åŠ  BR
        'æ„å¤§åˆ©è¯­': ['italian', 'æ„å¤§åˆ©è¯­'],
        'æ³°è¯­': ['thai', 'æ³°è¯­'],
        'é˜¿æ‹‰ä¼¯è¯­': ['arabic', 'é˜¿æ‹‰ä¼¯è¯­', 'sa'],  # æ·»åŠ  SA
    }

    for line in section_lines:
        if not line:
            continue
        line_lower = line.lower()
        if 'language:' in line_lower:
            for lang, keywords in language_keywords_map.items():
                for keyword in keywords:
                    if keyword.lower() in line_lower:
                        return lang
        # å°è¯•ä» Title: ä¸­æå–
        if 'title:' in line_lower:
            for lang, keywords in language_keywords_map.items():
                for keyword in keywords:
                    if keyword.lower() in line_lower:
                        return lang
    return None


# åˆ é™¤è¿™ä¸¤ä¸ªä¸å†ä½¿ç”¨çš„è¾…åŠ©å‡½æ•°
def _check_mandarin_in_audio_section(audio_lines):
    return False  # Placeholder to avoid errors during diff application


def _check_other_language_in_audio_section(audio_lines) -> str | None:
    return None  # Placeholder to avoid errors during diff application


def extract_origin_from_description(description_text: str) -> str:
    """
    ä»ç®€ä»‹è¯¦æƒ…ä¸­æå–äº§åœ°ä¿¡æ¯ã€‚

    :param description_text: ç®€ä»‹è¯¦æƒ…æ–‡æœ¬
    :return: äº§åœ°ä¿¡æ¯ï¼Œä¾‹å¦‚ "æ—¥æœ¬"ã€"ä¸­å›½" ç­‰
    """
    if not description_text:
        return ""

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… "â—äº§ã€€ã€€åœ°ã€€æ—¥æœ¬" è¿™ç§æ ¼å¼
    # æ”¯æŒå¤šç§å˜ä½“ï¼šâ—äº§åœ°ã€â—äº§ã€€ã€€åœ°ã€â—å›½ã€€ã€€å®¶ã€â—å›½å®¶åœ°åŒºç­‰
    # ä¿®å¤ï¼šä½¿ç”¨ [^\n\r]+ è€Œä¸æ˜¯ .+? æ¥æ­£ç¡®åŒ¹é…åŒ…å«ç©ºæ ¼çš„äº§åœ°åç§°ï¼ˆå¦‚"ä¸­å›½å¤§é™†"ï¼‰
    patterns = [
        r"â—\s*äº§\s*åœ°\s*([^\n\r]+?)(?:\n|$)",  # åŒ¹é… â—äº§åœ° ä¸­å›½å¤§é™†
        r"â—\s*å›½\s*å®¶\s*([^\n\r]+?)(?:\n|$)",  # åŒ¹é… â—å›½å®¶ ä¸­å›½å¤§é™†
        r"â—\s*åœ°\s*åŒº\s*([^\n\r]+?)(?:\n|$)",  # åŒ¹é… â—åœ°åŒº ä¸­å›½å¤§é™†
        r"â—\s*å›½å®¶åœ°åŒº\s*([^\n\r]+?)(?:\n|$)",  # åŒ¹é… â—å›½å®¶åœ°åŒº ä¸­å›½å¤§é™†
        r"åˆ¶ç‰‡å›½å®¶/åœ°åŒº[:\s]+([^\n\r]+?)(?:\n|$)",  # åŒ¹é… åˆ¶ç‰‡å›½å®¶/åœ°åŒº: ä¸­å›½å¤§é™†
        r"åˆ¶ç‰‡å›½å®¶[:\s]+([^\n\r]+?)(?:\n|$)",  # åŒ¹é… åˆ¶ç‰‡å›½å®¶: ä¸­å›½å¤§é™†
        r"å›½å®¶[:\s]+([^\n\r]+?)(?:\n|$)",  # åŒ¹é… å›½å®¶: ä¸­å›½å¤§é™†
        r"äº§åœ°[:\s]+([^\n\r]+?)(?:\n|$)",  # åŒ¹é… äº§åœ°: ä¸­å›½å¤§é™†
        r"[äº§]\s*åœ°[:\s]+([^ï¼Œ,\n\r]+)",
        r"[å›½]\s*å®¶[:\s]+([^ï¼Œ,\n\r]+)",
        r"[åœ°]\s*åŒº[:\s]+([^ï¼Œ,\n\r]+)"
    ]

    for pattern in patterns:
        match = re.search(pattern, description_text)
        if match:
            origin = match.group(1).strip()
            # æ¸…ç†å¯èƒ½çš„å¤šä½™å­—ç¬¦
            origin = re.sub(r'[\[\]ã€ã€‘\(\)]', '', origin).strip()
            # ç§»é™¤å¸¸è§çš„åˆ†éš”ç¬¦ï¼Œå¦‚" / "ã€","ç­‰
            origin = re.split(r'\s*/\s*|\s*,\s*|\s*;\s*|\s*&\s*',
                              origin)[0].strip()
            print("æå–åˆ°äº§åœ°ä¿¡æ¯:", origin)

            return origin

    return ""


def extract_resolution_from_mediainfo(mediainfo_text: str) -> str:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–åˆ†è¾¨ç‡ä¿¡æ¯ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œä¾‹å¦‚ "720p"ã€"1080p"ã€"2160p" ç­‰
    """
    if not mediainfo_text:
        return ""

    # æŸ¥æ‰¾ Video éƒ¨åˆ†
    video_section_match = re.search(r"Video[\s\S]*?(?=\n\n|\Z)",
                                    mediainfo_text)
    if not video_section_match:
        return ""

    video_section = video_section_match.group(0)

    # æŸ¥æ‰¾åˆ†è¾¨ç‡ä¿¡æ¯
    # åŒ¹é…æ ¼å¼å¦‚ï¼šWidth                                 : 1 920 pixels
    #            Height                                : 1 080 pixels
    # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 920" -> "1920"
    width_match = re.search(r"[Ww]idth\s*:\s*(\d+)\s*(\d*)\s*pixels?",
                            video_section)
    height_match = re.search(r"[Hh]eight\s*:\s*(\d+)\s*(\d*)\s*pixels?",
                             video_section)

    width = None
    height = None

    if width_match:
        # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 920" -> "1920"
        w_groups = width_match.groups()
        if w_groups and len(w_groups) >= 2 and w_groups[1]:
            width = int(f"{w_groups[0]}{w_groups[1]}")
        elif w_groups and len(w_groups) >= 1 and w_groups[0]:
            width = int(w_groups[0])
        else:
            width = None

    if height_match:
        # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 080" -> "1080"
        h_groups = height_match.groups()
        if h_groups and len(h_groups) >= 2 and h_groups[1]:
            height = int(f"{h_groups[0]}{h_groups[1]}")
        elif h_groups and len(h_groups) >= 1 and h_groups[0]:
            height = int(h_groups[0])
        else:
            height = None

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•å…¶ä»–æ ¼å¼
    if not width or not height:
        # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾ç±»ä¼¼ "1920 / 1080" çš„æ ¼å¼
        resolution_match = re.search(r"(\d{3,4})\s*/\s*(\d{3,4})",
                                     video_section)
        if resolution_match:
            width = int(resolution_match.group(1))
            height = int(resolution_match.group(2))
        else:
            # æŸ¥æ‰¾å…¶ä»–æ ¼å¼çš„åˆ†è¾¨ç‡ä¿¡æ¯
            other_resolution_match = re.search(r"(\d{3,4})\s*[xX]\s*(\d{3,4})",
                                               mediainfo_text)
            if other_resolution_match:
                width = int(other_resolution_match.group(1))
                height = int(other_resolution_match.group(2))

    # å¦‚æœæ‰¾åˆ°äº†å®½åº¦å’Œé«˜åº¦ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    if width and height:
        # æ ¹æ®é«˜åº¦ç¡®å®šæ ‡å‡†åˆ†è¾¨ç‡
        if height <= 480:
            return "480p"
        elif height <= 576:
            return "576p"
        elif height <= 720:
            return "720p"
        elif height <= 1080:
            return "1080p"
        elif height <= 1440:
            return "1440p"
        elif height <= 2160:
            return "2160p"
        else:
            # å¯¹äºå…¶ä»–éæ ‡å‡†åˆ†è¾¨ç‡ï¼Œè¿”å›åŸå§‹é«˜åº¦åŠ p
            return f"{height}p"

    return ""


def _upload_to_pixhost_direct(image_path: str, api_url: str, params: dict,
                              headers: dict):
    """ç›´æ¥ä¸Šä¼ å›¾ç‰‡åˆ°Pixhost"""
    try:
        with open(image_path, 'rb') as f:
            files = {'img': f}
            print("æ­£åœ¨å‘é€ä¸Šä¼ è¯·æ±‚åˆ° Pixhost...")
            response = requests.post(api_url,
                                     data=params,
                                     files=files,
                                     headers=headers,
                                     timeout=30)

            if response.status_code == 200:
                data = response.json()
                show_url = data.get('show_url')
                print(f"ç›´æ¥ä¸Šä¼ æˆåŠŸï¼å›¾ç‰‡é“¾æ¥: {show_url}")
                return show_url
            else:
                print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                return None
    except FileNotFoundError:
        print(f"   âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶")
        return None
    except requests.exceptions.SSLError as e:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: SSLè¿æ¥é”™è¯¯")
        return None
    except requests.exceptions.ConnectionError as e:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: ç½‘ç»œè¿æ¥è¢«é‡ç½®")
        return None
    except requests.exceptions.Timeout:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: è¯·æ±‚è¶…æ—¶")
        return None
    except Exception as e:
        # åªæ‰“å°å¼‚å¸¸ç±»å‹å’Œç®€çŸ­æè¿°ï¼Œä¸æ‰“å°å®Œæ•´å †æ ˆ
        error_type = type(e).__name__
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: {error_type}")
        return None


def _get_downloader_proxy_config(downloader_id: str = None):
    """
    æ ¹æ®ä¸‹è½½å™¨IDè·å–ä»£ç†é…ç½®ã€‚

    :param downloader_id: ä¸‹è½½å™¨ID
    :return: ä»£ç†é…ç½®å­—å…¸ï¼Œå¦‚æœä¸éœ€è¦ä»£ç†åˆ™è¿”å›None
    """
    if not downloader_id:
        return None

    config = config_manager.get()
    downloaders = config.get("downloaders", [])

    for downloader in downloaders:
        if downloader.get("id") == downloader_id:
            use_proxy = downloader.get("use_proxy", False)
            if use_proxy:
                host_value = downloader.get('host', '')
                proxy_port = downloader.get('proxy_port', 9090)
                if host_value.startswith(('http://', 'https://')):
                    parsed_url = urlparse(host_value)
                else:
                    parsed_url = urlparse(f"http://{host_value}")
                proxy_ip = parsed_url.hostname
                if not proxy_ip:
                    if '://' in host_value:
                        proxy_ip = host_value.split('://')[1].split(
                            ':')[0].split('/')[0]
                    else:
                        proxy_ip = host_value.split(':')[0]
                proxy_config = {
                    "proxy_base_url": f"http://{proxy_ip}:{proxy_port}",
                }
                return proxy_config
            break

    return None


def check_intro_completeness(body_text: str) -> dict:
    """
    æ£€æŸ¥ç®€ä»‹æ˜¯å¦å®Œæ•´ï¼ŒåŒ…å«å¿…è¦çš„å½±ç‰‡ä¿¡æ¯å­—æ®µã€‚
    
    :param body_text: ç®€ä»‹æ­£æ–‡å†…å®¹
    :return: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸ {
        "is_complete": bool,      # æ˜¯å¦å®Œæ•´
        "missing_fields": list,   # ç¼ºå¤±çš„å­—æ®µåˆ—è¡¨
        "found_fields": list      # å·²æ‰¾åˆ°çš„å­—æ®µåˆ—è¡¨
    }
    
    ç¤ºä¾‹:
        >>> result = check_intro_completeness(intro_body)
        >>> if not result["is_complete"]:
        >>>     print(f"ç¼ºå°‘å­—æ®µ: {result['missing_fields']}")
    """
    if not body_text:
        return {
            "is_complete": False,
            "missing_fields": ["æ‰€æœ‰å­—æ®µ"],
            "found_fields": []
        }

    # å®šä¹‰å¿…è¦å­—æ®µçš„åŒ¹é…æ¨¡å¼
    # æ¯ä¸ªå­—æ®µå¯ä»¥æœ‰å¤šä¸ªåŒ¹é…æ¨¡å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
    required_patterns = {
        "ç‰‡å": [
            r"â—\s*ç‰‡\s*å", r"â—\s*è¯‘\s*å", r"â—\s*æ ‡\s*é¢˜", r"ç‰‡å\s*[:ï¼š]",
            r"è¯‘å\s*[:ï¼š]", r"Title\s*[:ï¼š]"
        ],
        "å¹´ä»£": [
            r"â—\s*å¹´\s*ä»£", r"â—\s*å¹´\s*ä»½", r"å¹´ä»½\s*[:ï¼š]", r"å¹´ä»£\s*[:ï¼š]",
            r"Year\s*[:ï¼š]"
        ],
        "äº§åœ°": [
            r"â—\s*äº§\s*åœ°", r"â—\s*å›½\s*å®¶", r"â—\s*åœ°\s*åŒº", r"åˆ¶ç‰‡å›½å®¶/åœ°åŒº\s*[:ï¼š]",
            r"åˆ¶ç‰‡å›½å®¶\s*[:ï¼š]", r"å›½å®¶\s*[:ï¼š]", r"äº§åœ°\s*[:ï¼š]", r"Country\s*[:ï¼š]"
        ],
        "ç±»åˆ«": [
            r"â—\s*ç±»\s*åˆ«", r"â—\s*ç±»\s*å‹", r"ç±»å‹\s*[:ï¼š]", r"ç±»åˆ«\s*[:ï¼š]",
            r"Genre\s*[:ï¼š]"
        ],
        "è¯­è¨€": [r"â—\s*è¯­\s*è¨€", r"è¯­è¨€\s*[:ï¼š]", r"Language\s*[:ï¼š]"],
        "å¯¼æ¼”": [r"â—\s*å¯¼\s*æ¼”", r"å¯¼æ¼”\s*[:ï¼š]", r"Director\s*[:ï¼š]"],
        "ç®€ä»‹": [
            r"â—\s*ç®€\s*ä»‹", r"â—\s*å‰§\s*æƒ…", r"â—\s*å†…\s*å®¹", r"ç®€ä»‹\s*[:ï¼š]",
            r"å‰§æƒ…\s*[:ï¼š]", r"å†…å®¹ç®€ä»‹\s*[:ï¼š]", r"Plot\s*[:ï¼š]", r"Synopsis\s*[:ï¼š]"
        ]
    }

    found_fields = []
    missing_fields = []

    # æ£€æŸ¥æ¯ä¸ªå¿…è¦å­—æ®µ
    for field_name, patterns in required_patterns.items():
        field_found = False
        for pattern in patterns:
            if re.search(pattern, body_text, re.IGNORECASE):
                field_found = True
                break

        if field_found:
            found_fields.append(field_name)
        else:
            missing_fields.append(field_name)

    # åˆ¤æ–­å®Œæ•´æ€§ï¼šå¿…é¡»åŒ…å«ä»¥ä¸‹å…³é”®å­—æ®µ
    # ç‰‡åã€äº§åœ°ã€å¯¼æ¼”ã€ç®€ä»‹ è¿™4ä¸ªå­—æ®µæ˜¯æœ€å…³é”®çš„
    critical_fields = ["ç‰‡å", "äº§åœ°", "å¯¼æ¼”", "ç®€ä»‹"]
    is_complete = all(field in found_fields for field in critical_fields)

    return {
        "is_complete": is_complete,
        "missing_fields": missing_fields,
        "found_fields": found_fields
    }


def is_image_url_valid_robust(url: str) -> bool:
    """
    ä¸€ä¸ªæ›´ç¨³å¥çš„æ–¹æ³•ï¼Œå½“HEADè¯·æ±‚å¤±è´¥æ—¶ï¼Œä¼šå°è¯•ä½¿ç”¨GETè¯·æ±‚ï¼ˆæµå¼ï¼‰è¿›è¡ŒéªŒè¯ã€‚
    å¦‚æœç›´æ¥è¯·æ±‚å¤±è´¥ï¼Œä¼šå°è¯•ä½¿ç”¨å…¨å±€ä»£ç†é‡è¯•ä¸€æ¬¡ã€‚
    """
    if not url:
        return False

    # ç¬¬ä¸€æ¬¡å°è¯•ï¼šä¸ä½¿ç”¨ä»£ç†
    try:
        # é¦–å…ˆå°è¯•HEADè¯·æ±‚ï¼Œå…è®¸é‡å®šå‘
        response = requests.head(url, timeout=5, allow_redirects=True)
        response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

        # æ£€æŸ¥Content-Type
        content_type = response.headers.get('Content-Type')
        if content_type and content_type.startswith('image/'):
            return True
        else:
            logging.warning(
                f"é“¾æ¥æœ‰æ•ˆä½†å†…å®¹å¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url} (Content-Type: {content_type})")
            return False

    except requests.exceptions.RequestException:
        # å¦‚æœHEADè¯·æ±‚å¤±è´¥ï¼Œå°è¯•GETè¯·æ±‚
        try:
            response = requests.get(url,
                                    stream=True,
                                    timeout=5,
                                    allow_redirects=True)
            response.raise_for_status()

            # æ£€æŸ¥Content-Type
            content_type = response.headers.get('Content-Type')
            if content_type and content_type.startswith('image/'):
                return True
            else:
                logging.warning(
                    f"é“¾æ¥æœ‰æ•ˆä½†å†…å®¹å¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url} (Content-Type: {content_type})")
                return False

        except requests.exceptions.RequestException as e:
            logging.warning(f"å›¾ç‰‡é“¾æ¥GETè¯·æ±‚ä¹Ÿå¤±è´¥äº†: {url} - {e}")

            # ä¸ä½¿ç”¨å…¨å±€ä»£ç†é‡è¯•ï¼Œç›´æ¥è¿”å›å¤±è´¥
            return False


def extract_audio_codec_from_mediainfo(mediainfo_text: str) -> str:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªéŸ³é¢‘æµçš„æ ¼å¼ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: éŸ³é¢‘æ ¼å¼å­—ç¬¦ä¸² (ä¾‹å¦‚ "DTS", "AC-3", "FLAC")ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    if not mediainfo_text:
        return ""

    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª Audio éƒ¨åˆ† (æ”¯æŒ "Audio" å’Œ "Audio #1")
    audio_section_match = re.search(r"Audio(?: #1)?[\s\S]*?(?=\n\n|\Z)",
                                    mediainfo_text)
    if not audio_section_match:
        logging.warning("åœ¨MediaInfoä¸­æœªæ‰¾åˆ° 'Audio' éƒ¨åˆ†ã€‚")
        return ""

    audio_section = audio_section_match.group(0)

    # åœ¨ Audio éƒ¨åˆ†æŸ¥æ‰¾ Format
    format_match = re.search(r"Format\s*:\s*(.+)", audio_section)
    if format_match:
        audio_format = format_match.group(1).strip()
        logging.info(f"ä»MediaInfoçš„'Audio'éƒ¨åˆ†æå–åˆ°æ ¼å¼: {audio_format}")
        return audio_format

    logging.warning("åœ¨MediaInfoçš„'Audio'éƒ¨åˆ†æœªæ‰¾åˆ° 'Format' ä¿¡æ¯ã€‚")
    return ""


def _get_smart_poster_url(original_url: str) -> str:
    """
    æ™ºèƒ½æµ·æŠ¥URLè·å–å’ŒéªŒè¯ï¼Œå¹¶è‡ªåŠ¨è½¬å­˜åˆ°pixhost
    å‚è€ƒæ²¹çŒ´æ’ä»¶é€»è¾‘ï¼š
    1. ä¼˜å…ˆå°è¯•è±†ç“£å®˜æ–¹é«˜æ¸…å›¾ï¼ˆå¤šåŸŸåè½®è¯¢ img1-img9ï¼‰
    2. å°è¯•ä¸¤ç§æ¸…æ™°åº¦è·¯å¾„ï¼ˆl_ratio_poster é«˜æ¸…ï¼Œm_ratio_poster ä¸­æ¸…ï¼‰
    3. å¦‚æœè±†ç“£å…¨å¤±è´¥ï¼Œå°è¯•ç¬¬ä¸‰æ–¹æ‰˜ç®¡ï¼ˆdou.img.lithub.ccï¼‰
    4. éªŒè¯æˆåŠŸåè‡ªåŠ¨è½¬å­˜åˆ°pixhost
    
    :param original_url: åŸå§‹æµ·æŠ¥URL
    :return: pixhostç›´é“¾URLï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not original_url:
        return ""

    print(f"[*] å¼€å§‹éªŒè¯æµ·æŠ¥é“¾æ¥...")
    print(f"[*] æ£€æµ‹åˆ°épixhostå›¾ç‰‡ï¼Œæ‰§è¡Œæ™ºèƒ½æµ·æŠ¥è·å–...")
    print(f"å¼€å§‹æ™ºèƒ½æµ·æŠ¥URLéªŒè¯: {original_url}")

    # æ£€æŸ¥æ˜¯å¦ä¸ºè±†ç“£å›¾ç‰‡
    douban_match = re.search(r'https?://img(\d+)\.doubanio\.com.*?/(p\d+)',
                             original_url)

    if douban_match:
        original_domain_num = douban_match.group(1)
        image_id = douban_match.group(2)

        print(f"æ£€æµ‹åˆ°è±†ç“£å›¾ç‰‡: åŸŸåimg{original_domain_num}, å›¾ç‰‡ID={image_id}")

        # ç”Ÿæˆå€™é€‰URLåˆ—è¡¨
        candidates = []

        # ä¼˜å…ˆåŸå§‹åŸŸå
        domain_numbers = [original_domain_num]
        # æ·»åŠ å…¶ä»–åŸŸå1-9
        for i in range(1, 10):
            if str(i) != original_domain_num:
                domain_numbers.append(str(i))

        # è·¯å¾„ä¼˜å…ˆçº§ï¼šå…ˆé«˜æ¸…ï¼Œåä¸­æ¸…
        paths = [
            'view/photo/l_ratio_poster/public',  # é«˜æ¸…
            'view/photo/m_ratio_poster/public'  # ä¸­æ¸…
        ]

        # ç”Ÿæˆå€™é€‰URLçŸ©é˜µ
        for domain_num in domain_numbers:
            for path in paths:
                candidate_url = f"https://img{domain_num}.doubanio.com/{path}/{image_id}.jpg"
                candidates.append(candidate_url)

        print(f"ç”Ÿæˆ {len(candidates)} ä¸ªå€™é€‰URL")

        # ä¾æ¬¡éªŒè¯å€™é€‰URL
        for i, candidate_url in enumerate(candidates):
            domain_info = re.search(r'img(\d+)\.doubanio\.com', candidate_url)
            path_info = 'é«˜æ¸…' if 'l_ratio_poster' in candidate_url else 'ä¸­æ¸…'
            domain_num = domain_info.group(1) if domain_info else '?'

            print(
                f"æµ‹è¯• [{i+1}/{len(candidates)}] img{domain_num} ({path_info}): {candidate_url}"
            )

            if _validate_image_url(candidate_url):
                print(f"âœ“ éªŒè¯æˆåŠŸï¼ä½¿ç”¨ img{domain_num} åŸŸå")
                print(f"[*] æ™ºèƒ½æµ·æŠ¥è·å–æˆåŠŸ: {candidate_url}")
                
                # è½¬å­˜åˆ°pixhost
                pixhost_url = _transfer_poster_to_pixhost(candidate_url)
                if pixhost_url:
                    return pixhost_url
                else:
                    print("[!] pixhostè½¬å­˜å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éªŒè¯URL")
                    return candidate_url
            else:
                print(f"âœ— img{domain_num} éªŒè¯å¤±è´¥")

        # è±†ç“£å…¨éƒ¨å¤±è´¥ï¼Œå°è¯•ç¬¬ä¸‰æ–¹æ‰˜ç®¡
        print("è±†ç“£å®˜æ–¹å›¾ç‰‡å…¨éƒ¨å¤±è´¥ï¼Œå°è¯•ç¬¬ä¸‰æ–¹æ‰˜ç®¡...")

        # ä»åŸå§‹URLä¸­æå–è±†ç“£ID
        douban_id_match = re.search(r'/subject/(\d+)', original_url)
        if not douban_id_match:
            # å°è¯•ä»å›¾ç‰‡IDæ¨æµ‹ï¼ˆè¿™é€šå¸¸ä¸å¯è¡Œï¼Œä½†ä½œä¸ºå¤‡é€‰ï¼‰
            print("æ— æ³•æå–è±†ç“£IDï¼Œè·³è¿‡ç¬¬ä¸‰æ–¹æ‰˜ç®¡")
        else:
            douban_id = douban_id_match.group(1)
            third_party_url = f"https://dou.img.lithub.cc/movie/{douban_id}.jpg"
            print(f"æµ‹è¯•ç¬¬ä¸‰æ–¹URL: {third_party_url}")

            if _validate_image_url(third_party_url):
                print("âœ“ ç¬¬ä¸‰æ–¹URLéªŒè¯æˆåŠŸ")
                print(f"[*] æ™ºèƒ½æµ·æŠ¥è·å–æˆåŠŸ: {third_party_url}")
                
                # è½¬å­˜åˆ°pixhost
                pixhost_url = _transfer_poster_to_pixhost(third_party_url)
                if pixhost_url:
                    return pixhost_url
                else:
                    print("[!] pixhostè½¬å­˜å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éªŒè¯URL")
                    return third_party_url
            else:
                print("âœ— ç¬¬ä¸‰æ–¹URLéªŒè¯å¤±è´¥")

    else:
        # éè±†ç“£å›¾ç‰‡ï¼Œç›´æ¥éªŒè¯åŸå§‹URL
        print("éè±†ç“£å›¾ç‰‡ï¼Œç›´æ¥éªŒè¯åŸå§‹URL")
        if _validate_image_url(original_url):
            print("âœ“ åŸå§‹URLéªŒè¯æˆåŠŸ")
            print(f"[*] æ™ºèƒ½æµ·æŠ¥è·å–æˆåŠŸ: {original_url}")
            
            # è½¬å­˜åˆ°pixhost
            pixhost_url = _transfer_poster_to_pixhost(original_url)
            if pixhost_url:
                return pixhost_url
            else:
                print("[!] pixhostè½¬å­˜å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éªŒè¯URL")
                return original_url
        else:
            print("âœ— åŸå§‹URLéªŒè¯å¤±è´¥")

    print("æ‰€æœ‰URLéªŒè¯éƒ½å¤±è´¥")
    return ""


def _validate_image_url(url: str) -> bool:
    """
    éªŒè¯å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ
    ä½¿ç”¨HEADè¯·æ±‚éªŒè¯URLæ˜¯å¦å¯è®¿é—®ä¸”è¿”å›æœ‰æ•ˆå›¾ç‰‡
    
    :param url: å›¾ç‰‡URL
    :return: URLæœ‰æ•ˆè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not url:
        return False

    try:
        headers = {
            'User-Agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://movie.douban.com/'
        }

        response = requests.head(url,
                                 headers=headers,
                                 timeout=10,
                                 allow_redirects=True)

        if response.status_code == 200:
            # æ£€æŸ¥Content-Type
            content_type = response.headers.get('Content-Type', '').lower()
            if 'image/' in content_type:
                # æ£€æŸ¥Content-Lengthï¼ˆè‡³å°‘å¤§äº1KBï¼‰
                content_length = response.headers.get('Content-Length')
                if content_length:
                    file_size = int(content_length)
                    if file_size > 1024:
                        return True
                    else:
                        print(f"   æ–‡ä»¶å¤ªå°: {file_size} bytes")
                        return False
                else:
                    # å¦‚æœæ²¡æœ‰Content-Lengthï¼Œè®¤ä¸ºæœ‰æ•ˆ
                    return True
            else:
                print(f"   æ— æ•ˆçš„Content-Type: {content_type}")
                return False
        else:
            print(f"   HTTPçŠ¶æ€ç : {response.status_code}")
            return False

    except Exception as e:
        print(f"   éªŒè¯å¼‚å¸¸: {type(e).__name__}")
        return False


def _transfer_poster_to_pixhost(poster_url: str) -> str:
    """
    å°†æµ·æŠ¥å›¾ç‰‡è½¬å­˜åˆ°pixhost
    
    :param poster_url: æµ·æŠ¥å›¾ç‰‡URL
    :return: pixhostç›´é“¾URLï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not poster_url:
        return ""

    print(f"å¼€å§‹è½¬å­˜æµ·æŠ¥åˆ°pixhost: {poster_url}")

    try:
        # 1. ä¸‹è½½å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶
        headers = {
            'User-Agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://movie.douban.com/'
        }

        response = requests.get(poster_url, headers=headers, timeout=30)
        response.raise_for_status()

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if len(response.content) == 0:
            print("   ä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶ä¸ºç©º")
            return ""

        if len(response.content) > 10 * 1024 * 1024:
            print("   å›¾ç‰‡æ–‡ä»¶è¿‡å¤§ (>10MB)")
            return ""

        print(f"   å›¾ç‰‡ä¸‹è½½æˆåŠŸï¼Œå¤§å°: {len(response.content)} bytes")

        # 2. ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_file = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as f:
                f.write(response.content)
                temp_file = f.name

            print(f"   ä¸´æ—¶æ–‡ä»¶å·²ä¿å­˜: {temp_file}")

            # 3. ä¸Šä¼ åˆ°pixhost
            api_url = 'https://api.pixhost.to/images'
            params = {'content_type': 0, 'max_th_size': 420}
            upload_headers = {
                'User-Agent':
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json'
            }

            with open(temp_file, 'rb') as f:
                files = {'img': ('poster.jpg', f, 'image/jpeg')}
                upload_response = requests.post(api_url,
                                                data=params,
                                                files=files,
                                                headers=upload_headers,
                                                timeout=30)

            if upload_response.status_code == 200:
                data = upload_response.json()
                show_url = data.get('show_url')

                if not show_url:
                    print("   APIæœªè¿”å›æœ‰æ•ˆURL")
                    return ""

                # è½¬æ¢ä¸ºç›´é“¾URL
                direct_url = _convert_pixhost_url_to_direct(show_url)

                if direct_url:
                    print(f"   ä¸Šä¼ æˆåŠŸï¼ç›´é“¾: {direct_url}")
                    return direct_url
                else:
                    print("   URLè½¬æ¢å¤±è´¥")
                    return ""
            else:
                print(f"   ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {upload_response.status_code}")
                return ""

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                    print(f"   ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†: {temp_file}")
                except:
                    pass

    except Exception as e:
        print(f"   è½¬å­˜å¤±è´¥: {type(e).__name__} - {e}")
        return ""


def _convert_pixhost_url_to_direct(show_url: str) -> str:
    """
    å°†pixhostçš„show URLè½¬æ¢ä¸ºç›´é“¾URL
    å‚è€ƒæ²¹çŒ´æ’ä»¶çš„convertToDirectUrlå‡½æ•°
    
    :param show_url: pixhost show URL
    :return: ç›´é“¾URLï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not show_url:
        return ""

    try:
        # æ–¹æ¡ˆ1: ç›´æ¥æ›¿æ¢åŸŸåå’Œè·¯å¾„
        direct_url = show_url.replace(
            'https://pixhost.to/show/',
            'https://img1.pixhost.to/images/').replace(
                'https://pixhost.to/th/', 'https://img1.pixhost.to/images/')

        # ç§»é™¤ç¼©ç•¥å›¾åç¼€ï¼ˆå¦‚ _cover.jpg -> .jpgï¼‰
        direct_url = re.sub(r'_..\.jpg$', '.jpg', direct_url)

        # æ–¹æ¡ˆ2: å¦‚æœæ–¹æ¡ˆ1å¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™æå–é‡å»ºURL
        if not direct_url.startswith('https://img1.pixhost.to/images/'):
            match = re.search(r'(\d+)/([^/]+\.(jpg|png|gif))', show_url)
            if match:
                direct_url = f"https://img1.pixhost.to/images/{match.group(1)}/{match.group(2)}"

        # æœ€ç»ˆéªŒè¯
        if re.match(
                r'^https://img1\.pixhost\.to/images/\d+/[^/]+\.(jpg|png|gif)$',
                direct_url):
            return direct_url
        else:
            print(f"   URLæ ¼å¼éªŒè¯å¤±è´¥: {direct_url}")
            return ""

    except Exception as e:
        print(f"   URLè½¬æ¢å¼‚å¸¸: {e}")
        return ""
