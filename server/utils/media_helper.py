# utils/media_helper.py

import base64
import logging
import mimetypes
import re
import os
import shutil
import subprocess
import tempfile
import requests
import json
import time
import random
import cloudscraper
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from pymediainfo import MediaInfo
from config import TEMP_DIR, config_manager
from qbittorrentapi import Client as qbClient
from transmission_rpc import Client as TrClient
from utils import ensure_scheme
from PIL import Image


def translate_path(downloader_id: str, remote_path: str) -> str:
    """
    å°†ä¸‹è½½å™¨çš„è¿œç¨‹è·¯å¾„è½¬æ¢ä¸º PT Nexus å®¹å™¨å†…çš„æœ¬åœ°è·¯å¾„ã€‚

    :param downloader_id: ä¸‹è½½å™¨ID
    :param remote_path: ä¸‹è½½å™¨ä¸­çš„è¿œç¨‹è·¯å¾„
    :return: PT Nexus å®¹å™¨å†…å¯è®¿é—®çš„æœ¬åœ°è·¯å¾„
    """
    if not downloader_id or not remote_path:
        return remote_path

    # è·å–ä¸‹è½½å™¨é…ç½®
    config = config_manager.get()
    downloaders = config.get("downloaders", [])

    for downloader in downloaders:
        if downloader.get("id") == downloader_id:
            path_mappings = downloader.get("path_mappings", [])
            if not path_mappings:
                # æ²¡æœ‰é…ç½®è·¯å¾„æ˜ å°„ï¼Œç›´æ¥è¿”å›åŸè·¯å¾„
                return remote_path

            # æŒ‰è¿œç¨‹è·¯å¾„é•¿åº¦é™åºæ’åºï¼Œä¼˜å…ˆåŒ¹é…æœ€é•¿çš„è·¯å¾„ï¼ˆæ›´ç²¾ç¡®ï¼‰
            sorted_mappings = sorted(path_mappings,
                                     key=lambda x: len(x.get('remote', '')),
                                     reverse=True)

            for mapping in sorted_mappings:
                remote = mapping.get('remote', '')
                local = mapping.get('local', '')

                if not remote or not local:
                    continue

                # ç¡®ä¿è·¯å¾„æ¯”è¾ƒæ—¶ç»Ÿä¸€å¤„ç†æœ«å°¾çš„æ–œæ 
                remote = remote.rstrip('/')
                remote_path_normalized = remote_path.rstrip('/')

                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼ˆå®Œå…¨åŒ¹é…æˆ–å‰ç¼€åŒ¹é…ï¼‰
                if remote_path_normalized == remote:
                    # å®Œå…¨åŒ¹é…
                    return local
                elif remote_path_normalized.startswith(remote + '/'):
                    # å‰ç¼€åŒ¹é…ï¼Œæ›¿æ¢è·¯å¾„
                    relative_path = remote_path_normalized[len(remote
                                                               ):].lstrip('/')
                    return os.path.join(local, relative_path)

            # æ²¡æœ‰åŒ¹é…çš„æ˜ å°„ï¼Œè¿”å›åŸè·¯å¾„
            return remote_path

    # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ä¸‹è½½å™¨ï¼Œè¿”å›åŸè·¯å¾„
    return remote_path


def _upload_to_pixhost(image_path: str):
    """
    å°†å•ä¸ªå›¾ç‰‡æ–‡ä»¶ä¸Šä¼ åˆ° Pixhost.toã€‚

    :param image_path: æœ¬åœ°å›¾ç‰‡æ–‡ä»¶çš„è·¯å¾„ã€‚
    :return: æˆåŠŸæ—¶è¿”å›å›¾ç‰‡çš„å±•ç¤ºURLï¼Œå¤±è´¥æ—¶è¿”å›Noneã€‚
    """
    api_url = 'https://api.pixhost.to/images'
    params = {'content_type': 0}
    headers = {
        'User-Agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    print(f"å‡†å¤‡ä¸Šä¼ å›¾ç‰‡: {image_path}")

    if not os.path.exists(image_path):
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ {image_path}")
        return None

    # è¯»å–ä»£ç†é…ç½®
    config = config_manager.get()
    proxy_mode = config.get("cross_seed", {}).get("pixhost_proxy_mode",
                                                  "retry")
    global_proxy = config.get("network", {}).get("proxy_url")

    # æ ¹æ®ä»£ç†æ¨¡å¼å†³å®šä¸Šä¼ ç­–ç•¥
    if proxy_mode == "always" and global_proxy:
        print(f"ä»£ç†æ¨¡å¼è®¾ç½®ä¸ºæ€»æ˜¯ä½¿ç”¨ä»£ç†ï¼Œä½¿ç”¨ä»£ç†: {global_proxy}")
        return _upload_to_pixhost_with_proxy(image_path, api_url, params,
                                             headers, global_proxy)
    elif proxy_mode == "never":
        print("ä»£ç†æ¨¡å¼è®¾ç½®ä¸ºä¸ä½¿ç”¨ä»£ç†ï¼Œç›´æ¥ä¸Šä¼ ")
        return _upload_to_pixhost_direct(image_path, api_url, params, headers)
    else:
        # é»˜è®¤æ¨¡å¼ï¼šå¤±è´¥æ—¶é‡è¯•æˆ–æ²¡æœ‰é…ç½®ä»£ç†æ—¶ç›´æ¥ä¸Šä¼ 
        print("ä½¿ç”¨é»˜è®¤ä¸Šä¼ ç­–ç•¥ï¼šå…ˆå°è¯•ç›´æ¥ä¸Šä¼ ")
        result = _upload_to_pixhost_direct(image_path, api_url, params,
                                           headers)

        # å¦‚æœç›´æ¥ä¸Šä¼ å¤±è´¥ä¸”é…ç½®äº†ä»£ç†ï¼Œåˆ™å°è¯•ä»£ç†ä¸Šä¼ 
        if not result and global_proxy and proxy_mode == "retry":
            print("ç›´æ¥ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä»£ç†ä¸Šä¼ ...")
            result = _upload_to_pixhost_with_proxy(image_path, api_url, params,
                                                   headers, global_proxy)

        return result


def _get_agsv_auth_token():
    """ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é‚®ç®±å’Œå¯†ç è·å– æœ«æ—¥å›¾åºŠ çš„æˆæƒ Tokenã€‚"""
    config = config_manager.get().get("cross_seed", {})
    email = config.get("agsv_email")
    password = config.get("agsv_password")

    if not email or not password:
        logging.warning("æœ«æ—¥å›¾åºŠ é‚®ç®±æˆ–å¯†ç æœªé…ç½®ï¼Œæ— æ³•è·å– Tokenã€‚")
        return None

    token_url = "https://img.seedvault.cn/api/v1/tokens"
    payload = {"email": email, "password": password}
    headers = {"Accept": "application/json"}
    print("æ­£åœ¨ä¸º æœ«æ—¥å›¾åºŠ è·å–æˆæƒ Token...")
    try:
        response = requests.post(token_url,
                                 headers=headers,
                                 json=payload,
                                 timeout=30)
        if response.status_code == 200 and response.json().get("status"):
            token = response.json().get("data", {}).get("token")
            if token:
                print("   âœ… æˆåŠŸè·å– æœ«æ—¥å›¾åºŠ Tokenï¼")
                return token

        logging.error(
            f"è·å– æœ«æ—¥å›¾åºŠ Token å¤±è´¥ã€‚çŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}"
        )
        print(f"   âŒ è·å– æœ«æ—¥å›¾åºŠ Token å¤±è´¥: {response.text}")
        return None
    except requests.exceptions.RequestException as e:
        logging.error(f"è·å– æœ«æ—¥å›¾åºŠ Token æ—¶ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        print(f"   âŒ è·å– æœ«æ—¥å›¾åºŠ Token æ—¶ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return None


def _upload_to_agsv(image_path: str, token: str):
    """ä½¿ç”¨ç»™å®šçš„ Token ä¸Šä¼ å•ä¸ªå›¾ç‰‡åˆ° æœ«æ—¥å›¾åºŠã€‚"""
    upload_url = "https://img.seedvault.cn/api/v1/upload"
    headers = {
        "Authorization":
        f"Bearer {token}",
        "Accept":
        "application/json",
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    }

    mime_type = mimetypes.guess_type(
        image_path)[0] or 'application/octet-stream'
    image_name = os.path.basename(image_path)

    print(f"å‡†å¤‡ä¸Šä¼ å›¾ç‰‡åˆ° æœ«æ—¥å›¾åºŠ: {image_name}")
    try:
        with open(image_path, 'rb') as f:
            files = {'file': (image_name, f, mime_type)}
            response = requests.post(upload_url,
                                     headers=headers,
                                     files=files,
                                     timeout=120)

        data = response.json()
        if response.status_code == 200 and data.get("status"):
            image_url = data.get("data", {}).get("links", {}).get("url")
            print(f"   âœ… æœ«æ—¥å›¾åºŠ ä¸Šä¼ æˆåŠŸï¼URL: {image_url}")
            return image_url
        else:
            message = data.get('message', 'æ— è¯¦ç»†ä¿¡æ¯')
            logging.error(f"æœ«æ—¥å›¾åºŠ ä¸Šä¼ å¤±è´¥ã€‚API æ¶ˆæ¯: {message}")
            print(f"   âŒ æœ«æ—¥å›¾åºŠ ä¸Šä¼ å¤±è´¥: {message}")
            return None
    except (requests.exceptions.RequestException,
            requests.exceptions.JSONDecodeError) as e:
        logging.error(f"ä¸Šä¼ åˆ° æœ«æ—¥å›¾åºŠ æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(f"   âŒ ä¸Šä¼ åˆ° æœ«æ—¥å›¾åºŠ æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def _get_smart_screenshot_points(video_path: str,
                                 num_screenshots: int = 5) -> list[float]:
    """
    [ä¼˜åŒ–ç‰ˆ] ä½¿ç”¨ ffprobe æ™ºèƒ½åˆ†æè§†é¢‘å­—å¹•ï¼Œé€‰æ‹©æœ€ä½³çš„æˆªå›¾æ—¶é—´ç‚¹ã€‚
    - é€šè¿‡ `-read_intervals` å‚æ•°å®ç°åˆ†æ®µè¯»å–ï¼Œé¿å…å…¨æ–‡ä»¶æ‰«æï¼Œå¤§å¹…æå‡å¤§æ–‡ä»¶å¤„ç†é€Ÿåº¦ã€‚
    - ä¼˜å…ˆé€‰æ‹© ASS > SRT > PGS æ ¼å¼çš„å­—å¹•ã€‚
    - ä¼˜å…ˆåœ¨è§†é¢‘çš„ 30%-80% "é»„é‡‘æ—¶æ®µ" å†…éšæœºé€‰æ‹©ã€‚
    - åœ¨æ‰€æœ‰æ™ºèƒ½åˆ†æå¤±è´¥æ—¶ï¼Œä¼˜é›…åœ°å›é€€åˆ°æŒ‰ç™¾åˆ†æ¯”é€‰æ‹©ã€‚
    """
    print("\n--- å¼€å§‹æ™ºèƒ½æˆªå›¾æ—¶é—´ç‚¹åˆ†æ (å¿«é€Ÿæ‰«ææ¨¡å¼) ---")
    if not shutil.which("ffprobe"):
        print("è­¦å‘Š: æœªæ‰¾åˆ° ffprobeï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½åˆ†æã€‚")
        return []

    try:
        cmd_duration = [
            "ffprobe", "-v", "error", "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1", video_path
        ]
        result = subprocess.run(cmd_duration,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        duration = float(result.stdout.strip())
        print(f"è§†é¢‘æ€»æ—¶é•¿: {duration:.2f} ç§’")
    except Exception as e:
        print(f"é”™è¯¯ï¼šä½¿ç”¨ ffprobe è·å–è§†é¢‘æ—¶é•¿å¤±è´¥ã€‚{e}")
        return []

    # æ¢æµ‹å­—å¹•æµçš„éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒæœ¬èº«é€Ÿåº¦å¾ˆå¿«
    try:
        cmd_probe_subs = [
            "ffprobe", "-v", "quiet", "-print_format", "json", "-show_entries",
            "stream=index,codec_name,disposition", "-select_streams", "s",
            video_path
        ]
        result = subprocess.run(cmd_probe_subs,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        sub_data = json.loads(result.stdout)

        best_ass, best_srt, best_pgs = None, None, None
        for stream in sub_data.get("streams", []):
            disposition = stream.get("disposition", {})
            is_normal = not any([
                disposition.get("comment"),
                disposition.get("hearing_impaired"),
                disposition.get("visual_impaired")
            ])
            if is_normal:
                codec_name = stream.get("codec_name")
                if codec_name == "ass" and not best_ass: best_ass = stream
                elif codec_name == "subrip" and not best_srt: best_srt = stream
                elif codec_name == "hdmv_pgs_subtitle" and not best_pgs:
                    best_pgs = stream

        chosen_sub_stream = best_ass or best_srt or best_pgs
        if not chosen_sub_stream:
            print("æœªæ‰¾åˆ°åˆé€‚çš„æ­£å¸¸å­—å¹•æµã€‚")
            return []

        sub_index, sub_codec = chosen_sub_stream.get(
            "index"), chosen_sub_stream.get("codec_name")
        print(f"   âœ… æ‰¾åˆ°æœ€ä¼˜å­—å¹•æµ (æ ¼å¼: {sub_codec.upper()})ï¼Œæµç´¢å¼•: {sub_index}")

    except Exception as e:
        print(f"æ¢æµ‹å­—å¹•æµå¤±è´¥: {e}")
        return []

    subtitle_events = []
    try:
        # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ ---
        # 1. å®šä¹‰æˆ‘ä»¬è¦æ¢æµ‹çš„æ—¶é—´ç‚¹ï¼ˆä¾‹å¦‚ï¼Œè§†é¢‘çš„20%, 40%, 60%, 80%ä½ç½®ï¼‰
        probe_points = [0.2, 0.4, 0.6, 0.8]
        # 2. å®šä¹‰åœ¨æ¯ä¸ªæ¢æµ‹ç‚¹é™„è¿‘æ‰«æå¤šé•¿æ—¶é—´ï¼ˆä¾‹å¦‚ï¼Œ60ç§’ï¼‰ï¼Œæ—¶é—´è¶Šé•¿ï¼Œæ‰¾åˆ°å­—å¹•äº‹ä»¶è¶Šå¤šï¼Œä½†è€—æ—¶ä¹Ÿè¶Šé•¿
        probe_duration = 60

        # 3. æ„å»º -read_intervals å‚æ•°
        # æ ¼å¼ä¸º "start1%+duration1,start2%+duration2,..."
        intervals = []
        for point in probe_points:
            start_time = duration * point
            end_time = start_time + probe_duration
            if end_time > duration:
                end_time = duration  # ç¡®ä¿ä¸è¶…è¿‡è§†é¢‘æ€»é•¿
            intervals.append(f"{start_time}%{end_time}")

        read_intervals_arg = ",".join(intervals)
        print(f"   ğŸš€ å°†åªæ‰«æä»¥ä¸‹æ—¶é—´æ®µæ¥å¯»æ‰¾å­—å¹•: {read_intervals_arg}")

        # 4. å°† -read_intervals å‚æ•°æ·»åŠ åˆ° ffprobe å‘½ä»¤ä¸­
        cmd_extract = [
            "ffprobe",
            "-v",
            "quiet",
            "-read_intervals",
            read_intervals_arg,  # <--- æ–°å¢çš„å‚æ•°
            "-print_format",
            "json",
            "-show_packets",
            "-select_streams",
            str(sub_index),
            video_path
        ]

        # æ‰§è¡Œå‘½ä»¤ï¼Œç°åœ¨å®ƒä¼šå¿«éå¸¸å¤š
        result = subprocess.run(cmd_extract,
                                capture_output=True,
                                text=True,
                                check=True,
                                encoding='utf-8')
        # --- ã€æ ¸å¿ƒä¿®æ”¹ç»“æŸã€‘ ---

        events_data = json.loads(result.stdout)
        packets = events_data.get("packets", [])

        # åç»­å¤„ç†é€»è¾‘åŸºæœ¬ä¸å˜
        if sub_codec in ["ass", "subrip"]:
            for packet in packets:
                try:
                    start, dur = float(packet.get("pts_time")), float(
                        packet.get("duration_time"))
                    if dur > 0.1:
                        subtitle_events.append({
                            "start": start,
                            "end": start + dur
                        })
                except (ValueError, TypeError):
                    continue
        elif sub_codec == "hdmv_pgs_subtitle":
            for i in range(0, len(packets) - 1, 2):
                try:
                    start, end = float(packets[i].get("pts_time")), float(
                        packets[i + 1].get("pts_time"))
                    if end > start and (end - start) > 0.1:
                        subtitle_events.append({"start": start, "end": end})
                except (ValueError, TypeError):
                    continue

        if not subtitle_events: raise ValueError("åœ¨æŒ‡å®šåŒºé—´å†…æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆçš„æ—¶é—´äº‹ä»¶ã€‚")
        print(f"   âœ… æˆåŠŸä»æŒ‡å®šåŒºé—´æå–åˆ° {len(subtitle_events)} æ¡æœ‰æ•ˆå­—å¹•äº‹ä»¶ã€‚")
    except Exception as e:
        print(f"æ™ºèƒ½æå–æ—¶é—´äº‹ä»¶å¤±è´¥: {e}")
        return []

    # åç»­çš„éšæœºé€‰æ‹©é€»è¾‘ä¿æŒä¸å˜
    if len(subtitle_events) < num_screenshots:
        print("æœ‰æ•ˆå­—å¹•æ•°é‡ä¸è¶³ï¼Œæ— æ³•å¯åŠ¨æ™ºèƒ½é€‰æ‹©ã€‚")
        return []

    golden_start_time, golden_end_time = duration * 0.30, duration * 0.80
    golden_events = [
        e for e in subtitle_events
        if e["start"] >= golden_start_time and e["end"] <= golden_end_time
    ]
    print(
        f"   -> åœ¨è§†é¢‘ä¸­éƒ¨ ({(golden_start_time):.2f}s - {(golden_end_time):.2f}s) æ‰¾åˆ° {len(golden_events)} ä¸ªé»„é‡‘å­—å¹•äº‹ä»¶ã€‚"
    )

    target_events = golden_events
    if len(target_events) < num_screenshots:
        print("   -> é»„é‡‘å­—å¹•æ•°é‡ä¸è¶³ï¼Œå°†ä»æ‰€æœ‰å­—å¹•äº‹ä»¶ä¸­éšæœºé€‰æ‹©ã€‚")
        target_events = subtitle_events

    chosen_events = random.sample(target_events,
                                  min(num_screenshots, len(target_events)))

    screenshot_points = []
    for event in chosen_events:
        event_duration = event["end"] - event["start"]
        random_offset = event_duration * 0.1 + random.random() * (
            event_duration * 0.8)
        random_point = event["start"] + random_offset
        screenshot_points.append(random_point)
        print(
            f"   -> é€‰ä¸­æ—¶é—´æ®µ [{(event['start']):.2f}s - {(event['end']):.2f}s], éšæœºæˆªå›¾ç‚¹: {(random_point):.2f}s"
        )

    return sorted(screenshot_points)


def _find_target_video_file(path: str) -> str | None:
    """
    æ ¹æ®è·¯å¾„æ™ºèƒ½æŸ¥æ‰¾ç›®æ ‡è§†é¢‘æ–‡ä»¶ã€‚
    - å¦‚æœæ˜¯ç”µå½±ç›®å½•ï¼Œè¿”å›æœ€å¤§çš„è§†é¢‘æ–‡ä»¶ã€‚
    - å¦‚æœæ˜¯å‰§é›†ç›®å½•ï¼Œè¿”å›æŒ‰åç§°æ’åºçš„ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ã€‚

    :param path: è¦æœç´¢çš„ç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ã€‚
    :return: ç›®æ ‡è§†é¢‘æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› Noneã€‚
    """
    print(f"å¼€å§‹åœ¨è·¯å¾„ '{path}' ä¸­æŸ¥æ‰¾ç›®æ ‡è§†é¢‘æ–‡ä»¶...")
    VIDEO_EXTENSIONS = {
        ".mkv", ".mp4", ".ts", ".avi", ".wmv", ".mov", ".flv", ".m2ts"
    }

    if not os.path.exists(path):
        print(f"é”™è¯¯ï¼šæä¾›çš„è·¯å¾„ä¸å­˜åœ¨: {path}")
        return None

    # å¦‚æœæä¾›çš„è·¯å¾„æœ¬èº«å°±æ˜¯ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œç›´æ¥è¿”å›
    if os.path.isfile(path) and os.path.splitext(
            path)[1].lower() in VIDEO_EXTENSIONS:
        print(f"è·¯å¾„ç›´æ¥æŒ‡å‘ä¸€ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå°†ä½¿ç”¨: {path}")
        return path

    if not os.path.isdir(path):
        print(f"é”™è¯¯ï¼šè·¯å¾„ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç›®å½•æˆ–è§†é¢‘æ–‡ä»¶: {path}")
        return None

    video_files = []
    for root, _, files in os.walk(path):
        for file in files:
            if os.path.splitext(file)[1].lower() in VIDEO_EXTENSIONS:
                video_files.append(os.path.join(root, file))

    if not video_files:
        print(f"åœ¨ç›®å½• '{path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶ã€‚")
        return None

    # --- æ™ºèƒ½åˆ¤æ–­æ˜¯å‰§é›†è¿˜æ˜¯ç”µå½± ---
    # åŒ¹é… S01E01, s01e01, season 1 episode 1 ç­‰æ ¼å¼
    series_pattern = re.compile(
        r'[._\s-](S\d{1,2}E\d{1,3}|Season[._\s-]?\d{1,2}|E\d{1,3})[._\s-]',
        re.IGNORECASE)
    is_series = any(series_pattern.search(f) for f in video_files)

    if is_series:
        print("æ£€æµ‹åˆ°å‰§é›†å‘½åæ ¼å¼ï¼Œå°†é€‰æ‹©ç¬¬ä¸€é›†ã€‚")
        # æŒ‰æ–‡ä»¶åæ’åºï¼Œé€šå¸¸ç¬¬ä¸€é›†ä¼šåœ¨æœ€å‰é¢
        video_files.sort()
        target_file = video_files[0]
        print(f"å·²é€‰æ‹©å‰§é›†æ–‡ä»¶: {target_file}")
        return target_file
    else:
        print("æœªæ£€æµ‹åˆ°å‰§é›†æ ¼å¼ï¼Œå°†æŒ‰ç”µå½±å¤„ç†ï¼ˆé€‰æ‹©æœ€å¤§æ–‡ä»¶ï¼‰ã€‚")
        largest_file = ""
        max_size = -1
        for f in video_files:
            try:
                size = os.path.getsize(f)
                if size > max_size:
                    max_size = size
                    largest_file = f
            except OSError as e:
                print(f"æ— æ³•è·å–æ–‡ä»¶å¤§å° '{f}': {e}")
                continue

        if largest_file:
            print(f"å·²é€‰æ‹©æœ€å¤§æ–‡ä»¶ ({(max_size / 1024**3):.2f} GB): {largest_file}")
            return largest_file
        else:
            print("æ— æ³•ç¡®å®šæœ€å¤§çš„æ–‡ä»¶ã€‚")
            return None


# --- [ä¿®æ”¹] ä¸»å‡½æ•°ï¼Œæ•´åˆäº†æ–°çš„æ–‡ä»¶æŸ¥æ‰¾é€»è¾‘ ---
def upload_data_mediaInfo(mediaInfo: str,
                          save_path: str,
                          content_name: str = None,
                          downloader_id: str = None,
                          torrent_name: str = None,
                          force_refresh: bool = False):
    """
    æ£€æŸ¥ä¼ å…¥çš„æ–‡æœ¬æ˜¯æœ‰æ•ˆçš„ MediaInfo è¿˜æ˜¯ BDInfo æ ¼å¼ã€‚
    å¦‚æœæ²¡æœ‰ MediaInfo æˆ– BDInfo åˆ™å°è¯•ä» save_path æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶æå– MediaInfoã€‚
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ torrent_name (å®é™…æ–‡ä»¶å¤¹å) æˆ– content_name (è§£æåçš„æ ‡é¢˜) æ¥æ„å»ºæ›´ç²¾ç¡®çš„æœç´¢è·¯å¾„ã€‚
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ downloader_id æ¥åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä»£ç†è·å– MediaInfo
    ã€æ–°å¢ã€‘æ”¯æŒä¼ å…¥ force_refresh å¼ºåˆ¶é‡æ–°è·å– MediaInfoï¼Œå¿½ç•¥å·²æœ‰çš„æœ‰æ•ˆæ ¼å¼
    """
    print("å¼€å§‹æ£€æŸ¥ MediaInfo/BDInfo æ ¼å¼")
    print(f"æä¾›çš„ MediaInfo: {mediaInfo[:80]}...")  # æ‰“å°éƒ¨åˆ†MediaInfo

    # 1. (æ­¤éƒ¨åˆ†ä»£ç ä¸å˜) ...
    standard_mediainfo_keywords = [
        "General",
        "Video",
        "Audio",
        "Complete name",
        "File size",
        "Duration",
        "Width",
        "Height",
    ]
    bdinfo_required_keywords = ["DISC INFO", "PLAYLIST REPORT"]
    bdinfo_optional_keywords = [
        "VIDEO:",
        "AUDIO:",
        "SUBTITLES:",
        "FILES:",
        "Disc Label",
        "Disc Size",
        "BDInfo:",
        "Protection:",
        "Codec",
        "Bitrate",
        "Language",
        "Description",
    ]
    mediainfo_matches = sum(1 for keyword in standard_mediainfo_keywords
                            if keyword in mediaInfo)
    is_standard_mediainfo = mediainfo_matches >= 3
    bdinfo_required_matches = sum(1 for keyword in bdinfo_required_keywords
                                  if keyword in mediaInfo)
    bdinfo_optional_matches = sum(1 for keyword in bdinfo_optional_keywords
                                  if keyword in mediaInfo)
    is_bdinfo = (bdinfo_required_matches == len(bdinfo_required_keywords)) or \
                (bdinfo_required_matches >= 1 and bdinfo_optional_matches >= 2)

    if is_standard_mediainfo:
        if force_refresh:
            print(f"æ£€æµ‹åˆ°æ ‡å‡† MediaInfo æ ¼å¼ï¼Œä½†è®¾ç½®äº†å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡æ–°æå–ã€‚")
            # ä¸returnï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„æå–é€»è¾‘
        else:
            print(f"æ£€æµ‹åˆ°æ ‡å‡† MediaInfo æ ¼å¼ï¼ŒéªŒè¯é€šè¿‡ã€‚(åŒ¹é…å…³é”®å­—æ•°: {mediainfo_matches})")
            return mediaInfo
    elif is_bdinfo:
        if force_refresh:
            print(f"æ£€æµ‹åˆ° BDInfo æ ¼å¼ï¼Œä½†è®¾ç½®äº†å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡æ–°æå–ã€‚")
            # ä¸returnï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„æå–é€»è¾‘
        else:
            print(
                f"æ£€æµ‹åˆ° BDInfo æ ¼å¼ï¼ŒéªŒè¯é€šè¿‡ã€‚(å¿…è¦å…³é”®å­—: {bdinfo_required_matches}/{len(bdinfo_required_keywords)}, å¯é€‰å…³é”®å­—: {bdinfo_optional_matches})"
            )
            return mediaInfo
    elif not force_refresh:
        # åªæœ‰åœ¨ä¸æ˜¯å¼ºåˆ¶åˆ·æ–°æ—¶æ‰æ‰“å°è¿™ä¸ªæ¶ˆæ¯
        print("æä¾›çš„æ–‡æœ¬ä¸æ˜¯æœ‰æ•ˆçš„ MediaInfo/BDInfoï¼Œå°†å°è¯•ä»æœ¬åœ°æ–‡ä»¶æå–ã€‚")

    # å¦‚æœæ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜éœ€è¦é‡æ–°æå–ï¼ˆforce_refresh=True æˆ–è€…æ²¡æœ‰æœ‰æ•ˆæ ¼å¼ï¼‰
    if not save_path:
        print("é”™è¯¯ï¼šæœªæä¾› save_pathï¼Œæ— æ³•ä»æ–‡ä»¶æå– MediaInfoã€‚")
        return mediaInfo

    # --- ã€ä»£ç†æ£€æŸ¥å’Œå¤„ç†é€»è¾‘ã€‘ ---
    proxy_config = _get_downloader_proxy_config(downloader_id)

    if proxy_config:
        print(f"ä½¿ç”¨ä»£ç†å¤„ç† MediaInfo: {proxy_config['proxy_base_url']}")
        # æ„å»ºå®Œæ•´è·¯å¾„å‘é€ç»™ä»£ç†
        remote_path = save_path
        if torrent_name:
            remote_path = os.path.join(save_path, torrent_name)
            print(f"å·²æä¾› torrent_nameï¼Œå°†ä½¿ç”¨å®Œæ•´è·¯å¾„: '{remote_path}'")
        elif content_name:
            remote_path = os.path.join(save_path, content_name)
            print(f"å·²æä¾› content_nameï¼Œå°†ä½¿ç”¨æ‹¼æ¥è·¯å¾„: '{remote_path}'")

        try:
            response = requests.post(
                f"{proxy_config['proxy_base_url']}/api/media/mediainfo",
                json={"remote_path": remote_path},
                timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
            response.raise_for_status()
            result = response.json()
            if result.get("success"):
                print("é€šè¿‡ä»£ç†è·å– MediaInfo æˆåŠŸ")
                proxy_mediainfo = result.get("mediainfo", mediaInfo)
                # å¤„ç†ä»£ç†è¿”å›çš„ MediaInfoï¼Œåªä¿ç•™ Complete name ä¸­çš„æ–‡ä»¶å
                proxy_mediainfo = re.sub(
                    r'(Complete name\s*:\s*)(.+)', lambda m:
                    f"{m.group(1)}{os.path.basename(m.group(2).strip())}",
                    proxy_mediainfo)
                return proxy_mediainfo
            else:
                print(f"é€šè¿‡ä»£ç†è·å– MediaInfo å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        except Exception as e:
            print(f"é€šè¿‡ä»£ç†è·å– MediaInfo å¤±è´¥: {e}")

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä»¿ç…§æˆªå›¾é€»è¾‘ï¼Œæ„å»ºç²¾ç¡®çš„æœç´¢è·¯å¾„ ---
    # é¦–å…ˆåº”ç”¨è·¯å¾„æ˜ å°„è½¬æ¢
    translated_save_path = translate_path(downloader_id, save_path)
    if translated_save_path != save_path:
        print(f"è·¯å¾„æ˜ å°„: {save_path} -> {translated_save_path}")

    path_to_search = translated_save_path  # ä½¿ç”¨è½¬æ¢åçš„è·¯å¾„
    # ä¼˜å…ˆä½¿ç”¨ torrent_name (å®é™…æ–‡ä»¶å¤¹å)ï¼Œå¦‚æœä¸å­˜åœ¨å†ä½¿ç”¨ content_name (è§£æåçš„æ ‡é¢˜)
    if torrent_name:
        path_to_search = os.path.join(translated_save_path, torrent_name)
        print(f"å·²æä¾› torrent_nameï¼Œå°†åœ¨ç²¾ç¡®è·¯å¾„ä¸­æœç´¢: '{path_to_search}'")
    elif content_name:
        # å¦‚æœæä¾›äº†å…·ä½“çš„å†…å®¹åç§°ï¼ˆä¸»æ ‡é¢˜ï¼‰ï¼Œåˆ™æ‹¼æ¥æˆä¸€ä¸ªæ›´ç²¾ç¡®çš„è·¯å¾„
        path_to_search = os.path.join(translated_save_path, content_name)
        print(f"å·²æä¾› content_nameï¼Œå°†åœ¨ç²¾ç¡®è·¯å¾„ä¸­æœç´¢: '{path_to_search}'")

    # ä½¿ç”¨æ–°æ„å»ºçš„è·¯å¾„æ¥æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
    target_video_file = _find_target_video_file(path_to_search)

    if not target_video_file:
        print("æœªèƒ½åœ¨æŒ‡å®šè·¯å¾„ä¸­æ‰¾åˆ°åˆé€‚çš„è§†é¢‘æ–‡ä»¶ï¼Œæå–å¤±è´¥ã€‚")
        return mediaInfo

    try:
        print(f"å‡†å¤‡ä½¿ç”¨ MediaInfo å·¥å…·ä» '{target_video_file}' æå–...")
        media_info_parsed = MediaInfo.parse(target_video_file,
                                            output="text",
                                            full=False)
        # å¤„ç† Complete nameï¼Œåªä¿ç•™æœ€åä¸€ä¸ª / ä¹‹åçš„å†…å®¹
        media_info_str = str(media_info_parsed)
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ Complete name è¡Œä¸­çš„å®Œæ•´è·¯å¾„ä¸ºæ–‡ä»¶å
        media_info_str = re.sub(
            r'(Complete name\s*:\s*)(.+)',
            lambda m: f"{m.group(1)}{os.path.basename(m.group(2).strip())}",
            media_info_str)
        print("ä»æ–‡ä»¶é‡æ–°æå– MediaInfo æˆåŠŸã€‚")
        return media_info_str
    except Exception as e:
        print(f"ä»æ–‡ä»¶ '{target_video_file}' å¤„ç†æ—¶å‡ºé”™: {e}ã€‚å°†è¿”å›åŸå§‹ mediainfoã€‚")
        return mediaInfo


def upload_data_title(title: str, torrent_filename: str = ""):
    """
    ä»ç§å­ä¸»æ ‡é¢˜ä¸­æå–æ‰€æœ‰å‚æ•°ï¼Œå¹¶å¯é€‰åœ°ä»ç§å­æ–‡ä»¶åä¸­è¡¥å……ç¼ºå¤±å‚æ•°ã€‚
    """
    print(f"å¼€å§‹ä»ä¸»æ ‡é¢˜è§£æå‚æ•°: {title}")

    # 1. é¢„å¤„ç†
    original_title_str = title.strip()
    params = {}
    unrecognized_parts = []

    chinese_junk_match = re.search(r"([\u4e00-\u9fa5].*)$", original_title_str)
    if chinese_junk_match:
        unrecognized_parts.append(chinese_junk_match.group(1).strip())
        title = original_title_str[:chinese_junk_match.start()].strip()
    else:
        title = original_title_str

    title = re.sub(r"[ï¿¡â‚¬]", "", title)
    title = re.sub(r"\s*å‰©é¤˜æ™‚é–“.*$", "", title)
    title = re.sub(r"[\s\.]*(mkv|mp4)$", "", title,
                   flags=re.IGNORECASE).strip()
    title = re.sub(r"\[.*?\]|ã€.*?ã€‘", "", title).strip()
    title = title.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
    title = title.replace("'", "")
    title = re.sub(r"(\d+[pi])([A-Z])", r"\1 \2", title)

    # 2. ä¼˜å…ˆæå–åˆ¶ä½œç»„ä¿¡æ¯
    release_group = ""
    main_part = title

    # æ£€æŸ¥ç‰¹æ®Šåˆ¶ä½œç»„ï¼ˆå®Œæ•´åŒ¹é…ï¼‰
    special_groups = ["mUHD-FRDS", "MNHD-FRDS", "DMG&VCB-Studio", "VCB-Studio"]
    found_special_group = False
    for group in special_groups:
        if title.endswith(f" {group}") or title.endswith(f"-{group}"):
            release_group = group
            main_part = title[:-len(group) - 1].strip()
            found_special_group = True
            break

    # å¦‚æœä¸æ˜¯ç‰¹æ®Šåˆ¶ä½œç»„ï¼Œå…ˆå°è¯•åŒ¹é… VCB-Studio å˜ä½“
    if not found_special_group:
        # åŒ¹é…ç±»ä¼¼ -Nekomoe kissaten&VCB-Studio, -LoliHouse&VCB-Studio ç­‰æ ¼å¼
        # è¿™ä¸ªæ­£åˆ™ä¼šåŒ¹é… - å¼€å¤´ï¼Œä¸­é—´å¯èƒ½æœ‰å¤šä¸ªå•è¯ï¼ˆåŒ…å«ç©ºæ ¼ï¼‰ã€&ç¬¦å·ï¼Œæœ€åä»¥ VCB-Studio ç»“å°¾
        vcb_variant_pattern = re.compile(
            r"^(?P<main_part>.+?)[-](?P<release_group>[\w\s]+&VCB-Studio)$",
            re.IGNORECASE)
        vcb_match = vcb_variant_pattern.match(title)
        if vcb_match:
            main_part = vcb_match.group("main_part").strip()
            release_group = vcb_match.group("release_group")
            found_special_group = True
            print(f"æ£€æµ‹åˆ° VCB-Studio å˜ä½“åˆ¶ä½œç»„: {release_group}")

    # å¦‚æœè¿˜ä¸æ˜¯ç‰¹æ®Šåˆ¶ä½œç»„ï¼Œä½¿ç”¨é€šç”¨æ¨¡å¼åŒ¹é…
    if not found_special_group:
        # æ”¯æŒ - å’Œ @ ä¸¤ç§å‰ç¼€
        general_regex = re.compile(
            r"^(?P<main_part>.+?)(?:[-@](?P<internal_tag>[A-Za-z0-9@Â²Â³â´âµâ¶â·â¸â¹]+))?[-@](?P<release_group>[A-Za-z0-9@Â²Â³â´âµâ¶â·â¸â¹]+)$",
            re.VERBOSE | re.IGNORECASE,
        )
        match = general_regex.match(title)
        if match:
            main_part = match.group("main_part").strip()
            release_group_name = match.group("release_group")
            internal_tag = match.group("internal_tag")
            # ä¿®å¤ï¼šä¿æŒåŸå§‹æ ¼å¼ï¼Œä½¿ç”¨@è¿æ¥è€Œä¸æ˜¯æ‹¬å·æ ¼å¼
            if internal_tag:
                # å¦‚æœinternal_tagä¸­å·²ç»åŒ…å«@ï¼Œè¯´æ˜è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç»„åç‰‡æ®µ
                if "@" in internal_tag:
                    release_group = f"{internal_tag}-{release_group_name}"
                else:
                    # ä½¿ç”¨@è¿æ¥ï¼Œä¿æŒ DIY@Audies çš„æ ¼å¼
                    release_group = f"{internal_tag}@{release_group_name}"
            else:
                release_group = release_group_name
        else:
            # æ£€æŸ¥æ˜¯å¦ä»¥-NOGROUPç»“å°¾
            if title.upper().endswith("-NOGROUP"):
                release_group = "NOGROUP"
                main_part = title[:-8].strip()
            else:
                release_group = "N/A (æ— å‘å¸ƒç»„)"

    # 3. å­£é›†ã€å¹´ä»½æå–
    season_match = re.search(
        r"(?<!\w)(S\d{1,2}(?:(?:[-â€“~]\s*S?\d{1,2})?|(?:\s*E\d{1,3}(?:[-â€“~]\s*(?:S\d{1,2})?E?\d{1,3})*)?))(?!\w)",
        main_part,
        re.I,
    )
    if season_match:
        season_str = season_match.group(1)
        main_part = main_part.replace(season_str, " ").strip()
        params["season_episode"] = re.sub(r"\s", "", season_str.upper())

    title_part = main_part
    year_match = re.search(r"[\s\.\(]((?:19|20)\d{2})([\s\.\)]|$)", title_part)
    if year_match:
        params["year"] = year_match.group(1)
        title_part = title_part.replace(year_match.group(0), " ", 1).strip()

    # 4. æŠ€æœ¯æ ‡ç­¾æå–ï¼ˆæ’é™¤å·²è¯†åˆ«çš„åˆ¶ä½œç»„åç§°ï¼‰
    tech_patterns_definitions = {
        "medium":
        r"UHDTV|UHD\s*Blu-?ray|Blu-?ray\s+DIY|Blu-ray|BluRay\s+DIY|BluRay|WEB-DL|WEBrip|TVrip|DVDRip|HDTV",
        "audio":
        r"DTS-HD(?:\s*MA)?(?:\s*\d\.\d)?|(?:Dolby\s*)?TrueHD(?:\s*Atmos)?(?:\s*\d\.\d)?|Atmos(?:\s*TrueHD)?(?:\s*\d\.\d)?|DTS(?:\s*\d\.\d)?|DDP(?:\s*\d\.\d)?|DD\+(?:\s*\d\.\d)?|DD(?:\s*\d\.\d)?|AC3(?:\s*\d\.\d)?|FLAC(?:\s*\d\.\d)?|AAC(?:\s*\d\.\d)?|LPCM(?:\s*\d\.\d)?|AV3A\s*\d\.\d|\d+\s*Audios?|MP2|DUAL",
        "hdr_format":
        r"Dolby Vision|DoVi|HDR10\+|HDRVivid|HDR10|HLG|HDR|SDR|DV|Vivid",
        "resolution": r"\d{3,4}[pi]|4K",
        "video_codec":
        r"HEVC|AVC|x265|H\s*\.?\s*265|x264|H\s*\.?\s*264|VC-1|AV1|MPEG-2",
        "source_platform":
        r"Apple TV\+|ViuTV|MyTVSuper|AMZN|Netflix|NF|DSNP|MAX|ATVP|iTunes|friDay|USA|EUR|JPN|CEE|FRA|LINETV|EDR|PCOK|Hami|GBR|NowPlayer|CR|SEEZN|GER|CHN|MA|Viu|Baha|KKTV|IQ|HKG|ITA|ESP",
        "bit_depth": r"\b(?:8|10)bit\b",
        "framerate": r"\d{2,3}fps",
        "completion_status": r"Complete|COMPLETE",
        "video_format": r"3D|HSBS",
        "release_version": r"REMASTERED|REPACK|RERIP|PROPER|REPOST",
        "quality_modifier": r"MAXPLUS|HQ|EXTENDED|REMUX|UNRATED|EE|MiniBD",
    }
    priority_order = [
        "completion_status",
        "release_version",
        "medium",
        "resolution",
        "video_codec",
        "bit_depth",
        "hdr_format",
        "video_format",
        "framerate",
        "source_platform",
        "audio",
        "quality_modifier",
    ]

    title_candidate = title_part
    first_tech_tag_pos = len(title_candidate)
    all_found_tags = []

    # æ„å»ºåˆ¶ä½œç»„çš„å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºåç»­è¿‡æ»¤
    release_group_keywords = []
    if release_group and release_group != "N/A (æ— å‘å¸ƒç»„)":
        # å°†åˆ¶ä½œç»„åç§°æŒ‰@å’Œå…¶ä»–åˆ†éš”ç¬¦æ‹†åˆ†ï¼Œè·å–æ‰€æœ‰ç»„æˆéƒ¨åˆ†
        # ä¾‹å¦‚ "DIY@Audies" -> ["DIY", "Audies"]
        release_group_keywords = re.split(r'[@\-\s]+', release_group)
        release_group_keywords = [
            kw.strip() for kw in release_group_keywords if kw.strip()
        ]

    for key in priority_order:
        pattern = tech_patterns_definitions[key]
        search_pattern = (re.compile(r"(?<!\w)(" + pattern + r")(?!\w)",
                                     re.IGNORECASE) if r"\b" not in pattern
                          else re.compile(pattern, re.IGNORECASE))
        matches = list(search_pattern.finditer(title_candidate))
        if not matches:
            continue

        first_tech_tag_pos = min(first_tech_tag_pos, matches[0].start())
        raw_values = [
            m.group(0).strip() if r"\b" in pattern else m.group(1).strip()
            for m in matches
        ]

        # è¿‡æ»¤æ‰å±äºåˆ¶ä½œç»„åç§°çš„éƒ¨åˆ†
        filtered_values = []
        for val in raw_values:
            # æ£€æŸ¥è¿™ä¸ªå€¼æ˜¯å¦æ˜¯åˆ¶ä½œç»„å…³é”®è¯ä¹‹ä¸€
            is_release_group_part = any(val.upper() == kw.upper()
                                        for kw in release_group_keywords)
            if not is_release_group_part:
                filtered_values.append(val)

        all_found_tags.extend(filtered_values)
        raw_values = filtered_values
        processed_values = (
            [re.sub(r"(DD)\+", r"\1+", val, flags=re.I)
             for val in raw_values] if key == "audio" else raw_values)
        if key == "audio":
            processed_values = [
                re.sub(r"((?:FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))(\d(?:\.\d)?)",
                       r"\1 \2",
                       val,
                       flags=re.I) for val in processed_values
            ]

        unique_processed = sorted(
            list(set(processed_values)),
            key=lambda x: title_candidate.find(x.replace(" ", "")))
        if unique_processed:
            params[key] = unique_processed[0] if len(
                unique_processed) == 1 else unique_processed

    # --- [æ–°å¢] å¼€å§‹: ä»ç§å­æ–‡ä»¶åè¡¥å……ç¼ºå¤±çš„å‚æ•° ---
    if torrent_filename:
        print(f"å¼€å§‹ä»ç§å­æ–‡ä»¶åè¡¥å……å‚æ•°: {torrent_filename}")
        # é¢„å¤„ç†æ–‡ä»¶åï¼šç§»é™¤åç¼€ï¼Œç”¨ç©ºæ ¼æ›¿æ¢ç‚¹å’Œå…¶ä»–å¸¸ç”¨åˆ†éš”ç¬¦
        filename_base = re.sub(r'(\.original)?\.torrent',
                               '',
                               torrent_filename,
                               flags=re.IGNORECASE)
        filename_candidate = re.sub(r'[\._\[\]\(\)]', ' ', filename_base)

        # å†æ¬¡éå†æ‰€æœ‰æŠ€æœ¯æ ‡ç­¾å®šä¹‰ï¼Œä»¥è¡¥å……ä¿¡æ¯
        for key in priority_order:
            # å¦‚æœä¸»æ ‡é¢˜ä¸­å·²è§£æå‡ºæ­¤å‚æ•°ï¼Œåˆ™è·³è¿‡ï¼Œä¼˜å…ˆä½¿ç”¨ä¸»æ ‡é¢˜çš„ç»“æœ
            if key in params and params.get(key):
                continue

            pattern = tech_patterns_definitions[key]
            search_pattern = (re.compile(r"(?<!\w)(" + pattern + r")(?!\w)",
                                         re.IGNORECASE) if r"\b" not in pattern
                              else re.compile(pattern, re.IGNORECASE))

            matches = list(search_pattern.finditer(filename_candidate))
            if matches:
                # æå–æ‰€æœ‰åŒ¹é…åˆ°çš„å€¼
                raw_values = [
                    m.group(0).strip()
                    if r"\b" in pattern else m.group(1).strip()
                    for m in matches
                ]

                # (å¤åˆ¶ä¸»è§£æé€»è¾‘ä¸­çš„ audio ç‰¹æ®Šå¤„ç†)
                processed_values = ([
                    re.sub(r"(DD)\\+", r"\1+", val, flags=re.I)
                    for val in raw_values
                ] if key == "audio" else raw_values)
                if key == "audio":
                    processed_values = [
                        re.sub(
                            r"((?:FLAC|DDP|AV3A|AAC|LPCM|AC3|DD))(\d(?:\.\d)?)",
                            r"\1 \2",
                            val,
                            flags=re.I) for val in processed_values
                    ]

                # å–ç‹¬ä¸€æ— äºŒçš„å€¼å¹¶æŒ‰å‡ºç°é¡ºåºæ’åº
                unique_processed = sorted(
                    list(set(processed_values)),
                    key=lambda x: filename_candidate.find(x.replace(" ", "")))

                if unique_processed:
                    print(f"   [æ–‡ä»¶åè¡¥å……] æ‰¾åˆ°ç¼ºå¤±å‚æ•° '{key}': {unique_processed}")
                    # å°†è¡¥å……çš„å‚æ•°å­˜å…¥ params å­—å…¸
                    params[key] = unique_processed[0] if len(
                        unique_processed) == 1 else unique_processed
                    # å°†æ–°æ‰¾åˆ°çš„æ ‡ç­¾ä¹ŸåŠ å…¥ all_found_tagsï¼Œä»¥ä¾¿åç»­æ­£ç¡®è®¡ç®—"æ— æ³•è¯†åˆ«"éƒ¨åˆ†
                    all_found_tags.extend(unique_processed)
    # --- [æ–°å¢] ç»“æŸ ---

    # å°†åˆ¶ä½œç»„ä¿¡æ¯æ·»åŠ åˆ°æœ€åçš„å‚æ•°ä¸­
    params["release_info"] = release_group

    if "quality_modifier" in params:
        modifiers = params.pop("quality_modifier")
        if not isinstance(modifiers, list):
            modifiers = [modifiers]
        if "medium" in params:
            medium_str = (params["medium"] if isinstance(
                params["medium"], str) else params["medium"][0])
            params["medium"] = f"{medium_str} {' '.join(sorted(modifiers))}"

    # 5. æœ€ç»ˆæ ‡é¢˜å’Œæœªè¯†åˆ«å†…å®¹ç¡®å®š
    title_zone = title_part[:first_tech_tag_pos].strip()
    tech_zone = title_part[first_tech_tag_pos:].strip()
    params["title"] = re.sub(r"[\s\.]+", " ", title_zone).strip()

    cleaned_tech_zone = tech_zone
    for tag in sorted(all_found_tags, key=len, reverse=True):
        pattern_to_remove = r"\b" + re.escape(tag) + r"(?!\w)"
        cleaned_tech_zone = re.sub(pattern_to_remove,
                                   " ",
                                   cleaned_tech_zone,
                                   flags=re.IGNORECASE)

    remains = re.split(r"[\s\.]+", cleaned_tech_zone)
    unrecognized_parts.extend([part for part in remains if part])
    if unrecognized_parts:
        params["unrecognized"] = " ".join(sorted(list(
            set(unrecognized_parts))))

    english_params = {}
    key_order = [
        "title",
        "year",
        "season_episode",
        "completion_status",
        "release_version",
        "resolution",
        "medium",
        "source_platform",
        "video_codec",
        "video_format",
        "hdr_format",
        "bit_depth",
        "framerate",
        "audio",
        "release_info",
        "unrecognized",
    ]
    for key in key_order:
        if key in params and params[key]:
            if key == "audio" and isinstance(params[key], list):
                sorted_audio = sorted(params[key],
                                      key=lambda s:
                                      (s.upper().endswith("AUDIOS"), -len(s)))
                english_params[key] = " ".join(sorted_audio)
            else:
                english_params[key] = params[key]

    if "source_platform" in english_params and "audio" in english_params:
        is_sp_list = isinstance(english_params["source_platform"], list)
        sp_values = (english_params["source_platform"]
                     if is_sp_list else [english_params["source_platform"]])
        if "MA" in sp_values and "MA" in str(english_params["audio"]):
            sp_values.remove("MA")
            if not sp_values:
                del english_params["source_platform"]
            elif len(sp_values) == 1 and not is_sp_list:
                english_params["source_platform"] = sp_values[0]
            elif is_sp_list:
                english_params["source_platform"] = sp_values

    # 6. æœ‰æ•ˆæ€§è´¨æ£€
    is_valid = bool(english_params.get("title"))
    if is_valid:
        if not any(
                key in english_params
                for key in ["resolution", "medium", "video_codec", "audio"]):
            is_valid = False
        release_info = english_params.get("release_info", "")
        if "N/A" in release_info and "NOGROUP" not in release_info:
            core_tech_keys = ["resolution", "medium", "video_codec"]
            if sum(1 for key in core_tech_keys if key in english_params) < 2:
                is_valid = False

    if not is_valid:
        print("ä¸»æ ‡é¢˜è§£æå¤±è´¥æˆ–æœªé€šè¿‡è´¨æ£€ã€‚")
        english_params = {"title": original_title_str, "unrecognized": "è§£æå¤±è´¥"}

    translation_map = {
        "title": "ä¸»æ ‡é¢˜",
        "year": "å¹´ä»½",
        "season_episode": "å­£é›†",
        "resolution": "åˆ†è¾¨ç‡",
        "medium": "åª’ä»‹",
        "source_platform": "ç‰‡æºå¹³å°",
        "video_codec": "è§†é¢‘ç¼–ç ",
        "hdr_format": "HDRæ ¼å¼",
        "bit_depth": "è‰²æ·±",
        "framerate": "å¸§ç‡",
        "audio": "éŸ³é¢‘ç¼–ç ",
        "release_info": "åˆ¶ä½œç»„",
        "completion_status": "å‰§é›†çŠ¶æ€",
        "unrecognized": "æ— æ³•è¯†åˆ«",
        "video_format": "è§†é¢‘æ ¼å¼",
        "release_version": "å‘å¸ƒç‰ˆæœ¬",
    }

    chinese_keyed_params = {}
    for key, value in english_params.items():
        chinese_key = translation_map.get(key)
        if chinese_key:
            chinese_keyed_params[chinese_key] = value

    # å®šä¹‰å‰ç«¯æ˜¾ç¤ºçš„å®Œæ•´å‚æ•°åˆ—è¡¨å’Œå›ºå®šé¡ºåº
    all_possible_keys_ordered = [
        "ä¸»æ ‡é¢˜",
        "å¹´ä»½",
        "å­£é›†",
        "å‰§é›†çŠ¶æ€",
        "å‘å¸ƒç‰ˆæœ¬",
        "åˆ†è¾¨ç‡",
        "åª’ä»‹",
        "ç‰‡æºå¹³å°",
        "è§†é¢‘ç¼–ç ",
        "è§†é¢‘æ ¼å¼",
        "HDRæ ¼å¼",
        "è‰²æ·±",
        "å¸§ç‡",
        "éŸ³é¢‘ç¼–ç ",
        "åˆ¶ä½œç»„",
        "æ— æ³•è¯†åˆ«",
    ]

    final_components_list = []
    for key in all_possible_keys_ordered:
        final_components_list.append({
            "key": key,
            "value": chinese_keyed_params.get(key, "")
        })

    print(f"ä¸»æ ‡é¢˜è§£ææˆåŠŸã€‚")
    return final_components_list


def upload_data_screenshot(source_info,
                           save_path,
                           torrent_name=None,
                           downloader_id=None):
    """
    [æœ€ç»ˆHDRä¼˜åŒ–ç‰ˆ] ä½¿ç”¨ mpv ä»è§†é¢‘æ–‡ä»¶ä¸­æˆªå–å¤šå¼ å›¾ç‰‡ï¼Œå¹¶ä¸Šä¼ åˆ°å›¾åºŠã€‚
    - æ–°å¢HDRè‰²è°ƒæ˜ å°„å‚æ•°ï¼Œç¡®ä¿HDRè§†é¢‘æˆªå›¾é¢œè‰²æ­£å¸¸ã€‚
    - æŒ‰é¡ºåºä¸€å¼ ä¸€å¼ å¤„ç†ï¼Œç®€åŒ–æµç¨‹ã€‚
    - é‡‡ç”¨æ™ºèƒ½æ—¶é—´ç‚¹åˆ†æã€‚
    """
    if Image is None:
        print("é”™è¯¯ï¼šPillow åº“æœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡Œæˆªå›¾ä»»åŠ¡ã€‚")
        return ""

    print("å¼€å§‹æ‰§è¡Œæˆªå›¾å’Œä¸Šä¼ ä»»åŠ¡ (å¼•æ“: mpv, è¾“å‡ºæ ¼å¼: JPEG, æ¨¡å¼: é¡ºåºæ‰§è¡Œ)...")
    config = config_manager.get()
    hoster = config.get("cross_seed", {}).get("image_hoster", "pixhost")
    num_screenshots = 5
    print(f"å·²é€‰æ‹©å›¾åºŠæœåŠ¡: {hoster}, æˆªå›¾æ•°é‡: {num_screenshots}")

    # é¦–å…ˆåº”ç”¨è·¯å¾„æ˜ å°„è½¬æ¢
    translated_save_path = translate_path(downloader_id, save_path)
    if translated_save_path != save_path:
        print(f"è·¯å¾„æ˜ å°„: {save_path} -> {translated_save_path}")

    if torrent_name:
        full_video_path = os.path.join(translated_save_path, torrent_name)
        print(f"ä½¿ç”¨å®Œæ•´è§†é¢‘è·¯å¾„: {full_video_path}")
    else:
        full_video_path = translated_save_path
        print(f"ä½¿ç”¨åŸå§‹è·¯å¾„: {full_video_path}")

    # --- ä»£ç†æ£€æŸ¥å’Œå¤„ç†é€»è¾‘ (æ­¤éƒ¨åˆ†ä¿æŒä¸å˜) ---
    use_proxy = False
    proxy_config = None
    if downloader_id:
        downloaders = config.get("downloaders", [])
        for downloader in downloaders:
            if downloader.get("id") == downloader_id:
                use_proxy = downloader.get("use_proxy", False)
                if use_proxy:
                    host_value = downloader.get('host', '')
                    proxy_port = downloader.get('proxy_port', 9090)
                    if host_value.startswith(('http://', 'https://')):
                        parsed_url = urlparse(host_value)
                    else:
                        parsed_url = urlparse(f"http://{host_value}")
                    proxy_ip = parsed_url.hostname
                    if not proxy_ip:
                        if '://' in host_value:
                            proxy_ip = host_value.split('://')[1].split(
                                ':')[0].split('/')[0]
                        else:
                            proxy_ip = host_value.split(':')[0]
                    proxy_config = {
                        "proxy_base_url": f"http://{proxy_ip}:{proxy_port}",
                    }
                break

    if use_proxy and proxy_config:
        print(f"ä½¿ç”¨ä»£ç†å¤„ç†æˆªå›¾: {proxy_config['proxy_base_url']}")
        try:
            response = requests.post(
                f"{proxy_config['proxy_base_url']}/api/media/screenshot",
                json={"remote_path": full_video_path},
                timeout=300)
            response.raise_for_status()
            result = response.json()
            if result.get("success"):
                print("ä»£ç†æˆªå›¾ä¸Šä¼ æˆåŠŸ")
                return result.get("bbcode", "")
            else:
                print(f"ä»£ç†æˆªå›¾ä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return ""
        except Exception as e:
            print(f"é€šè¿‡ä»£ç†è·å–æˆªå›¾å¤±è´¥: {e}")
            return ""

    # --- æœ¬åœ°æˆªå›¾é€»è¾‘ ---
    target_video_file = _find_target_video_file(full_video_path)
    if not target_video_file:
        print("é”™è¯¯ï¼šåœ¨æŒ‡å®šè·¯å¾„ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ã€‚")
        return ""

    if not shutil.which("mpv"):
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ° mpvã€‚è¯·ç¡®ä¿å®ƒå·²å®‰è£…å¹¶å·²æ·»åŠ åˆ°ç³»ç»Ÿç¯å¢ƒå˜é‡ PATH ä¸­ã€‚")
        return ""

    screenshot_points = _get_smart_screenshot_points(target_video_file,
                                                     num_screenshots)
    if len(screenshot_points) < num_screenshots:
        print("è­¦å‘Š: æ™ºèƒ½åˆ†æå¤±è´¥æˆ–å­—å¹•ä¸è¶³ï¼Œå›é€€åˆ°æŒ‰ç™¾åˆ†æ¯”æˆªå›¾ã€‚")
        try:
            cmd_duration = [
                "ffprobe", "-v", "error", "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1", target_video_file
            ]
            result = subprocess.run(cmd_duration,
                                    capture_output=True,
                                    text=True,
                                    check=True,
                                    encoding='utf-8')
            duration = float(result.stdout.strip())
            screenshot_points = [
                duration * p for p in [0.15, 0.30, 0.50, 0.70, 0.85]
            ]
        except Exception as e:
            print(f"é”™è¯¯: è¿è·å–è§†é¢‘æ—¶é•¿éƒ½å¤±è´¥äº†ï¼Œæ— æ³•æˆªå›¾ã€‚{e}")
            return ""

    auth_token = _get_agsv_auth_token() if hoster == "agsv" else None
    if hoster == "agsv" and not auth_token:
        print("âŒ æ— æ³•è·å– æœ«æ—¥å›¾åºŠ Tokenï¼Œæˆªå›¾ä¸Šä¼ ä»»åŠ¡ç»ˆæ­¢ã€‚")
        return ""

    uploaded_urls = []
    temp_files_to_cleanup = []

    for i, screenshot_time in enumerate(screenshot_points):
        print(f"\n--- å¼€å§‹å¤„ç†ç¬¬ {i+1}/{len(screenshot_points)} å¼ æˆªå›¾ ---")

        safe_name = re.sub(r'[\\/*?:"<>|\'\s\.]+', '_',
                           source_info.get('main_title', f'screenshot_{i+1}'))
        timestamp = f"{time.time():.0f}"
        intermediate_png_path = os.path.join(
            TEMP_DIR, f"{safe_name}_{i+1}_{timestamp}_temp.png")
        final_jpeg_path = os.path.join(TEMP_DIR,
                                       f"{safe_name}_{i+1}_{timestamp}.jpg")
        temp_files_to_cleanup.extend([intermediate_png_path, final_jpeg_path])

        # --- [æ ¸å¿ƒä¿®æ”¹] ---
        # ä¸º mpv å‘½ä»¤æ·»åŠ  HDR è‰²è°ƒæ˜ å°„å‚æ•°
        cmd_screenshot = [
            "mpv",
            "--no-audio",
            f"--start={screenshot_time:.2f}",
            "--frames=1",

            # --- HDR è‰²è°ƒæ˜ å°„å‚æ•° ---
            # æŒ‡å®šè¾“å‡ºä¸ºæ ‡å‡†çš„sRGBè‰²å½©ç©ºé—´ï¼Œè¿™æ˜¯æ‰€æœ‰SDRå›¾ç‰‡çš„åŸºç¡€
            "--target-trc=srgb",
            # ä½¿ç”¨ 'hable' ç®—æ³•è¿›è¡Œè‰²è°ƒæ˜ å°„ï¼Œå®ƒèƒ½åœ¨ä¿ç•™é«˜å…‰å’Œé˜´å½±ç»†èŠ‚æ–¹é¢å–å¾—è‰¯å¥½å¹³è¡¡
            "--tone-mapping=hable",
            # å¦‚æœè‰²å½©ä¾ç„¶ä¸å‡†ï¼Œå¯ä»¥å°è¯•æ›´ç°ä»£çš„ 'bt.2390' ç®—æ³•
            # "--tone-mapping=bt.2390",
            f"--o={intermediate_png_path}",
            target_video_file
        ]
        # --- [æ ¸å¿ƒä¿®æ”¹ç»“æŸ] ---

        try:
            subprocess.run(cmd_screenshot,
                           check=True,
                           capture_output=True,
                           timeout=120)

            if not os.path.exists(intermediate_png_path):
                print(f"âŒ é”™è¯¯: mpv å‘½ä»¤æ‰§è¡ŒæˆåŠŸï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶ {intermediate_png_path}")
                continue

            print(
                f"   -> ä¸­é—´PNGå›¾ {os.path.basename(intermediate_png_path)} ç”ŸæˆæˆåŠŸã€‚"
            )

            try:
                with Image.open(intermediate_png_path) as img:
                    rgb_img = img.convert('RGB')
                    rgb_img.save(final_jpeg_path, 'jpeg', quality=85)
                print(
                    f"   -> JPEGå‹ç¼©æˆåŠŸ (è´¨é‡: 85) -> {os.path.basename(final_jpeg_path)}"
                )
            except Exception as e:
                print(f"   âŒ é”™è¯¯: å›¾ç‰‡ä»PNGè½¬æ¢ä¸ºJPEGå¤±è´¥: {e}")
                continue

            max_retries = 3
            image_url = None
            for attempt in range(max_retries):
                print(f"   -> æ­£åœ¨ä¸Šä¼  (ç¬¬ {attempt+1}/{max_retries} æ¬¡å°è¯•)...")
                try:
                    if hoster == "agsv":
                        image_url = _upload_to_agsv(final_jpeg_path,
                                                    auth_token)
                    else:
                        image_url = _upload_to_pixhost(final_jpeg_path)
                    if image_url:
                        uploaded_urls.append(image_url)
                        break
                    else:
                        time.sleep(2)
                except Exception as e:
                    print(f"   -> ä¸Šä¼ å°è¯• {attempt+1} å‡ºç°å¼‚å¸¸: {e}")
                    time.sleep(2)

            if not image_url:
                print(f"âš ï¸  ç¬¬ {i+1} å¼ å›¾ç‰‡ç»è¿‡ {max_retries} æ¬¡å°è¯•åä»ç„¶ä¸Šä¼ å¤±è´¥ã€‚")

        except subprocess.CalledProcessError as e:
            error_output = e.stderr.decode('utf-8', errors='ignore')
            print(f"âŒ é”™è¯¯: mpv æˆªå›¾å¤±è´¥ã€‚")
            print(f"   -> Stderr: {error_output}")
            continue
        except subprocess.TimeoutExpired:
            print(f"âŒ é”™è¯¯: mpv æˆªå›¾è¶…æ—¶ (è¶…è¿‡60ç§’)ã€‚")
            continue

    print("\n--- æ‰€æœ‰æˆªå›¾å¤„ç†å®Œæ¯• ---")
    print(f"æ­£åœ¨æ¸…ç†ä¸´æ—¶ç›®å½•ä¸­çš„ {len(temp_files_to_cleanup)} ä¸ªæˆªå›¾æ–‡ä»¶...")
    for item_path in temp_files_to_cleanup:
        try:
            if os.path.exists(item_path):
                os.remove(item_path)
        except OSError as e:
            print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {item_path} å¤±è´¥: {e}")

    if not uploaded_urls:
        print("ä»»åŠ¡å®Œæˆï¼Œä½†æ²¡æœ‰æˆåŠŸä¸Šä¼ ä»»ä½•å›¾ç‰‡ã€‚")
        return ""

    bbcode_links = []
    for url in sorted(uploaded_urls):
        if "pixhost.to/show/" in url:
            bbcode_links.append(
                f"[img]{url.replace('https://pixhost.to/show/', 'https://img1.pixhost.to/images/')}[/img]"
            )
        else:
            bbcode_links.append(f"[img]{url}[/img]")

    screenshots = "\n".join(bbcode_links)
    print("æ‰€æœ‰æˆªå›¾å·²æˆåŠŸä¸Šä¼ å¹¶å·²æ ¼å¼åŒ–ä¸ºBBCodeã€‚")
    return screenshots


def upload_data_poster(douban_link: str, imdb_link: str):
    """
    é€šè¿‡PT-Gen APIè·å–ç”µå½±ä¿¡æ¯çš„æµ·æŠ¥å’ŒIMDbé“¾æ¥ã€‚
    æ”¯æŒä»è±†ç“£é“¾æ¥æˆ–IMDbé“¾æ¥è·å–ä¿¡æ¯ã€‚
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨upload_data_movie_infoæ›¿ä»£ã€‚
    """
    # è°ƒç”¨æ–°çš„ç»Ÿä¸€å‡½æ•°è·å–æ‰€æœ‰ä¿¡æ¯
    status, poster, description, imdb_link_result = upload_data_movie_info(
        douban_link, imdb_link)

    if status:
        return True, poster, imdb_link_result
    else:
        return False, description, imdb_link_result


def upload_data_movie_info(douban_link: str, imdb_link: str):
    """
    é€šè¿‡å¤šä¸ªPT-Gen APIè·å–ç”µå½±ä¿¡æ¯çš„å®Œæ•´å†…å®¹ï¼ŒåŒ…æ‹¬æµ·æŠ¥ã€ç®€ä»‹å’ŒIMDbé“¾æ¥ã€‚
    æ”¯æŒä»è±†ç“£é“¾æ¥æˆ–IMDbé“¾æ¥è·å–ä¿¡æ¯ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢APIã€‚
    è¿”å›: (çŠ¶æ€, æµ·æŠ¥, ç®€ä»‹, IMDbé“¾æ¥)
    """
    # APIé…ç½®åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    api_configs = [
        {
            'name': 'ptn-ptgen.sqing33.dpdns.org',
            'base_url': 'https://ptn-ptgen.sqing33.dpdns.org',
            'type': 'url_format'
        },
        {
            'name': 'ptgen.tju.pt',
            'base_url': 'https://ptgen.tju.pt/infogen',
            'type': 'tju_format',
            'force_douban': True  # å¼ºåˆ¶ä½¿ç”¨site=doubanæ¨¡å¼
        },
        {
            'name': 'ptgen.homeqian.top',
            'base_url': 'https://ptgen.homeqian.top',
            'type': 'url_format'
        },
        {
            'name': 'api.iyuu.cn',
            'base_url': 'https://api.iyuu.cn/App.Movie.Ptgen',
            'type': 'iyuu_format'
        }
    ]

    # ç¡®å®šè¦ä½¿ç”¨çš„èµ„æºURLï¼ˆè±†ç“£ä¼˜å…ˆï¼‰
    if not douban_link and not imdb_link:
        return False, "", "", "æœªæä¾›è±†ç“£æˆ–IMDbé“¾æ¥ã€‚"

    # å°è¯•æ¯ä¸ªAPI
    last_error = ""
    for api_config in api_configs:
        try:
            print(f"å°è¯•ä½¿ç”¨API: {api_config['name']}")

            if api_config['type'] == 'tju_format':
                # TJUæ ¼å¼API (ptgen.tju.pt) - å¼ºåˆ¶ä½¿ç”¨è±†ç“£æ¨¡å¼
                success, poster, description, imdb_link_result = _call_tju_format_api(
                    api_config, douban_link, imdb_link)
            elif api_config['type'] == 'url_format':
                # URLæ ¼å¼API (workers.dev, homeqian.top)
                success, poster, description, imdb_link_result = _call_url_format_api(
                    api_config, douban_link, imdb_link)
            elif api_config['type'] == 'iyuu_format':
                # IYUUæ ¼å¼API (api.iyuu.cn)
                success, poster, description, imdb_link_result = _call_iyuu_format_api(
                    api_config, douban_link, imdb_link)
            else:
                continue

            if success:
                print(f"API {api_config['name']} è°ƒç”¨æˆåŠŸ")
                return True, poster, description, imdb_link_result
            else:
                last_error = description  # é”™è¯¯ä¿¡æ¯å­˜å‚¨åœ¨descriptionä¸­
                print(f"API {api_config['name']} è¿”å›å¤±è´¥: {last_error}")

        except Exception as e:
            last_error = f"API {api_config['name']} è¯·æ±‚å¼‚å¸¸: {e}"
            print(last_error)
            continue

    # æ‰€æœ‰APIéƒ½å¤±è´¥
    return False, "", "", f"æ‰€æœ‰PT-Gen APIéƒ½å¤±è´¥ã€‚æœ€åé”™è¯¯: {last_error}"


def _call_tju_format_api(api_config: dict, douban_link: str, imdb_link: str):
    """
    è°ƒç”¨TJUæ ¼å¼API (ptgen.tju.pt) - å¼ºåˆ¶ä½¿ç”¨site=doubanæ¨¡å¼
    """
    try:
        # å¼ºåˆ¶ä½¿ç”¨site=doubanï¼Œè¿™æ ·IMDbé“¾æ¥ä¹Ÿä¼šè¢«è½¬æ¢æŸ¥è¯¢è±†ç“£
        if douban_link:
            # ä»è±†ç“£é“¾æ¥æå–ID
            douban_id = _extract_douban_id(douban_link)
            if douban_id:
                url = f"{api_config['base_url']}?site=douban&sid={douban_id}"
            else:
                raise ValueError("æ— æ³•ä»è±†ç“£é“¾æ¥æå–ID")
        elif imdb_link:
            # ä»IMDbé“¾æ¥æå–IDï¼Œä½†å¼ºåˆ¶ä½¿ç”¨doubanæ¨¡å¼
            imdb_id = _extract_imdb_id(imdb_link)
            if imdb_id:
                url = f"{api_config['base_url']}?site=douban&sid={imdb_id}"
            else:
                raise ValueError("æ— æ³•ä»IMDbé“¾æ¥æå–ID")
        else:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„é“¾æ¥")

        response = requests.get(url, timeout=30)
        response.raise_for_status()

        data = response.json()

        if not data.get('success', False):
            error_msg = data.get('error', 'æœªçŸ¥é”™è¯¯')
            return False, "", f"APIè¿”å›å¤±è´¥: {error_msg}", ""

        format_data = data.get('format', '')
        if not format_data:
            return False, "", "APIæœªè¿”å›æœ‰æ•ˆçš„æ ¼å¼åŒ–å†…å®¹", ""

        # æå–ä¿¡æ¯
        extracted_imdb_link = data.get('imdb_link', '')
        poster = ""
        description = ""

        # æå–æµ·æŠ¥å›¾ç‰‡
        img_match = re.search(r'(\[img\].*?\[/img\])', format_data)
        if img_match:
            poster = re.sub(r'img1', 'img9', img_match.group(1))

        # æå–ç®€ä»‹å†…å®¹ï¼ˆå»é™¤æµ·æŠ¥éƒ¨åˆ†ï¼‰
        description = re.sub(r'\[img\].*?\[/img\]', '', format_data).strip()
        description = re.sub(r'\n{3,}', '\n\n', description)

        return True, poster, description, extracted_imdb_link

    except Exception as e:
        return False, "", f"TJUæ ¼å¼APIè°ƒç”¨å¤±è´¥: {e}", ""


def _call_url_format_api(api_config: dict, douban_link: str, imdb_link: str):
    """
    è°ƒç”¨URLæ ¼å¼API (workers.dev, homeqian.top)
    """
    try:
        resource_url = douban_link or imdb_link
        url = f"{api_config['base_url']}/?url={resource_url}"

        response = requests.get(url, timeout=30)
        response.raise_for_status()

        # å°è¯•è§£æä¸ºJSON
        try:
            data = response.json()
        except:
            # å¦‚æœä¸æ˜¯JSONï¼Œå¯èƒ½æ˜¯ç›´æ¥è¿”å›çš„æ–‡æœ¬æ ¼å¼
            text_content = response.text.strip()
            if text_content and ('[img]' in text_content
                                 or 'â—' in text_content):
                # ç›´æ¥è¿”å›æ–‡æœ¬å†…å®¹ä½œä¸ºformat
                return _parse_format_content(text_content)
            else:
                return False, "", "APIè¿”å›äº†æ— æ•ˆçš„å†…å®¹æ ¼å¼", ""

        # JSONæ ¼å¼å¤„ç†
        if isinstance(data, dict):
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if data.get('success') is False:
                error_msg = data.get('message', data.get('error', 'æœªçŸ¥é”™è¯¯'))
                return False, "", f"APIè¿”å›å¤±è´¥: {error_msg}", ""

            # è·å–æ ¼å¼åŒ–å†…å®¹
            format_data = data.get('format', data.get('content', ''))
            if format_data:
                return _parse_format_content(format_data,
                                             data.get('imdb_link', ''))
            else:
                return False, "", "APIæœªè¿”å›æœ‰æ•ˆçš„æ ¼å¼åŒ–å†…å®¹", ""
        else:
            return False, "", "APIè¿”å›äº†æ— æ•ˆçš„æ•°æ®æ ¼å¼", ""

    except Exception as e:
        return False, "", f"URLæ ¼å¼APIè°ƒç”¨å¤±è´¥: {e}", ""


def _call_iyuu_format_api(api_config: dict, douban_link: str, imdb_link: str):
    """
    è°ƒç”¨IYUUæ ¼å¼API (api.iyuu.cn)
    """
    try:
        resource_url = douban_link or imdb_link
        url = f"{api_config['base_url']}?url={resource_url}"

        response = requests.get(url, timeout=30)
        response.raise_for_status()

        data = response.json()

        # æ£€æŸ¥ä¸šåŠ¡çŠ¶æ€ç 
        if data.get('ret') != 200 and data.get('ret') != 0:
            error_msg = data.get('msg', 'æœªçŸ¥é”™è¯¯')
            return False, "", f"APIè¿”å›é”™è¯¯(çŠ¶æ€ç {data.get('ret')}): {error_msg}", ""

        format_data = data.get('format') or data.get('data', {}).get(
            'format', '')
        if not format_data:
            return False, "", "APIæœªè¿”å›æœ‰æ•ˆçš„ç®€ä»‹å†…å®¹", ""

        return _parse_format_content(format_data)

    except Exception as e:
        return False, "", f"IYUUæ ¼å¼APIè°ƒç”¨å¤±è´¥: {e}", ""


def _parse_format_content(format_data: str, provided_imdb_link: str = ""):
    """
    è§£ææ ¼å¼åŒ–å†…å®¹ï¼Œæå–æµ·æŠ¥ã€ç®€ä»‹å’ŒIMDbé“¾æ¥
    """
    try:
        # æå–ä¿¡æ¯
        extracted_imdb_link = provided_imdb_link
        poster = ""
        description = ""

        # å¦‚æœæ²¡æœ‰æä¾›IMDbé“¾æ¥ï¼Œå°è¯•ä»æ ¼å¼åŒ–å†…å®¹ä¸­æå–
        if not extracted_imdb_link:
            imdb_match = re.search(
                r'â—IMDbé“¾æ¥\s*(https?://www\.imdb\.com/title/tt\d+/)',
                format_data)
            if imdb_match:
                extracted_imdb_link = imdb_match.group(1)

        # æå–æµ·æŠ¥å›¾ç‰‡
        img_match = re.search(r'(\[img\].*?\[/img\])', format_data)
        if img_match:
            poster = re.sub(r'img1', 'img9', img_match.group(1))

        # æå–ç®€ä»‹å†…å®¹ï¼ˆå»é™¤æµ·æŠ¥éƒ¨åˆ†ï¼‰
        description = re.sub(r'\[img\].*?\[/img\]', '', format_data).strip()
        description = re.sub(r'\n{3,}', '\n\n', description)

        return True, poster, description, extracted_imdb_link

    except Exception as e:
        return False, "", f"è§£ææ ¼å¼åŒ–å†…å®¹å¤±è´¥: {e}", ""


def _extract_douban_id(douban_link: str) -> str:
    """
    ä»è±†ç“£é“¾æ¥ä¸­æå–ID
    ä¾‹å¦‚: https://movie.douban.com/subject/34832354/ -> 34832354
    """
    match = re.search(r'/subject/(\d+)', douban_link)
    return match.group(1) if match else ""


def _extract_imdb_id(imdb_link: str) -> str:
    """
    ä»IMDbé“¾æ¥ä¸­æå–ID
    ä¾‹å¦‚: https://www.imdb.com/title/tt13721828/ -> tt13721828
    """
    match = re.search(r'/title/(tt\d+)', imdb_link)
    return match.group(1) if match else ""


# (ç¡®ä¿æ–‡ä»¶é¡¶éƒ¨æœ‰ import bencoder, import json)

# utils/media_helper.py


def add_torrent_to_downloader(detail_page_url: str, save_path: str,
                              downloader_id: str, db_manager, config_manager):
    """
    ä»ç§å­è¯¦æƒ…é¡µä¸‹è½½ .torrent æ–‡ä»¶å¹¶æ·»åŠ åˆ°æŒ‡å®šçš„ä¸‹è½½å™¨ã€‚
    [æœ€ç»ˆä¿®å¤ç‰ˆ] ä¿®æ­£äº†å‘ Transmission å‘é€æ•°æ®æ—¶çš„åŒé‡ç¼–ç é—®é¢˜ã€‚
    """
    logging.info(
        f"å¼€å§‹è‡ªåŠ¨æ·»åŠ ä»»åŠ¡: URL='{detail_page_url}', Path='{save_path}', DownloaderID='{downloader_id}'"
    )

    # 1. æŸ¥æ‰¾å¯¹åº”çš„ç«™ç‚¹é…ç½®
    conn = db_manager._get_connection()
    cursor = db_manager._get_cursor(conn)
    cursor.execute(
        "SELECT nickname, base_url, cookie, proxy, speed_limit FROM sites")
    site_info = None
    for site in cursor.fetchall():
        # [ä¿®å¤] ç¡®ä¿ base_url å­˜åœ¨ä¸”ä¸ä¸ºç©º
        if site['base_url'] and site['base_url'] in detail_page_url:
            site_info = dict(site)  # [ä¿®å¤] å°† sqlite3.Row è½¬æ¢ä¸º dict
            break
    conn.close()

    if not site_info or not site_info.get("cookie"):
        msg = f"æœªèƒ½æ‰¾åˆ°ä¸URL '{detail_page_url}' åŒ¹é…çš„ç«™ç‚¹é…ç½®æˆ–è¯¥ç«™ç‚¹ç¼ºå°‘Cookieã€‚"
        logging.error(msg)
        return False, msg

    try:
        # 2. ä¸‹è½½ç§å­æ–‡ä»¶
        common_headers = {
            "Cookie":
            site_info["cookie"],
            "User-Agent":
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
        }
        scraper = cloudscraper.create_scraper()

        # Add proxy support for downloading torrent
        proxies = None
        if site_info.get("proxy"):
            try:
                conf = (config_manager.get() or {})
                # ä¼˜å…ˆä½¿ç”¨è½¬ç§è®¾ç½®ä¸­çš„ä»£ç†åœ°å€ï¼Œå…¶æ¬¡å…¼å®¹æ—§çš„ network.proxy_url
                proxy_url = (conf.get("cross_seed", {})
                             or {}).get("proxy_url") or (conf.get(
                                 "network", {}) or {}).get("proxy_url")
                if proxy_url:
                    proxies = {"http": proxy_url, "https": proxy_url}
                    logging.info(f"ä½¿ç”¨ä»£ç†ä¸‹è½½ç§å­: {proxy_url}")
            except Exception as e:
                logging.warning(f"ä»£ç†è®¾ç½®å¤±è´¥: {e}")
                proxies = None

        # Add retry logic for network requests
        max_retries = 3
        for attempt in range(max_retries):
            try:
                details_response = scraper.get(detail_page_url,
                                               headers=common_headers,
                                               timeout=120,
                                               proxies=proxies)
                break  # Success, exit retry loop
            except Exception as e:
                if attempt < max_retries - 1:
                    logging.warning(
                        f"Attempt {attempt + 1} failed to fetch details page: {e}. Retrying..."
                    )
                    time.sleep(2**attempt)  # Exponential backoff
                else:
                    raise  # Re-raise the exception if all retries failed
        details_response.raise_for_status()

        soup = BeautifulSoup(details_response.text, "html.parser")
        torrent_id_match = re.search(r"id=(\d+)", detail_page_url)
        if not torrent_id_match: raise ValueError("æ— æ³•ä»è¯¦æƒ…é¡µURLä¸­æå–ç§å­IDã€‚")
        torrent_id = torrent_id_match.group(1)

        download_link_tag = soup.select_one(
            f'a.index[href^="download.php?id={torrent_id}"]')
        if not download_link_tag: raise RuntimeError("åœ¨è¯¦æƒ…é¡µHTMLä¸­æœªèƒ½æ‰¾åˆ°ä¸‹è½½é“¾æ¥ï¼")

        download_url_part = download_link_tag['href']
        full_download_url = f"{ensure_scheme(site_info['base_url'])}/{download_url_part}"

        common_headers["Referer"] = detail_page_url
        # Add retry logic for torrent download
        for attempt in range(max_retries):
            try:
                torrent_response = scraper.get(full_download_url,
                                               headers=common_headers,
                                               timeout=120,
                                               proxies=proxies)
                torrent_response.raise_for_status()
                break  # Success, exit retry loop
            except Exception as e:
                if attempt < max_retries - 1:
                    logging.warning(
                        f"Attempt {attempt + 1} failed to download torrent: {e}. Retrying..."
                    )
                    time.sleep(2**attempt)  # Exponential backoff
                else:
                    raise  # Re-raise the exception if all retries failed

        torrent_content = torrent_response.content
        logging.info("å·²æˆåŠŸä¸‹è½½æœ‰æ•ˆçš„ç§å­æ–‡ä»¶å†…å®¹ã€‚")

    except Exception as e:
        msg = f"åœ¨ä¸‹è½½ç§å­æ–‡ä»¶æ­¥éª¤å‘ç”Ÿé”™è¯¯: {e}"
        logging.error(msg, exc_info=True)
        return False, msg

    # 3. æ‰¾åˆ°ä¸‹è½½å™¨é…ç½®
    config = config_manager.get()
    downloader_config = next(
        (d for d in config.get("downloaders", [])
         if d.get("id") == downloader_id and d.get("enabled")), None)

    if not downloader_config:
        msg = f"æœªæ‰¾åˆ°IDä¸º '{downloader_id}' çš„å·²å¯ç”¨ä¸‹è½½å™¨é…ç½®ã€‚"
        logging.error(msg)
        return False, msg

    # 4. æ·»åŠ åˆ°ä¸‹è½½å™¨ (æ ¸å¿ƒä¿®æ”¹åœ¨æ­¤ï¼) - æ·»åŠ é‡è¯•æœºåˆ¶
    max_retries = 3
    for attempt in range(max_retries):
        try:
            from core.services import _prepare_api_config

            api_config = _prepare_api_config(downloader_config)
            client_name = downloader_config['name']

            if downloader_config['type'] == 'qbittorrent':
                client = qbClient(**api_config)
                client.auth_log_in()

                # å‡†å¤‡ qBittorrent å‚æ•°
                qb_params = {
                    'torrent_files': torrent_content,
                    'save_path': save_path,
                    'is_paused': False,
                    'skip_checking': True
                }

                # å¦‚æœç«™ç‚¹è®¾ç½®äº†é€Ÿåº¦é™åˆ¶ï¼Œåˆ™æ·»åŠ é€Ÿåº¦é™åˆ¶å‚æ•°
                # æ•°æ®åº“ä¸­å­˜å‚¨çš„æ˜¯MB/sï¼Œéœ€è¦è½¬æ¢ä¸ºbytes/sä¼ é€’ç»™ä¸‹è½½å™¨API
                if site_info and site_info.get('speed_limit', 0) > 0:
                    speed_limit = int(
                        site_info['speed_limit']) * 1024 * 1024  # è½¬æ¢ä¸º bytes/s
                    qb_params['upload_limit'] = speed_limit
                    logging.info(
                        f"ä¸ºç«™ç‚¹ '{site_info['nickname']}' è®¾ç½®ä¸Šä¼ é€Ÿåº¦é™åˆ¶: {site_info['speed_limit']} MB/s"
                    )

                result = client.torrents_add(**qb_params)
                logging.info(f"å·²å°†ç§å­æ·»åŠ åˆ° qBittorrent '{client_name}': {result}")

            elif downloader_config['type'] == 'transmission':
                client = TrClient(**api_config)

                # å‡†å¤‡ Transmission å‚æ•°
                tr_params = {
                    'torrent': torrent_content,
                    'download_dir': save_path,
                    'paused': False
                }

                # å¦‚æœç«™ç‚¹è®¾ç½®äº†é€Ÿåº¦é™åˆ¶ï¼Œåˆ™æ·»åŠ é€Ÿåº¦é™åˆ¶å‚æ•°
                # æ•°æ®åº“ä¸­å­˜å‚¨çš„æ˜¯MB/sï¼Œéœ€è¦è½¬æ¢ä¸ºbytes/sä¼ é€’ç»™ä¸‹è½½å™¨API
                if site_info and site_info.get('speed_limit', 0) > 0:
                    speed_limit = int(
                        site_info['speed_limit']) * 1024 * 1024  # è½¬æ¢ä¸º bytes/s
                    tr_params['uploadLimit'] = speed_limit
                    tr_params['uploadLimited'] = True
                    logging.info(
                        f"ä¸ºç«™ç‚¹ '{site_info['nickname']}' è®¾ç½®ä¸Šä¼ é€Ÿåº¦é™åˆ¶: {site_info['speed_limit']} MB/s"
                    )

                result = client.add_torrent(**tr_params)
                logging.info(
                    f"å·²å°†ç§å­æ·»åŠ åˆ° Transmission '{client_name}': ID={result.id}")

            return True, f"æˆåŠŸå°†ç§å­æ·»åŠ åˆ°ä¸‹è½½å™¨ '{client_name}'ã€‚"

        except Exception as e:
            logging.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•æ·»åŠ ç§å­åˆ°ä¸‹è½½å™¨å¤±è´¥: {e}")

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            if attempt < max_retries - 1:
                import time
                wait_time = 2**attempt  # æŒ‡æ•°é€€é¿
                logging.info(f"ç­‰å¾… {wait_time} ç§’åè¿›è¡Œç¬¬ {attempt + 2} æ¬¡å°è¯•...")
                time.sleep(wait_time)
            else:
                msg = f"æ·»åŠ åˆ°ä¸‹è½½å™¨ '{downloader_config['name']}' æ—¶å¤±è´¥: {e}"
                logging.error(msg, exc_info=True)
                return False, msg


def extract_tags_from_title(title_components: list) -> list:
    """
    ä»æ ‡é¢˜å‚æ•°ä¸­æå–æ ‡ç­¾ï¼Œä¸»è¦ä»åª’ä»‹å’Œåˆ¶ä½œç»„å­—æ®µæå– DIY å’Œ VCB-Studio æ ‡ç­¾ã€‚
    
    è¿”å›åŸå§‹æ ‡ç­¾åç§°ï¼ˆå¦‚ "DIY", "VCB-Studio"ï¼‰ï¼Œè€Œä¸æ˜¯æ ‡å‡†åŒ–é”®ã€‚
    è¿™æ ·å¯ä»¥è¢« ParameterMapper æ­£ç¡®æ˜ å°„åˆ° global_mappings.yaml ä¸­å®šä¹‰çš„æ ‡å‡†åŒ–é”®ã€‚

    :param title_components: æ ‡é¢˜ç»„ä»¶åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"key": "ä¸»æ ‡é¢˜", "value": "..."}, ...]
    :return: ä¸€ä¸ªåŒ…å«åŸå§‹æ ‡ç­¾åç§°çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ ['DIY', 'VCB-Studio']
    """
    if not title_components:
        return []

    found_tags = set()

    # å°† title_components è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿æŸ¥æ‰¾
    title_dict = {
        item.get('key'): item.get('value', '')
        for item in title_components
    }

    # å®šä¹‰éœ€è¦æ£€æŸ¥çš„å­—æ®µå’Œå¯¹åº”çš„æ ‡ç­¾æ˜ å°„
    # æ ¼å¼ï¼šå­—æ®µå -> [(æ­£åˆ™æ¨¡å¼, åŸå§‹æ ‡ç­¾å), ...]
    # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯åŸå§‹æ ‡ç­¾åï¼ˆå¦‚ "DIY"ï¼‰ï¼Œè€Œä¸æ˜¯æ ‡å‡†åŒ–é”®ï¼ˆå¦‚ "tag.diy"ï¼‰
    tag_extraction_rules = {
        'åª’ä»‹': [
            (r'\bDIY\b', 'DIY'),
            (r'\bBlu-?ray\s+DIY\b', 'DIY'),
            (r'\bBluRay\s+DIY\b', 'DIY'),
        ],
        'åˆ¶ä½œç»„': [
            (r'\bDIY\b', 'DIY'),
            (r'\bVCB-Studio\b', 'VCB-Studio'),
            (r'\bVCB\b', 'VCB-Studio'),
        ]
    }

    # éå†éœ€è¦æ£€æŸ¥çš„å­—æ®µ
    for field_name, patterns in tag_extraction_rules.items():
        field_value = title_dict.get(field_name, '')

        if not field_value:
            continue

        # å¦‚æœå­—æ®µå€¼æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(field_value, list):
            field_value = ' '.join(str(v) for v in field_value)
        else:
            field_value = str(field_value)

        # æ£€æŸ¥æ¯ä¸ªæ­£åˆ™æ¨¡å¼
        for pattern, tag_name in patterns:
            if re.search(pattern, field_value, re.IGNORECASE):
                found_tags.add(tag_name)
                print(
                    f"ä»æ ‡é¢˜å‚æ•° '{field_name}' ä¸­æå–åˆ°æ ‡ç­¾: {tag_name} (åŒ¹é…: {pattern})")

    result_tags = list(found_tags)
    if result_tags:
        print(f"ä»æ ‡é¢˜å‚æ•°ä¸­æå–åˆ°çš„æ ‡ç­¾: {result_tags}")
    else:
        print("ä»æ ‡é¢˜å‚æ•°ä¸­æœªæå–åˆ°ä»»ä½•æ ‡ç­¾")

    return result_tags


def extract_tags_from_description(description_text: str) -> list:
    """
    ä»ç®€ä»‹æ–‡æœ¬çš„"ç±»åˆ«"å­—æ®µä¸­æå–æ ‡ç­¾ã€‚
    
    :param description_text: ç®€ä»‹æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬statementå’Œbodyï¼‰
    :return: æ ‡ç­¾åˆ—è¡¨ï¼Œä¾‹å¦‚ ['tag.å–œå‰§', 'tag.åŠ¨ç”»']
    """
    if not description_text:
        return []

    found_tags = []

    # ä»ç®€ä»‹ä¸­æå–ç±»åˆ«å­—æ®µ
    category_match = re.search(r"â—\s*ç±»\s*åˆ«\s*(.+?)(?:\n|$)", description_text)
    if category_match:
        category_text = category_match.group(1).strip()
        print(f"ä»ç®€ä»‹ä¸­æå–åˆ°ç±»åˆ«: {category_text}")

        # å®šä¹‰ç±»åˆ«å…³é”®è¯åˆ°æ ‡ç­¾çš„æ˜ å°„
        category_tag_map = {
            'å–œå‰§': 'tag.å–œå‰§',
            'Comedy': 'tag.å–œå‰§',
            'åŠ¨ç”»': 'tag.åŠ¨ç”»',
            'Animation': 'tag.åŠ¨ç”»',
            'åŠ¨ä½œ': 'tag.åŠ¨ä½œ',
            'Action': 'tag.åŠ¨ä½œ',
            'çˆ±æƒ…': 'tag.çˆ±æƒ…',
            'Romance': 'tag.çˆ±æƒ…',
            'ç§‘å¹»': 'tag.ç§‘å¹»',
            'Sci-Fi': 'tag.ç§‘å¹»',
            'ææ€–': 'tag.ææ€–',
            'Horror': 'tag.ææ€–',
            'æƒŠæ‚š': 'tag.æƒŠæ‚š',
            'Thriller': 'tag.æƒŠæ‚š',
            'æ‚¬ç–‘': 'tag.æ‚¬ç–‘',
            'Mystery': 'tag.æ‚¬ç–‘',
            'çŠ¯ç½ª': 'tag.çŠ¯ç½ª',
            'Crime': 'tag.çŠ¯ç½ª',
            'æˆ˜äº‰': 'tag.æˆ˜äº‰',
            'War': 'tag.æˆ˜äº‰',
            'å†’é™©': 'tag.å†’é™©',
            'Adventure': 'tag.å†’é™©',
            'å¥‡å¹»': 'tag.å¥‡å¹»',
            'Fantasy': 'tag.å¥‡å¹»',
            'å®¶åº­': 'tag.å®¶åº­',
            'Family': 'tag.å®¶åº­',
            'å‰§æƒ…': 'tag.å‰§æƒ…',
            'Drama': 'tag.å‰§æƒ…',
        }

        # æ£€æŸ¥ç±»åˆ«æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        for keyword, tag in category_tag_map.items():
            if keyword in category_text:
                found_tags.append(tag)
                print(f"   ä»ç±»åˆ«ä¸­æå–åˆ°æ ‡ç­¾: {tag} (åŒ¹é…å…³é”®è¯: {keyword})")

    if found_tags:
        print(f"ä»ç®€ä»‹ç±»åˆ«ä¸­æå–åˆ°çš„æ ‡ç­¾: {found_tags}")
    else:
        print("ä»ç®€ä»‹ç±»åˆ«ä¸­æœªæå–åˆ°ä»»ä½•æ ‡ç­¾")

    return found_tags


def check_animation_type_from_description(description_text: str) -> bool:
    """
    æ£€æŸ¥ç®€ä»‹çš„ç±»åˆ«å­—æ®µä¸­æ˜¯å¦åŒ…å«"åŠ¨ç”»"ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®æ­£ç±»å‹ä¸ºåŠ¨æ¼«ã€‚
    
    :param description_text: ç®€ä»‹æ–‡æœ¬å†…å®¹ï¼ˆåŒ…æ‹¬statementå’Œbodyï¼‰
    :return: å¦‚æœåŒ…å«"åŠ¨ç”»"è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not description_text:
        return False

    # ä»ç®€ä»‹ä¸­æå–ç±»åˆ«å­—æ®µ
    category_match = re.search(r"â—\s*ç±»\s*åˆ«\s*(.+?)(?:\n|$)", description_text)
    if category_match:
        category_text = category_match.group(1).strip()

        # æ£€æŸ¥ç±»åˆ«ä¸­æ˜¯å¦åŒ…å«"åŠ¨ç”»"å…³é”®è¯
        if "åŠ¨ç”»" in category_text or "Animation" in category_text:
            print(f"æ£€æµ‹åˆ°ç±»åˆ«ä¸­åŒ…å«'åŠ¨ç”»': {category_text}")
            return True

    return False


def extract_tags_from_mediainfo(mediainfo_text: str) -> list:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ ‡ç­¾åˆ—è¡¨ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: ä¸€ä¸ªåŒ…å«è¯†åˆ«å‡ºçš„æ ‡ç­¾å­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼Œä¾‹å¦‚ ['tag.å›½è¯­', 'tag.ä¸­å­—', 'tag.HDR10']ã€‚
    """
    if not mediainfo_text:
        return []

    found_tags = set()
    lines = mediainfo_text.split('\n')  # ä¸è½¬å°å†™ï¼Œä¿æŒåŸå§‹å¤§å°å†™

    # å®šä¹‰å…³é”®è¯åˆ°æ ‡å‡†åŒ–æ ‡ç­¾çš„æ˜ å°„
    tag_keywords_map = {
        # è¯­è¨€æ ‡ç­¾
        'å›½è¯­': ['å›½è¯­', 'mandarin'],
        'ç²¤è¯­': ['ç²¤è¯­', 'cantonese'],
        # å­—å¹•æ ‡ç­¾
        'ä¸­å­—': ['ä¸­å­—', 'chinese', 'ç®€', 'ç¹'],
        'è‹±å­—': ['è‹±å­—', 'english'],
        # HDR æ ¼å¼æ ‡ç­¾
        'Dolby Vision': ['dolby vision', 'æœæ¯”è§†ç•Œ'],
        'HDR10+': ['hdr10+'],
        'HDR10': ['hdr10'],
        'HDR': ['hdr'],  # ä½œä¸ºé€šç”¨ HDR çš„å¤‡ç”¨é€‰é¡¹
        'HDRVivid': ['hdr vivid'],
    }

    # å®šä¹‰æ£€æŸ¥èŒƒå›´ï¼Œå‡å°‘ä¸å¿…è¦çš„æ‰«æ
    # is_audio_section/is_text_section ç”¨äºé™å®šè¯­è¨€å’Œå­—å¹•çš„æ£€æŸ¥èŒƒå›´
    is_audio_section = False
    is_text_section = False
    audio_section_lines = []

    for line in lines:
        line_stripped = line.strip()

        # åˆ¤å®šå½“å‰æ˜¯å¦å¤„äºç‰¹å®šä¿¡æ¯å—ä¸­
        if 'audio' in line_stripped.lower() and '#' in line_stripped:
            is_audio_section = True
            is_text_section = False
            audio_section_lines = [line_stripped]  # å¼€å§‹æ–°çš„éŸ³é¢‘å—
            continue
        if 'text' in line_stripped.lower() and '#' in line_stripped:
            is_text_section = True
            is_audio_section = False
            continue
        if 'video' in line_stripped.lower() and '#' in line_stripped:
            is_audio_section = False
            is_text_section = False
            # å¤„ç†éŸ³é¢‘å—ä¸­çš„å›½è¯­æ£€æµ‹
            if audio_section_lines:
                if _check_mandarin_in_audio_section(audio_section_lines):
                    found_tags.add('tag.å›½è¯­')
                audio_section_lines = []  # æ¸…ç©ºéŸ³é¢‘å—
            continue

        # æ”¶é›†éŸ³é¢‘å—çš„è¡Œ
        if is_audio_section:
            audio_section_lines.append(line_stripped)

        # æ£€æŸ¥å­—å¹•æ ‡ç­¾ (ä»…åœ¨ Text å—ä¸­)
        if is_text_section:
            line_lower = line_stripped.lower()
            if 'ä¸­å­—' in tag_keywords_map and any(
                    kw in line_lower for kw in tag_keywords_map['ä¸­å­—']):
                found_tags.add('tag.ä¸­å­—')
            if 'è‹±å­—' in tag_keywords_map and any(
                    kw in line_lower for kw in tag_keywords_map['è‹±å­—']):
                found_tags.add('tag.è‹±å­—')

        # æ£€æŸ¥ HDR æ ¼å¼æ ‡ç­¾ (å…¨å±€æ£€æŸ¥)
        line_lower = line_stripped.lower()
        if 'dolby vision' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['Dolby Vision']):
            found_tags.add('tag.Dolby Vision')
        if 'hdr10+' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR10+']):
            found_tags.add('tag.HDR10+')
        # HDR10 è¦æ”¾åœ¨ HDR ä¹‹å‰æ£€æŸ¥ï¼Œä»¥è·å¾—æ›´ç²¾ç¡®åŒ¹é…
        if 'hdr10' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR10']):
            found_tags.add('tag.HDR10')
        elif 'hdr' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDR']):
            # é¿å…é‡å¤æ·»åŠ ï¼Œå¦‚æœå·²æœ‰æ›´å…·ä½“çš„HDRæ ¼å¼ï¼Œåˆ™ä¸æ·»åŠ é€šç”¨çš„'HDR'
            if not any(hdr_tag in found_tags for hdr_tag in
                       ['tag.Dolby Vision', 'tag.HDR10+', 'tag.HDR10']):
                found_tags.add('tag.HDR')
        if 'hdrvivid' in tag_keywords_map and any(
                kw in line_lower for kw in tag_keywords_map['HDRVivid']):
            # æ³¨æ„ï¼šç«™ç‚¹å¯èƒ½æ²¡æœ‰ HDRVivid æ ‡ç­¾ï¼Œä½†æˆ‘ä»¬å…ˆæå–å‡ºæ¥
            found_tags.add('tag.HDRVivid')

    # å¤„ç†æœ€åä¸€ä¸ªéŸ³é¢‘å—ï¼ˆå¦‚æœæ–‡ä»¶æœ«å°¾æ²¡æœ‰videoå—ï¼‰
    if audio_section_lines:
        if _check_mandarin_in_audio_section(audio_section_lines):
            found_tags.add('tag.å›½è¯­')

    # ä¸ºæ‰€æœ‰æ ‡ç­¾æ·»åŠ  tag. å‰ç¼€
    prefixed_tags = set()
    for tag in found_tags:
        if not tag.startswith('tag.'):
            prefixed_tags.add(f'tag.{tag}')
        else:
            prefixed_tags.add(tag)

    print(f"ä» MediaInfo ä¸­æå–åˆ°çš„æ ‡ç­¾: {list(prefixed_tags)}")
    return list(prefixed_tags)


def _check_mandarin_in_audio_section(audio_lines):
    """
    æ£€æŸ¥éŸ³é¢‘å—ä¸­æ˜¯å¦åŒ…å«å›½è¯­ç›¸å…³æ ‡è¯†ã€‚
    
    :param audio_lines: éŸ³é¢‘å—çš„æ‰€æœ‰è¡Œ
    :return: å¦‚æœæ£€æµ‹åˆ°å›½è¯­è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    for line in audio_lines:
        # æ£€æŸ¥ Title: ä¸­æ–‡ æˆ– Language: Chinese
        if 'title:' in line.lower() and 'ä¸­æ–‡' in line:
            return True
        if 'language:' in line.lower() and ('chinese' in line.lower()
                                            or 'mandarin' in line.lower()):
            return True
        # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„å›½è¯­æ ‡è¯†
        if 'mandarin' in line.lower():
            return True

    return False


def extract_origin_from_description(description_text: str) -> str:
    """
    ä»ç®€ä»‹è¯¦æƒ…ä¸­æå–äº§åœ°ä¿¡æ¯ã€‚

    :param description_text: ç®€ä»‹è¯¦æƒ…æ–‡æœ¬
    :return: äº§åœ°ä¿¡æ¯ï¼Œä¾‹å¦‚ "æ—¥æœ¬"ã€"ä¸­å›½" ç­‰
    """
    if not description_text:
        return ""

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… "â—äº§ã€€ã€€åœ°ã€€æ—¥æœ¬" è¿™ç§æ ¼å¼
    # æ”¯æŒå¤šç§å˜ä½“ï¼šâ—äº§åœ°ã€â—äº§ã€€ã€€åœ°ã€â—å›½ã€€ã€€å®¶ç­‰
    patterns = [
        r"â—\s*äº§\s*åœ°\s*(.+?)(?:\s|$)",  # åŒ¹é… â—äº§åœ° æ—¥æœ¬
        r"â—\s*å›½\s*å®¶\s*(.+?)(?:\s|$)",  # åŒ¹é… â—å›½å®¶ æ—¥æœ¬
        r"â—\s*åœ°\s*åŒº\s*(.+?)(?:\s|$)",  # åŒ¹é… â—åœ°åŒº æ—¥æœ¬
        r"åˆ¶ç‰‡å›½å®¶/åœ°åŒº[:\s]+(.+?)(?:\s|$)",  # åŒ¹é… åˆ¶ç‰‡å›½å®¶/åœ°åŒº: æ—¥æœ¬
        r"åˆ¶ç‰‡å›½å®¶[:\s]+(.+?)(?:\s|$)",  # åŒ¹é… åˆ¶ç‰‡å›½å®¶: æ—¥æœ¬
        r"å›½å®¶[:\s]+(.+?)(?:\s|$)",  # åŒ¹é… å›½å®¶: æ—¥æœ¬
        r"äº§åœ°[:\s]+(.+?)(?:\s|$)",  # åŒ¹é… äº§åœ°: æ—¥æœ¬
        r"[äº§]\s*åœ°[:\s]+([^ï¼Œ,\n\r]+)",
        r"[å›½]\s*å®¶[:\s]+([^ï¼Œ,\n\r]+)",
        r"[åœ°]\s*åŒº[:\s]+([^ï¼Œ,\n\r]+)"
    ]

    for pattern in patterns:
        match = re.search(pattern, description_text)
        if match:
            origin = match.group(1).strip()
            # æ¸…ç†å¯èƒ½çš„å¤šä½™å­—ç¬¦
            origin = re.sub(r'[\[\]ã€ã€‘\(\)]', '', origin).strip()
            # ç§»é™¤å¸¸è§çš„åˆ†éš”ç¬¦ï¼Œå¦‚" / "ã€","ç­‰
            origin = re.split(r'\s*/\s*|\s*,\s*|\s*;\s*|\s*&\s*',
                              origin)[0].strip()
            print("æå–åˆ°äº§åœ°ä¿¡æ¯:", origin)

            return origin

    return ""


def extract_resolution_from_mediainfo(mediainfo_text: str) -> str:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–åˆ†è¾¨ç‡ä¿¡æ¯ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œä¾‹å¦‚ "720p"ã€"1080p"ã€"2160p" ç­‰
    """
    if not mediainfo_text:
        return ""

    # æŸ¥æ‰¾ Video éƒ¨åˆ†
    video_section_match = re.search(r"Video[\s\S]*?(?=\n\n|\Z)",
                                    mediainfo_text)
    if not video_section_match:
        return ""

    video_section = video_section_match.group(0)

    # æŸ¥æ‰¾åˆ†è¾¨ç‡ä¿¡æ¯
    # åŒ¹é…æ ¼å¼å¦‚ï¼šWidth                                 : 1 920 pixels
    #            Height                                : 1 080 pixels
    # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 920" -> "1920"
    width_match = re.search(r"[Ww]idth\s*:\s*(\d+)\s*(\d*)\s*pixels?",
                            video_section)
    height_match = re.search(r"[Hh]eight\s*:\s*(\d+)\s*(\d*)\s*pixels?",
                             video_section)

    width = None
    height = None

    if width_match:
        # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 920" -> "1920"
        w_groups = width_match.groups()
        if len(w_groups) >= 2 and w_groups[1]:
            width = int(f"{w_groups[0]}{w_groups[1]}")
        else:
            width = int(w_groups[0]) if w_groups[0] else None

    if height_match:
        # å¤„ç†å¸¦ç©ºæ ¼çš„æ•°å­—æ ¼å¼ï¼Œå¦‚ "1 080" -> "1080"
        h_groups = height_match.groups()
        if len(h_groups) >= 2 and h_groups[1]:
            height = int(f"{h_groups[0]}{h_groups[1]}")
        else:
            height = int(h_groups[0]) if h_groups[0] else None

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•å…¶ä»–æ ¼å¼
    if not width or not height:
        # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾ç±»ä¼¼ "1920 / 1080" çš„æ ¼å¼
        resolution_match = re.search(r"(\d{3,4})\s*/\s*(\d{3,4})",
                                     video_section)
        if resolution_match:
            width = int(resolution_match.group(1))
            height = int(resolution_match.group(2))
        else:
            # æŸ¥æ‰¾å…¶ä»–æ ¼å¼çš„åˆ†è¾¨ç‡ä¿¡æ¯
            other_resolution_match = re.search(r"(\d{3,4})\s*[xX]\s*(\d{3,4})",
                                               mediainfo_text)
            if other_resolution_match:
                width = int(other_resolution_match.group(1))
                height = int(other_resolution_match.group(2))

    # å¦‚æœæ‰¾åˆ°äº†å®½åº¦å’Œé«˜åº¦ï¼Œè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    if width and height:
        # æ ¹æ®é«˜åº¦ç¡®å®šæ ‡å‡†åˆ†è¾¨ç‡
        if height <= 480:
            return "480p"
        elif height <= 576:
            return "576p"
        elif height <= 720:
            return "720p"
        elif height <= 1080:
            return "1080p"
        elif height <= 1440:
            return "1440p"
        elif height <= 2160:
            return "2160p"
        else:
            # å¯¹äºå…¶ä»–éæ ‡å‡†åˆ†è¾¨ç‡ï¼Œè¿”å›åŸå§‹é«˜åº¦åŠ p
            return f"{height}p"

    return ""


def _upload_to_pixhost_direct(image_path: str, api_url: str, params: dict,
                              headers: dict):
    """ç›´æ¥ä¸Šä¼ å›¾ç‰‡åˆ°Pixhost"""
    try:
        with open(image_path, 'rb') as f:
            files = {'img': f}
            print("æ­£åœ¨å‘é€ä¸Šä¼ è¯·æ±‚åˆ° Pixhost...")
            response = requests.post(api_url,
                                     data=params,
                                     files=files,
                                     headers=headers,
                                     timeout=30)

            if response.status_code == 200:
                data = response.json()
                show_url = data.get('show_url')
                print(f"ç›´æ¥ä¸Šä¼ æˆåŠŸï¼å›¾ç‰‡é“¾æ¥: {show_url}")
                return show_url
            else:
                print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                return None
    except FileNotFoundError:
        print(f"   âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶")
        return None
    except requests.exceptions.SSLError as e:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: SSLè¿æ¥é”™è¯¯")
        return None
    except requests.exceptions.ConnectionError as e:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: ç½‘ç»œè¿æ¥è¢«é‡ç½®")
        return None
    except requests.exceptions.Timeout:
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: è¯·æ±‚è¶…æ—¶")
        return None
    except Exception as e:
        # åªæ‰“å°å¼‚å¸¸ç±»å‹å’Œç®€çŸ­æè¿°ï¼Œä¸æ‰“å°å®Œæ•´å †æ ˆ
        error_type = type(e).__name__
        print(f"   âŒ ç›´æ¥ä¸Šä¼ å¤±è´¥: {error_type}")
        return None


def _upload_to_pixhost_with_proxy(image_path: str, api_url: str, params: dict,
                                  headers: dict, proxy_url: str):
    """é€šè¿‡ä»£ç†ä¸Šä¼ å›¾ç‰‡åˆ°Pixhost"""
    if not proxy_url:
        print("   âš ï¸  æœªé…ç½®å…¨å±€ä»£ç†ï¼Œè·³è¿‡ä»£ç†ä¸Šä¼ ")
        return None

    try:
        # ä½¿ç”¨æ ‡å‡†HTTPä»£ç†æ–¹å¼
        with open(image_path, 'rb') as f:
            files = {'img': f}

            # è®¾ç½®ä»£ç†
            proxies = {'http': proxy_url, 'https': proxy_url}

            response = requests.post(api_url,
                                     data=params,
                                     files=files,
                                     headers=headers,
                                     proxies=proxies,
                                     timeout=30)

            if response.status_code == 200:
                try:
                    data = response.json()
                    show_url = data.get('show_url')
                    if show_url:
                        print(f"   âœ… ä»£ç†ä¸Šä¼ æˆåŠŸï¼å›¾ç‰‡é“¾æ¥: {show_url}")
                        return show_url
                    else:
                        print(f"   âŒ ä»£ç†ä¸Šä¼ å“åº”å¼‚å¸¸: æ— æ³•è§£æURL")
                        return None
                except Exception:
                    # JSONè§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å“åº”æ–‡æœ¬
                    if response.text and 'pixhost' in response.text:
                        print(f"   âœ… ä»£ç†ä¸Šä¼ æˆåŠŸï¼å›¾ç‰‡é“¾æ¥: {response.text.strip()}")
                        return response.text.strip()
                    else:
                        print(f"   âŒ ä»£ç†ä¸Šä¼ å“åº”å¼‚å¸¸: æ— æ•ˆæ ¼å¼")
                        return None
            else:
                print(f"   âŒ ä»£ç†ä¸Šä¼ å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                return None
    except requests.exceptions.SSLError:
        print(f"   âŒ ä»£ç†ä¸Šä¼ å¤±è´¥: SSLè¿æ¥é”™è¯¯")
        return None
    except requests.exceptions.ConnectionError:
        print(f"   âŒ ä»£ç†ä¸Šä¼ å¤±è´¥: ç½‘ç»œè¿æ¥é”™è¯¯")
        return None
    except requests.exceptions.Timeout:
        print(f"   âŒ ä»£ç†ä¸Šä¼ å¤±è´¥: è¯·æ±‚è¶…æ—¶")
        return None
    except FileNotFoundError:
        print(f"   âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶")
        return None
    except Exception as e:
        # åªæ‰“å°å¼‚å¸¸ç±»å‹ï¼Œä¸æ‰“å°å®Œæ•´å †æ ˆ
        error_type = type(e).__name__
        print(f"   âŒ ä»£ç†ä¸Šä¼ å¤±è´¥: {error_type}")
        return None


def _get_downloader_proxy_config(downloader_id: str = None):
    """
    æ ¹æ®ä¸‹è½½å™¨IDè·å–ä»£ç†é…ç½®ã€‚

    :param downloader_id: ä¸‹è½½å™¨ID
    :return: ä»£ç†é…ç½®å­—å…¸ï¼Œå¦‚æœä¸éœ€è¦ä»£ç†åˆ™è¿”å›None
    """
    if not downloader_id:
        return None

    config = config_manager.get()
    downloaders = config.get("downloaders", [])

    for downloader in downloaders:
        if downloader.get("id") == downloader_id:
            use_proxy = downloader.get("use_proxy", False)
            if use_proxy:
                host_value = downloader.get('host', '')
                proxy_port = downloader.get('proxy_port', 9090)
                if host_value.startswith(('http://', 'https://')):
                    parsed_url = urlparse(host_value)
                else:
                    parsed_url = urlparse(f"http://{host_value}")
                proxy_ip = parsed_url.hostname
                if not proxy_ip:
                    if '://' in host_value:
                        proxy_ip = host_value.split('://')[1].split(
                            ':')[0].split('/')[0]
                    else:
                        proxy_ip = host_value.split(':')[0]
                proxy_config = {
                    "proxy_base_url": f"http://{proxy_ip}:{proxy_port}",
                }
                return proxy_config
            break

    return None


def check_intro_completeness(body_text: str) -> dict:
    """
    æ£€æŸ¥ç®€ä»‹æ˜¯å¦å®Œæ•´ï¼ŒåŒ…å«å¿…è¦çš„å½±ç‰‡ä¿¡æ¯å­—æ®µã€‚
    
    :param body_text: ç®€ä»‹æ­£æ–‡å†…å®¹
    :return: åŒ…å«æ£€æµ‹ç»“æœçš„å­—å…¸ {
        "is_complete": bool,      # æ˜¯å¦å®Œæ•´
        "missing_fields": list,   # ç¼ºå¤±çš„å­—æ®µåˆ—è¡¨
        "found_fields": list      # å·²æ‰¾åˆ°çš„å­—æ®µåˆ—è¡¨
    }
    
    ç¤ºä¾‹:
        >>> result = check_intro_completeness(intro_body)
        >>> if not result["is_complete"]:
        >>>     print(f"ç¼ºå°‘å­—æ®µ: {result['missing_fields']}")
    """
    if not body_text:
        return {
            "is_complete": False,
            "missing_fields": ["æ‰€æœ‰å­—æ®µ"],
            "found_fields": []
        }

    # å®šä¹‰å¿…è¦å­—æ®µçš„åŒ¹é…æ¨¡å¼
    # æ¯ä¸ªå­—æ®µå¯ä»¥æœ‰å¤šä¸ªåŒ¹é…æ¨¡å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰
    required_patterns = {
        "ç‰‡å": [
            r"â—\s*ç‰‡\s*å", r"â—\s*è¯‘\s*å", r"â—\s*æ ‡\s*é¢˜", r"ç‰‡å\s*[:ï¼š]",
            r"è¯‘å\s*[:ï¼š]", r"Title\s*[:ï¼š]"
        ],
        "å¹´ä»£": [
            r"â—\s*å¹´\s*ä»£", r"â—\s*å¹´\s*ä»½", r"å¹´ä»½\s*[:ï¼š]", r"å¹´ä»£\s*[:ï¼š]",
            r"Year\s*[:ï¼š]"
        ],
        "äº§åœ°": [
            r"â—\s*äº§\s*åœ°", r"â—\s*å›½\s*å®¶", r"â—\s*åœ°\s*åŒº", r"åˆ¶ç‰‡å›½å®¶/åœ°åŒº\s*[:ï¼š]",
            r"åˆ¶ç‰‡å›½å®¶\s*[:ï¼š]", r"å›½å®¶\s*[:ï¼š]", r"äº§åœ°\s*[:ï¼š]", r"Country\s*[:ï¼š]"
        ],
        "ç±»åˆ«": [
            r"â—\s*ç±»\s*åˆ«", r"â—\s*ç±»\s*å‹", r"ç±»å‹\s*[:ï¼š]", r"ç±»åˆ«\s*[:ï¼š]",
            r"Genre\s*[:ï¼š]"
        ],
        "è¯­è¨€": [r"â—\s*è¯­\s*è¨€", r"è¯­è¨€\s*[:ï¼š]", r"Language\s*[:ï¼š]"],
        "å¯¼æ¼”": [r"â—\s*å¯¼\s*æ¼”", r"å¯¼æ¼”\s*[:ï¼š]", r"Director\s*[:ï¼š]"],
        "ç®€ä»‹": [
            r"â—\s*ç®€\s*ä»‹", r"â—\s*å‰§\s*æƒ…", r"â—\s*å†…\s*å®¹", r"ç®€ä»‹\s*[:ï¼š]",
            r"å‰§æƒ…\s*[:ï¼š]", r"å†…å®¹ç®€ä»‹\s*[:ï¼š]", r"Plot\s*[:ï¼š]", r"Synopsis\s*[:ï¼š]"
        ]
    }

    found_fields = []
    missing_fields = []

    # æ£€æŸ¥æ¯ä¸ªå¿…è¦å­—æ®µ
    for field_name, patterns in required_patterns.items():
        field_found = False
        for pattern in patterns:
            if re.search(pattern, body_text, re.IGNORECASE):
                field_found = True
                break

        if field_found:
            found_fields.append(field_name)
        else:
            missing_fields.append(field_name)

    # åˆ¤æ–­å®Œæ•´æ€§ï¼šå¿…é¡»åŒ…å«ä»¥ä¸‹å…³é”®å­—æ®µ
    # ç‰‡åã€äº§åœ°ã€å¯¼æ¼”ã€ç®€ä»‹ è¿™4ä¸ªå­—æ®µæ˜¯æœ€å…³é”®çš„
    critical_fields = ["ç‰‡å", "äº§åœ°", "å¯¼æ¼”", "ç®€ä»‹"]
    is_complete = all(field in found_fields for field in critical_fields)

    return {
        "is_complete": is_complete,
        "missing_fields": missing_fields,
        "found_fields": found_fields
    }


def is_image_url_valid_robust(url: str) -> bool:
    """
    ä¸€ä¸ªæ›´ç¨³å¥çš„æ–¹æ³•ï¼Œå½“HEADè¯·æ±‚å¤±è´¥æ—¶ï¼Œä¼šå°è¯•ä½¿ç”¨GETè¯·æ±‚ï¼ˆæµå¼ï¼‰è¿›è¡ŒéªŒè¯ã€‚
    å¦‚æœç›´æ¥è¯·æ±‚å¤±è´¥ï¼Œä¼šå°è¯•ä½¿ç”¨å…¨å±€ä»£ç†é‡è¯•ä¸€æ¬¡ã€‚
    """
    if not url:
        return False

    # ç¬¬ä¸€æ¬¡å°è¯•ï¼šä¸ä½¿ç”¨ä»£ç†
    try:
        # é¦–å…ˆå°è¯•HEADè¯·æ±‚ï¼Œå…è®¸é‡å®šå‘
        response = requests.head(url, timeout=5, allow_redirects=True)
        response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯2xxï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

        # æ£€æŸ¥Content-Type
        content_type = response.headers.get('Content-Type')
        if content_type and content_type.startswith('image/'):
            return True
        else:
            logging.warning(
                f"é“¾æ¥æœ‰æ•ˆä½†å†…å®¹å¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url} (Content-Type: {content_type})")
            return False

    except requests.exceptions.RequestException:
        # å¦‚æœHEADè¯·æ±‚å¤±è´¥ï¼Œå°è¯•GETè¯·æ±‚
        try:
            response = requests.get(url,
                                    stream=True,
                                    timeout=5,
                                    allow_redirects=True)
            response.raise_for_status()

            # æ£€æŸ¥Content-Type
            content_type = response.headers.get('Content-Type')
            if content_type and content_type.startswith('image/'):
                return True
            else:
                logging.warning(
                    f"é“¾æ¥æœ‰æ•ˆä½†å†…å®¹å¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url} (Content-Type: {content_type})")
                return False

        except requests.exceptions.RequestException as e:
            logging.warning(f"å›¾ç‰‡é“¾æ¥GETè¯·æ±‚ä¹Ÿå¤±è´¥äº†: {url} - {e}")

            # ç¬¬äºŒæ¬¡å°è¯•ï¼šä½¿ç”¨ä»£ç†é‡è¯•
            config = config_manager.get()
            global_proxy = config.get("network", {}).get("proxy_url")

            if global_proxy:
                logging.info(f"å°è¯•ä½¿ç”¨ä»£ç†é‡æ–°éªŒè¯å›¾ç‰‡é“¾æ¥: {url}")
                try:
                    proxies = {'http': global_proxy, 'https': global_proxy}

                    # å…ˆå°è¯•HEADè¯·æ±‚
                    response = requests.head(url,
                                             timeout=5,
                                             allow_redirects=True,
                                             proxies=proxies)
                    response.raise_for_status()

                    # æ£€æŸ¥Content-Type
                    content_type = response.headers.get('Content-Type')
                    if content_type and content_type.startswith('image/'):
                        logging.info(f"é€šè¿‡ä»£ç†éªŒè¯æˆåŠŸ: {url}")
                        return True
                    else:
                        logging.warning(
                            f"ä»£ç†éªŒè¯ï¼šé“¾æ¥æœ‰æ•ˆä½†å†…å®¹å¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url} (Content-Type: {content_type})"
                        )
                        return False

                except requests.exceptions.RequestException:
                    # HEADè¯·æ±‚å¤±è´¥ï¼Œå°è¯•GETè¯·æ±‚
                    try:
                        response = requests.get(url,
                                                stream=True,
                                                timeout=5,
                                                allow_redirects=True,
                                                proxies=proxies)
                        response.raise_for_status()

                        # æ£€æŸ¥Content-Type
                        content_type = response.headers.get('Content-Type')
                        if content_type and content_type.startswith('image/'):
                            logging.info(f"é€šè¿‡ä»£ç†GETè¯·æ±‚éªŒè¯æˆåŠŸ: {url}")
                            return True
                        else:
                            logging.warning(
                                f"ä»£ç†GETéªŒè¯ï¼šé“¾æ¥æœ‰æ•ˆä½†å†…å®¹å¯èƒ½ä¸æ˜¯å›¾ç‰‡: {url} (Content-Type: {content_type})"
                            )
                            return False

                    except requests.exceptions.RequestException as proxy_e:
                        logging.warning(f"ä½¿ç”¨ä»£ç†éªŒè¯å›¾ç‰‡é“¾æ¥ä¹Ÿå¤±è´¥äº†: {url} - {proxy_e}")
                        return False
            else:
                # æ²¡æœ‰é…ç½®ä»£ç†ï¼Œç›´æ¥è¿”å›å¤±è´¥
                return False


def extract_audio_codec_from_mediainfo(mediainfo_text: str) -> str:
    """
    ä» MediaInfo æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªéŸ³é¢‘æµçš„æ ¼å¼ã€‚

    :param mediainfo_text: å®Œæ•´çš„ MediaInfo æŠ¥å‘Šå­—ç¬¦ä¸²ã€‚
    :return: éŸ³é¢‘æ ¼å¼å­—ç¬¦ä¸² (ä¾‹å¦‚ "DTS", "AC-3", "FLAC")ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    if not mediainfo_text:
        return ""

    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª Audio éƒ¨åˆ† (æ”¯æŒ "Audio" å’Œ "Audio #1")
    audio_section_match = re.search(r"Audio(?: #1)?[\s\S]*?(?=\n\n|\Z)",
                                    mediainfo_text)
    if not audio_section_match:
        logging.warning("åœ¨MediaInfoä¸­æœªæ‰¾åˆ° 'Audio' éƒ¨åˆ†ã€‚")
        return ""

    audio_section = audio_section_match.group(0)

    # åœ¨ Audio éƒ¨åˆ†æŸ¥æ‰¾ Format
    format_match = re.search(r"Format\s*:\s*(.+)", audio_section)
    if format_match:
        audio_format = format_match.group(1).strip()
        logging.info(f"ä»MediaInfoçš„'Audio'éƒ¨åˆ†æå–åˆ°æ ¼å¼: {audio_format}")
        return audio_format

    logging.warning("åœ¨MediaInfoçš„'Audio'éƒ¨åˆ†æœªæ‰¾åˆ° 'Format' ä¿¡æ¯ã€‚")
    return ""
